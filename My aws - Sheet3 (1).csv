No,Content,A,B,C,D,Answered
2,"[""A company wants to share information with a third party. The third party has an\nHTTP API endpoint that the company can use to share the information. The company\nhas the required API key to access the HTTP API.\nThe company needs a way to manage the API key by using code. The integration of\nthe API key with the application code cannot affect application performance.\nWhich solution will meet these requirements MOST securely?""]",Store the API credentials in AWS Secrets Manager. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,Store the API credentials in a local code variable. Push the code to a secure Git repository. Use the local code variable at runtime to make the API call.,Store the API credentials as an object in a private Amazon S3 bucket. Restrict access to the S3 object by using IAM policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,Store the API credentials in an Amazon DynamoDB table. Restrict access to the table by using resource-based policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,"# Answer
- **Correct option:** A

- **Reason:** Storing the API credentials in AWS Secrets Manager allows for secure management and retrieval of sensitive information like API keys. Secrets Manager automatically handles encryption, access control, and key rotation, while also providing easy access via the AWS SDK, ensuring that the application performance remains unaffected at runtime. This approach is also in line with best practices for security and compliance.

# Example / Analogy
- Think of AWS Secrets Manager like a secure vault where you can store your valuable jewels (API keys). Just like you wouldn’t want to carry your expensive jewelry around in the open, secrets should not be hardcoded in your application. Instead, they should be stored securely and accessed only when needed.

# Common Mistakes / Traps
- A common mistake is to think that storing sensitive information directly in application code (like choice B) or simply using a less secure method (like S3 or DynamoDB without added security features) is sufficient. This goes against security best practices and can expose sensitive information to unintended access.

# Memory Tip
- Always store secrets securely, not in code; use AWS Secrets Manager for best security practices!"
3,"[""A company wants to share information with a third party. The third party has an\nHTTP API endpoint that the company can use to share the information. The company\nhas the required API key to access the HTTP API.\nThe company needs a way to manage the API key by using code. The integration of\nthe API key with the application code cannot affect application performance.\nWhich solution will meet these requirements MOST securely?""]",Store the API credentials in AWS Secrets Manager. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,Store the API credentials in a local code variable. Push the code to a secure Git repository. Use the local code variable at runtime to make the API call.,Store the API credentials as an object in a private Amazon S3 bucket. Restrict access to the S3 object by using IAM policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,Store the API credentials in an Amazon DynamoDB table. Restrict access to the table by using resource-based policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,"# Answer
- **Correct option:** A
- **Reason:** Storing the API key in AWS Secrets Manager provides a secure and controlled way to manage sensitive information. This method allows for automatic retrieval of the credentials at runtime without compromising application performance. Secrets Manager also offers built-in encryption and fine-grained access control, ensuring that only authorized entities can access the API key.

# Example / Analogy
- Think of AWS Secrets Manager as a secure vault where you can store your valuables (like API keys) along with instructions on how to access them. Just as you wouldn’t keep your valuables in plain sight at home, you shouldn’t hardcode sensitive data (like API keys) in your application code. Instead, you can ""check out"" what you need from the vault when you need it, ensuring only the right people can access it.

# Common Mistakes / Traps
- A common trap is to consider local variables or configuration files for storing API keys, as this can lead to unintentional exposure or hardcoding of credentials in the source code. Many developers might think pushing code to a secure Git repository keeps it safe, but repositories can be inadvertently accessed by unauthorized users or leaked.

# Memory Tip
- **""Use the vault for your secrets: store them securely in Secrets Manager!""**"
4,"[""A developer is deploying a new application to Amazon Elastic Container Service\n(Amazon ECS). The developer needs to securely store and retrieve different types\nof variables. These variables include authentication information for a remote\nAPI, the URL for the API, and credentials. The authentication information and\nAPI URL must be available to all current and future deployed versions of the\napplication across development, testing, and production environments.\nHow should the developer retrieve the variables with the FEWEST application\nchanges?""]",Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.,Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment.,Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment.,Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process.,"# Answer
- **Correct option:** A
- **Reason:** Option A suggests using AWS Systems Manager Parameter Store for application variables and AWS Secrets Manager for credentials. This approach allows for secure storage and retrieval of sensitive information with minimal application code changes across different environments (development, testing, and production). Parameter Store offers straightforward integration with ECS and maintains a single source of truth for all environments, ensuring consistency.

# Example / Analogy
- Think of Parameter Store like a secure vault for your app's keys. Just as a hotel might have a single lock system for all rooms, you keep all environment variables in the same “vault”. You simply call out to that vault when you need a key (variable), and it provides the right one based on what room (environment) you are currently in.

# Common Mistakes / Traps
- A common mistake is assuming that each environment needs separate code or hardcoding values into the application, which complicates deployments and can lead to security risks. Option B might seem appealing with AWS KMS but is not really designed for easy retrieval of config values like Parameter Store or Secrets Manager.

# Memory Tip
- AWS Parameter Store is the vault for variables; use it to avoid hardcoding and control access securely."
5,"[""A developer is creating an application that will give users the ability to store\nphotos from their cellphones in the cloud. The application needs to support tens\nof thousands of users. The application uses an Amazon API Gateway REST API that\nis integrated with AWS Lambda functions to process the photos. The application\nstores details about the photos in Amazon DynamoDB.\nUsers need to create an account to access the application. In the application,\nusers must be able to upload photos and retrieve previously uploaded photos. The\nphotos will range in size from 300 KB to 5 MB.\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos and details in the DynamoDB table. Retrieve previously uploaded photos directly from the DynamoDB table.,Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.,Create an IAM user for each user of the application during the sign-up process. Use IAM authentication to access the API Gateway API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.,Create a users table in DynamoDB. Use the table to manage user accounts. Create a Lambda authorizer that validates user credentials against the users table. Integrate the Lambda authorizer with API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as par of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.,"# Answer
- **Correct option:** B
- **Reason:** Option B uses Amazon Cognito user pools to manage user accounts, which simplifies user management and authentication, especially for applications that need to scale to tens of thousands of users. It also leverages Amazon S3 for storing photos, which is the ideal solution for handling large file uploads efficiently. Storing the S3 key in DynamoDB minimizes the operational overhead while enabling quick retrieval of the photos.

# Example / Analogy
- Think of Amazon S3 as a warehouse where all your photos are stored, and DynamoDB as an indexed catalog that tells you where each photo is located in the warehouse. Instead of digging through the warehouse (S3) to find a photo, you just look it up in the catalog (DynamoDB) for quick access.

# Common Mistakes / Traps
- A common trap is to try to store the photos in DynamoDB directly, which is not efficient given DynamoDB's limitations on item size and costs related to storage. Also, relying on IAM user creation for each individual can lead to immense management overhead and loss of scalability.

# Memory Tip
- ""Cognito for accounts, S3 for storage, DynamoDB for indexing."""
6,"[""A company receives food orders from multiple partners. The company has a\nmicroservices application that uses Amazon API Gateway APIs with AWS Lambda\nintegration. Each partner sends orders by calling a customized API that is\nexposed through API Gateway. The API call invokes a shared Lambda function to\nprocess the orders.\nPartners need to be notified after the Lambda function processes the orders.\nEach partner must receive updates for only the partner's own orders. The company\nwants to add new partners in the future with the fewest code changes possible.\nWhich solution will meet these requirements in the MOST scalable way?""]",Create a different Amazon Simple Notification Service (Amazon SNS) topic for each partner. Configure the Lambda function to publish messages for each partner to the partner's SNS topic.,Create a different Lambda function for each partner. Configure the Lambda function to notify each partner's service endpoint directly.,Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure the Lambda function to publish messages with specific attributes to the SNS topic. Subscribe each partner to the SNS topic. Apply the appropriate filter policy to the topic subscriptions.,Create one Amazon Simple Notification Service (Amazon SNS) topic. Subscribe all partners to the SNS topic.,"# Answer
- **Correct option:** C

- **Reason:** This solution allows for scalability by using a single Amazon Simple Notification Service (SNS) topic that can accommodate many partners easily. By publishing messages with specific attributes, the Lambda function can differentiate notifications for each partner. Each partner can subscribe to the SNS topic with a filter policy that ensures they only receive updates relevant to their own orders. This setup reduces the need for multiple resources and code changes when adding new partners.

# Example / Analogy
- Imagine a post office where each partner is a house that receives mail. Instead of building separate mailboxes for each house (which would be cumbersome), the post office has one central mailbox (the SNS topic). Letters dispatched from the post office have specific addresses (attributes). When a new house moves in (a new partner), the post office simply adds their address to the mailing list without needing a new mailbox or significant changes to the system.

# Common Mistakes / Traps
- A common mistake would be to choose option A and create separate SNS topics for each partner. While this meets the requirement, it does not scale well since every new partner would necessitate additional topics, complicating configuration and management. Options B and D also fall short as they either lead to increased complex configurations or all partners receiving every notification without filtering.

# Memory Tip
- Think of SNS like a central post office: one topic, many recipients, filtered mail."
7,"[""A developer has written an AWS Lambda function. The function is CPU-bound. The\ndeveloper wants to ensure that the function returns responses quickly.\nHow can the developer improve the function's performance?""]",Increase the function's CPU core count.,Increase the function's memory.,Increase the function's reserved concurrency.,Increase the function's timeout.,"# Answer
- **Correct option:** B : Increase the function's memory.

- **Reason:** In AWS Lambda, the CPU and memory allocation are linked. By increasing the memory for a Lambda function, you also increase the CPU resources allocated to it. For CPU-bound tasks, having more memory means that the function will have more CPU power available, allowing it to process tasks more quickly and efficiently.

# Example / Analogy
- Think of a chef in a kitchen where the number of stoves (CPU) is limited by the kitchen's size (memory). If the chef has access to more space, they can set up more stoves, cook more dishes at once, and thus serve the customers faster. By increasing the memory allocated to the Lambda function, you're effectively allowing it more ""stoves"" to work with, speeding up the processing time.

# Common Mistakes / Traps
- A common mistake is selecting option A (Increase the function's CPU core count). While it seems logical to want more CPU power, AWS Lambda does not allow you to directly adjust the number of CPU cores; it allows you to adjust memory, which indirectly increases CPU allocation based on the memory size.

# Memory Tip
- **More memory = more CPU power in Lambda!**"
8,"[""A company is building a serverless application on AWS. The application uses an\nAWS Lambda function to process customer orders 24 hours a day, 7 days a week.\nThe Lambda function calls an external vendor's HTTP API to process payments.\nDuring load tests, a developer discovers that the external vendor payment\nprocessing API occasionally times out and returns errors. The company expects\nthat some payment processing API calls will return errors.\nThe company wants the support team to receive notifications in near real time\nonly when the payment processing external API error rate exceed 5% of the total\nnumber of transactions in an hour. Developers need to use an existing Amazon\nSimple Notification Service (Amazon SNS) topic that is configured to notify the\nsupport team.\nWhich solution will meet these requirements?""]",Write the results of payment processing API calls to Amazon CloudWatch. Use Amazon CloudWatch Logs Insights to query the CloudWatch logs. Schedule the Lambda function to check the CloudWatch logs and notify the existing SNS topic.,Publish custom metrics to CloudWatch that record the failures of the external payment processing API calls. Configure a CloudWatch alarm to notify the existing SNS topic when error rate exceeds the specified rate.,Publish the results of the external payment processing API calls to a new Amazon SNS topic. Subscribe the support team members to the new SNS topic.,Write the results of the external payment processing API calls to Amazon S3. Schedule an Amazon Athena query to run at regular intervals. Configure Athena to send notifications to the existing SNS topic when the error rate exceeds the specified rate.,"# Answer
- **Correct option:** B
- **Reason:** Publishing custom metrics to Amazon CloudWatch allows the application to track error rates by capturing failures specifically from the payment processing API calls. A CloudWatch alarm can be configured to monitor these metrics and can trigger a notification via the existing SNS topic if the error rate exceeds 5%, meeting the company's requirements for near real-time alerts based on defined thresholds.

# Example / Analogy
- Imagine you're managing a restaurant's orders, and you set up a system that logs all customer complaints. If more than 5% of the customers report issues with orders in an hour, you want to be notified immediately to address the problem. By continuously tracking complaints (similar to CloudWatch metrics), you can swiftly react if the business's performance dips.

# Common Mistakes / Traps
- A common mistake is to think that simply logging the results (as in option A) or storing results in S3 (as in option D) is adequate for monitoring error rates. While these methods can provide data, they don’t facilitate immediate alerting and monitoring of key metrics, which is critical for timely response.

# Memory Tip
- Track key metrics with CloudWatch to alert swiftly; don't rely on logs or delayed queries."
9,"[""A company is offering APIs as a service over the internet to provide\nunauthenticated read access to statistical information that is updated daily.\nThe company uses Amazon API Gateway and AWS Lambda to develop the APIs. The\nservice has become popular, and the company wants to enhance the responsiveness\nof the APIs.\nWhich action can help the company achieve this goal?""]",Enable API caching in API Gateway.,Configure API Gateway to use an interface VPC endpoint.,Enable cross-origin resource sharing (CORS) for the APIs.,Configure usage plans and API keys in API Gateway.,"# Answer
- **Correct option:** A : Enable API caching in API Gateway.
- **Reason:** Enabling API caching allows responses from the API to be stored temporarily, which can significantly reduce the latency for subsequent requests, as these requests can be served from the cache rather than invoking the backend services (like Lambda) each time. This is particularly effective for read access APIs where the data does not change frequently, such as daily statistics.

# Example / Analogy
- Think of API caching like a library that allows you to borrow books. Instead of ordering a new book every time someone wants to read it, the library keeps popular books on a shelf ready for quick access. This reduces waiting time for readers (similar to reducing response time for users) since they can get the book immediately without having to wait for it to be ordered and delivered again.

# Common Mistakes / Traps
- A common mistake is to assume that enabling CORS or configuring usage plans directly improves API performance. While these are useful features, they do not directly enhance responsiveness in terms of reducing latency for read access operations.

# Memory Tip
- ""Cache to dash: speed up APIs with cached responses!"""
10,"[""A developer wants to store information about movies. Each movie has a title,\nrelease year, and genre. The movie information also can include additional\nproperties about the cast and production crew. This additional information is\ninconsistent across movies. For example, one movie might have an assistant\ndirector, and another movie might have an animal trainer.\nThe developer needs to implement a solution to support the following use cases:\nFor a given title and release year, get all details about the movie that has\nthat title and release year.\nFor a given title, get all details about all movies that have that title.\nFor a given genre, get all details about all movies in that genre.\nWhich data store configuration will meet these requirements?""]",Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the title as the partition key and the release year as the sort key. Create a global secondary index that uses the genre as the partition key and the title as the sort key.,Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the genre as the partition key and the release year as the sort key. Create a global secondary index that uses the title as the partition key.,"On an Amazon RDS DB instance, create a table that contains columns for title, release year, and genre. Configure the title as the primary key.","On an Amazon RDS DB instance, create a table where the primary key is the title and all other data is encoded into JSON format as one additional column.","# Answer
- **Correct option:** A
- **Reason:** Option A proposes a suitable design using Amazon DynamoDB with a primary key composed of the title as the partition key and the release year as the sort key. This allows for efficient querying of a specific movie detail using both the title and release year. Moreover, the global secondary index (GSI) on genre enables querying all movies by their genre. This structure effectively meets all the specified use cases.

# Example / Analogy
- Real-life example: Think of a library where books are organized by both title and year of publication. If you know the title and year, you can easily find the book, much like querying in DynamoDB with the primary key. Additionally, if you want to find all books of a specific genre, a separate shelf for genres acts like the global secondary index in DynamoDB.

# Common Mistakes / Traps
- A common mistake is assuming that a single primary key can satisfy all query requirements. Some options might seem simpler but could limit querying flexibility. For instance, Option C confining title as the primary key would not allow querying by genre or release year effectively, as it lacks sufficient granularity.

# Memory Tip
- **Remember:** ""DynamoDB needs the right keys for the queries you want to perform."""
11,"[""A company is migrating an on-premises database to Amazon RDS for MySQL. The\ncompany has read-heavy workloads. The company wants to refactor the code to\nachieve optimum read performance for queries.\nWhich solution will meet this requirement with LEAST current and future effort?""]",Use a multi-AZ Amazon RDS deployment. Increase the number of connections that the code makes to the database or increase the connection pool size if a connection pool is in use.,Use a multi-AZ Amazon RDS deployment. Modify the code so that queries access the secondary RDS instance.,Deploy Amazon RDS with one or more read replicas. Modify the application code so that queries use the URL for the read replicas.,Use open source replication software to create a copy of the MySQL database on an Amazon EC2 instance. Modify the application code so that queries use the IP address of the EC2 instance.,"# Answer
- **Correct option:** C : Deploy Amazon RDS with one or more read replicas. Modify the application code so that queries use the URL for the read replicas.
- **Reason:** Deploying read replicas allows you to offload read traffic from the primary database. This is optimal for read-heavy workloads, as it enhances performance while reducing latency for read queries. Each read replica can handle read queries independently, freeing up resources on the primary instance for write operations. This solution requires minimal changes to the code, involving only the modification of the connection strings to direct read queries to the replicas.

# Example / Analogy
- Think of a primary database like a busy restaurant kitchen. If all customers (queries) are directed to that kitchen, the chefs (the database) can get overwhelmed, leading to long wait times. By setting up read replicas as additional kitchens, you can channel diners (read queries) to these smaller kitchens, allowing them to be served faster without overloading the main kitchen.

# Common Mistakes / Traps
- A common mistake is to assume that a multi-AZ deployment will increase read performance. Multi-AZ is primarily designed for high availability and failover, not for handling read-heavy workloads. Another mistake is overlooking the need for application code modifications, as simply improving the database configuration isn't enough.

# Memory Tip
- **Tip:** Remember that read replicas boost read performance by splitting the workload—like having multiple kitchen setups for a busy restaurant."
12,"[""A developer has an application that stores data in an Amazon S3 bucket. The\napplication uses an HTTP API to store and retrieve objects. When the PutObject\nAPI operation adds objects to the S3 bucket the developer must encrypt these\nobjects at rest by using server-side encryption with Amazon S3 managed keys\n(SSE-S3).\nWhich solution will meet this requirement?""]",Create an AWS Key Management Service (AWS KMS) key. Assign the KMS key to the S3 bucket.,Set the x-amz-server-side-encryption header when invoking the PutObject API operation.,Provide the encryption key in the HTTP header of every request.,Apply TLS to encrypt the traffic to the S3 bucket.,"# Answer
- **Correct option:** B : Set the x-amz-server-side-encryption header when invoking the PutObject API operation.

- **Reason:** When using server-side encryption with Amazon S3 managed keys (SSE-S3), the developer needs to specify this encryption method during the `PutObject` API operation. This is done by setting the `x-amz-server-side-encryption` HTTP header to `AES256`. This instructs S3 to automatically encrypt the object at rest using its managed keys.

# Example / Analogy
- Think of it like checking a box to request a special wrapping when sending a gift. Just as you need to indicate that you want the gift wrapped upon sending it (in this case, by setting up a header), when uploading an object to S3, you specify that it should be encrypted using the SSE-S3 method with a header. 

# Common Mistakes / Traps
- A common mistake is to confuse SSE-S3 with AWS KMS. Option A mentions creating an AWS KMS key, which is not necessary for SSE-S3, as SSE-S3 uses its own Amazon-managed keys. Additionally, some might think that encryption should be applied at the transport layer (like TLS in option D), but that does not equate to encrypting data at rest.

# Memory Tip
- Remember: ""Set the header to encrypt, just like checking the box for gift wrapping!"""
13,"[""A developer needs to perform geographic load testing of an API. The developer\nmust deploy resources to multiple AWS Regions to support the load testing of the\nAPI.\nHow can the developer meet these requirements without additional application\ncode?""]",Create and deploy an AWS Lambda function in each desired Region. Configure the Lambda function to create a stack from an AWS CloudFormation template in that Region when the function is invoked.,Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI create-stack-set command to create a stack set in the desired Regions.,Create an AWS Systems Manager document that defines the resources. Use the document to create the resources in the desired Regions.,Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI deploy command to create a stack from the template in each Region.,"# Answer
- **Correct option:** B : Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI create-stack-set command to create a stack set in the desired Regions.

- **Reason:** The AWS CloudFormation stack set functionality allows you to easily deploy a single CloudFormation template across multiple AWS Regions simultaneously. This eliminates the need for additional application code and enables efficient resource deployment tailored to geographic load testing.

# Example / Analogy
- Imagine you are a chef preparing the same dish in multiple restaurants located in different cities. Instead of going to each restaurant and cooking the meal manually, you create a recipe (CloudFormation template). With stack sets, you can hand the recipe to a manager (the AWS CLI) at each restaurant, who can then execute it, ensuring each location prepares the dish consistently and simultaneously without additional work from you.

# Common Mistakes / Traps
- One common mistake is thinking that simply deploying resources in each region with separate commands or templates is sufficient, which does not leverage the efficiency and centralized management that stack sets offer. Some may also consider manually invoking resources in each region using Lambda or other methods, which is not as efficient or manageable.

# Memory Tip
- Use stack sets for efficient multi-region resource deployment; simplify your process."
14,"[""A developer is creating a template that uses AWS CloudFormation to deploy an\napplication. The application is serverless and uses Amazon API Gateway, Amazon\nDynamoDB, and AWS Lambda.\nWhich AWS service or tool should the developer use to define serverless\nresources in YAML?""]",CloudFormation serverless intrinsic functions,AWS Elastic Beanstalk,AWS Serverless Application Model (AWS SAM),AWS Cloud Development Kit (AWS CDK),"# Answer
- **Correct option:** C : AWS Serverless Application Model (AWS SAM)
- **Reason:** AWS SAM is specifically designed to simplify the creation and management of serverless applications. It extends AWS CloudFormation with a simplified syntax for defining serverless resources, allowing developers to define APIs, Lambda functions, and DynamoDB tables in a YAML format.

# Example / Analogy
- Think of AWS SAM like a recipe book for cooking a meal. Just as a recipe simplifies the cooking process by detailing the ingredients and steps, AWS SAM simplifies the deployment of serverless applications by providing a structured format to define the necessary resources and their configurations.

# Common Mistakes / Traps
- A common mistake is to confuse AWS SAM with AWS Cloud Development Kit (AWS CDK). While CDK allows you to define infrastructure using familiar programming languages and could also be used for serverless architectures, SAM is tailored specifically for serverless applications and offers an easier path with its simplified abstractions.

# Memory Tip
- Remember: ""SAM simplifies serverless."""
15,"[""A developer wants to insert a record into an Amazon DynamoDB table as soon as a\nnew file is added to an Amazon S3 bucket.\nWhich set of steps would be necessary to achieve this?""]",Create an event with Amazon EventBridge that will monitor the S3 bucket and then insert the records into DynamoDB.,Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.,Create an AWS Lambda function that will poll the S3 bucket and then insert the records into DynamoDB.,Create a cron job that will run at a scheduled time and insert the records into DynamoDB.,"# Answer
- **Correct option:** B : Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.

- **Reason:** This option allows for a direct and efficient way to respond to events in S3. By configuring an event notification on the S3 bucket, you can trigger a Lambda function automatically whenever a new file is added. This function can then handle the logic to insert a record into the DynamoDB table without the need for continuous polling or additional event management services.

# Example / Analogy
- Imagine you have a smart doorbell that notifies you when someone rings the bell (S3 event), and you have a second service (Lambda) that performs an action when it receives that notification, like recording the visitor’s name (inserting a record into DynamoDB). This direct trigger-response model is efficient and automated.

# Common Mistakes / Traps
- A common mistake is to overlook alternatives, like using EventBridge or polling methods. However, those methods introduce unnecessary complexity for this straightforward use case. The question specifically asks about actions as soon as a file is added, which the Lambda trigger handles perfectly.

# Memory Tip
- Remember: ""S3 events are like triggers; they call Lambda to take action instantly."""
16,"[""A development team maintains a web application by using a single AWS\nCloudFormation template. The template defines web servers and an Amazon RDS\ndatabase. The team uses the Cloud Formation template to deploy the Cloud\nFormation stack to different environments.\nDuring a recent application deployment, a developer caused the primary\ndevelopment database to be dropped and recreated. The result of this incident\nwas a loss of data. The team needs to avoid accidental database deletion in the\nfuture.\nWhich solutions will meet these requirements? (Choose two.)""]",Add a CloudFormation Deletion Policy attribute with the Retain value to the database resource.,Update the CloudFormation stack policy to prevent updates to the database.,Modify the database to use a Multi-AZ deployment.,Create a CloudFormation stack set for the web application and database deployments.,"# Answer
- **Correct options:** A and B

- **Reason:**  
Option A is correct because adding a CloudFormation Deletion Policy attribute with the Retain value to the database resource will ensure that if the stack is deleted, the database resource will not be deleted, thus preserving the data. Option B is also correct because updating the CloudFormation stack policy to prevent updates to the database will lock it down from being modified or deleted unexpectedly during stack updates, thereby protecting it from accidental deletion.

# Example / Analogy
- Imagine you have a precious family heirloom (the database) kept in a secure display cabinet (the CloudFormation stack). If you put a ""do not touch"" sign (the stack policy) on the cabinet, people know they can't disturb what's inside. Further, if you put a safeguard in place that says ""keep this inside even if you dismantle the cabinet"" (the retain policy), your heirloom stays safe regardless of what happens to the cabinet.

# Common Mistakes / Traps
- A common mistake is confusing the Deletion Policy with stack policies. The Deletion Policy specifically addresses how resources are treated upon stack deletion, while stack policies control what can be modified within the stack's lifecycle. Additionally, some might be tempted to consider options like Multi-AZ deployments (Option C) as a solution, but that primarily enhances availability rather than preventing deletion.

# Memory Tip
- Remember: **""Retain to sustain, policy to protect!""** (Deletion Policy for retainment; stack policy for protection)."
17,"[""An application that is hosted on an Amazon EC2 instance needs access to files\nthat are stored in an Amazon S3 bucket. The application lists the objects that\nare stored in the S3 bucket and displays a table to the user. During testing, a\ndeveloper discovers that the application does not show any objects in the list.\nWhat is the MOST secure way to resolve this issue?""]",Update the IAM instance profile that is attached to the EC2 instance to include the S3:* permission for the S3 bucket.,Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket.,Update the developer's user permissions to include the S3:ListBucket permission for the S3 bucket.,Update the S3 bucket policy by including the S3:ListBucket permission and by setting the Principal element to specify the account number of the EC2 instance.,"# Answer
- **Correct option:** B : Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket.

- **Reason:** The application running on the EC2 instance needs the `S3:ListBucket` permission to retrieve the list of objects in the specified S3 bucket. This permission allows the application to access the bucket's metadata and display the content. Granting broader permissions like `S3:*` is less secure and should be avoided when only `S3:ListBucket` is needed.

# Example / Analogy
- Imagine a library where you can only see the book titles on the shelves (metadata) if you have a specific library card (permission) that allows you to look at the index. The `S3:ListBucket` permission is like that library card—it lets the application see what is available, without giving full access to check out the books.

# Common Mistakes / Traps
- A common mistake is to assume that a user’s permissions are the same as those needed by an application. The application runs under the EC2 instance's IAM role, not under the developer's permissions. Also, choosing overly broad permissions can lead to security vulnerabilities.

# Memory Tip
- ""Grant only what’s necessary: List permissions to see, but don’t give full access!"""
18,"[""A company has deployed infrastructure on AWS. A development team wants to create\nan AWS Lambda function that will retrieve data from an Amazon Aurora database.\nThe Amazon Aurora database is in a private subnet in company's VPC. The VPC is\nnamed VPC1. The data is relational in nature. The Lambda function needs to\naccess the data securely.\nWhich solution will meet these requirements?""]",Create the Lambda function. Configure VPC1 access for the function. Attach a security group named SG1 to both the Lambda function and the database. Configure the security group inbound and outbound rules to allow TCP traffic on Port 3306.,Create and launch a Lambda function in a new public subnet that is in a new VPC named VPC2. Create a peering connection between VPC1 and VPC2.,Create the Lambda function. Configure VPC1 access for the function. Assign a security group named SG1 to the Lambda function. Assign a second security group named SG2 to the database. Add an inbound rule to SG1 to allow TCP traffic from Port 3306.,Export the data from the Aurora database to Amazon S3. Create and launch a Lambda function in VPC1. Configure the Lambda function query the data from Amazon S3.,"# Answer
- **Correct option:** A
- **Reason:** This option correctly implements VPC access for the Lambda function while maintaining the security requirements. By configuring the Lambda function to run within VPC1 and attaching a security group (SG1) that allows inbound and outbound TCP traffic on Port 3306 (the standard MySQL port used by Amazon Aurora), the Lambda function can securely communicate with the Aurora database.

# Example / Analogy
- Think of the Lambda function as a guest who needs special permission to enter a restricted area (the private subnet where the Aurora database resides). By giving the guest a pass (security group SG1) that allows access to specific areas (inbound and outbound rules for TCP on Port 3306), they can safely access the resources (the database) they need without exposing it to the entire internet.

# Common Mistakes / Traps
- A common mistake might be to assume that creating a new public subnet or a new VPC (as in option B) is necessary for a Lambda function to communicate with a private database. This would expose the setup to unnecessary complexities like VPC peering and would not adhere to the requirement for secure access to a private database.

# Memory Tip
- Remember: “Keep Lambda in the same VPC as the database for secure access!”"
19,"[""A developer is creating a mobile app that calls a backend service by using an\nAmazon API Gateway REST API. For integration testing during the development\nphase, the developer wants to simulate different backend responses without\ninvoking the backend service.\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Create an AWS Lambda function. Use API Gateway proxy integration to return constant HTTP responses.,Create an Amazon EC2 instance that serves the backend REST API by using an AWS CloudFormation template.,Customize the API Gateway stage to select a response type based on the request.,Use a request mapping template to select the mock integration response.,"# Answer
- **Correct option:** D : Use a request mapping template to select the mock integration response.

- **Reason:** This method allows you to define mock responses directly in API Gateway without any additional infrastructure or resources. By using request mapping templates, you can simulate various backend responses, which is efficient and requires the least operational overhead compared to setting up Lambda functions or EC2 instances.

# Example / Analogy
- Imagine you're a chef preparing a new dish in your restaurant. Instead of sourcing every ingredient and cooking it, you decide to create a sample dish just using what you have in your pantry. This way, you can quickly adjust flavors and portions without the need for extra supplies. Similarly, using mapping templates in API Gateway lets you quickly simulate backend responses without needing additional resources.

# Common Mistakes / Traps
- A common mistake is to choose options that require setting up additional infrastructure (like options A and B) thinking they may provide more flexibility, but they, in fact, introduce unnecessary complexity and operational overhead. 

# Memory Tip
- Think ""mock responses"" in API Gateway for quick tests—just like a chef trying flavors before the final dish!"
20,"[""A developer is creating an application that will store personal health\ninformation (PHI). The PHI needs to be encrypted at all times. An encrypted\nAmazon RDS for MySQL DB instance is storing the data. The developer wants to\nincrease the performance of the application by caching frequently accessed data\nwhile adding the ability to sort or rank the cached datasets.\nWhich solution will meet these requirements?""]",Create an Amazon ElastiCache for Redis instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.,Create an Amazon ElastiCache for Memcached instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.,Create an Amazon RDS for MySQL read replica. Connect to the read replica by using SSL. Configure the read replica to store frequently accessed data.,Create an Amazon DynamoDB table and a DynamoDB Accelerator (DAX) cluster for the table. Store frequently accessed data in the DynamoDB table.,"# Answer
- **Correct option:** A : Create an Amazon ElastiCache for Redis instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.

- **Reason:** Amazon ElastiCache for Redis supports advanced caching features, such as sorting and ranking capabilities, in addition to offering options for data encryption both at rest and in transit. This makes it suitable for handling personal health information (PHI) while enhancing application performance by caching frequently accessed data.

# Example / Analogy
- Think of ElastiCache for Redis like a high-speed filing system. When you need to retrieve important documents quickly (frequently accessed data), you keep them handy in a drawer (cache) where you can easily access them instead of going to the long-term archive (RDS). Additionally, ensuring that the drawer is locked (encryption) secures the important information inside.

# Common Mistakes / Traps
- A common mistake might be to choose Amazon ElastiCache for Memcached (Option B) instead of Redis, as Memcached does not support data persistence and lacks the advanced data structure features that Redis provides, which are necessary for sorting and ranking the cached data.

# Memory Tip
- Reflect: ""Use Redis for speed (cache) and encryption (security) of valuable data."""
21,"[""A company wants to deploy and maintain static websites on AWS. Each website's\nsource code is hosted in one of several version control systems, including AWS\nCodeCommit, Bitbucket, and GitHub.\nThe company wants to implement phased releases by using development, staging,\nuser acceptance testing, and production environments in the AWS Cloud.\nDeployments to each environment must be started by code merges on the relevant\nGit branch. The company wants to use HTTPS for all data exchange. The company\nneeds a solution that does not require servers to run continuously.\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Host each website by using AWS Amplify with a serverless backend. Conned the repository branches that correspond to each of the desired environments. Start deployments by merging code changes to a desired branch.,Host each website in AWS Elastic Beanstalk with multiple environments. Use the EB CLI to link each repository branch. Integrate AWS CodePipeline to automate deployments from version control code merges.,Host each website in different Amazon S3 buckets for each environment. Configure AWS CodePipeline to pull source code from version control. Add an AWS CodeBuild stage to copy source code to Amazon S3.,Host each website on its own Amazon EC2 instance. Write a custom deployment script to bundle each website's static assets. Copy the assets to Amazon EC2. Set up a workflow to run the script when code is merged.,"# Answer
- **Correct option:** A

- **Reason:** Hosting each website using AWS Amplify allows the company to easily connect repository branches that correspond to different environments, such as development, staging, user acceptance testing, and production. AWS Amplify provides a fully managed service that automatically deploys the website when code changes are merged into the relevant Git branch, and it handles HTTPS automatically. This solution requires no servers to run continuously, ensuring minimal operational overhead.

# Example / Analogy
- Think of AWS Amplify like a restaurant that offers a seamless take-out service. You simply call in your order (merge code changes), and the restaurant (Amplify) prepares your meal (deploys the website) without needing you to worry about the kitchen (servers) every time you want to order. Each time you want to try a new dish (deploy to an environment), you just call to place your order.

# Common Mistakes / Traps
- A common mistake is choosing options that require more management and operational overhead, such as EC2 instances or Elastic Beanstalk, which involve more configuration and ongoing management.

# Memory Tip
- **Remember AWS Amplify for serverless web deployment and effortless environment branching!**"
22,"[""A developer is creating an application that will be deployed on IoT devices. The\napplication will send data to a RESTful API that is deployed as an AWS Lambda\nfunction. The application will assign each API request a unique identifier. The\nvolume of API requests from the application can randomly increase at any given\ntime of day.\nDuring periods of request throttling, the application might need to retry\nrequests. The API must be able to handle duplicate requests without\ninconsistencies or data loss.\nWhich solution will meet these requirements?""]",Create an Amazon RDS for MySQL DB instance. Store the unique identifier for each request in a database table. Modify the Lambda function to check the table for the identifier before processing the request.,Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to check the table for the identifier before processing the request.,Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to return a client error response when the function receives a duplicate request.,Create an Amazon ElastiCache for Memcached instance. Store the unique identifier for each request in the cache. Modify the Lambda function to check the cache for the identifier before processing the request.,"# Answer
- **Correct option:** B 

- **Reason:** 
  Using an Amazon DynamoDB table to store the unique identifier for each API request allows the Lambda function to effectively check for and handle duplicate requests. DynamoDB provides high availability and can handle bursts of requests, making it suitable for IoT applications with fluctuating workloads. By storing incoming request identifiers, the Lambda function can prevent processing the same request multiple times, ensuring data integrity and consistency.

# Example / Analogy
- Think of it like a ticketing system at a concert where each ticket has a unique serial number. If someone tries to enter with a duplicate ticket number, the venue can reference a master list (like DynamoDB) to check for duplicates and deny entry, ensuring each person only experiences the concert once, just as the application ensures no duplicate data is processed.

# Common Mistakes / Traps
- A common mistake could be selecting option A instead of B, assuming relational databases like RDS would work efficiently with high-velocity requests. However, DynamoDB's design is inherently better suited for fast, scalable key-value lookups necessary for this use case.

# Memory Tip
- **Think “DynamoDB for data integrity” to handle duplicates effectively!**"
23,"[""A developer is creating an application that includes an Amazon API Gateway REST\nAPI in the us-east-2 Region. The developer wants to use Amazon CloudFront and a\ncustom domain name for the API. The developer has acquired an SSL/TLS\ncertificate for the domain from a third-party provider.\nHow should the developer configure the custom domain for the application?""]",Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS A record for the custom domain.,Import the SSL/TLS certificate into CloudFront. Create a DNS CNAME record for the custom domain.,Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS CNAME record for the custom domain.,Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. Create a DNS CNAME record for the custom domain.,"# Answer
- **Correct option:** D
- **Reason:** The developer should import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Region because API Gateway requires the certificate to be in that region when using a custom domain with CloudFront. After the certificate is imported, the developer can create a DNS CNAME record for the custom domain pointing to the CloudFront distribution.

# Example / Analogy
- Think of it like needing a special permit (the SSL certificate) that is issued only from a specific office (us-east-1) in a city (AWS). Even if your business (API) is located in a different part of the city (us-east-2), you still need that permit to operate legally. You’ll then need to let people know about your new location (creating a DNS record).

# Common Mistakes / Traps
- A common mistake is to think that the SSL/TLS certificate can be in the same region as the API (us-east-2) when in fact it must be imported to us-east-1 for the API Gateway custom domain setup. Developers might also misinterpret how to link the DNS records, mixing up A records and CNAME records.

# Memory Tip
- Remember: ""SSL in us-east-1, CNAME to disguise the endpoint!"""
24,"[""A developer is building a new application on AWS. The application uses an AWS\nLambda function that retrieves information from an Amazon DynamoDB table. The\ndeveloper hard coded the DynamoDB table name into the Lambda function code. The\ntable name might change over time. The developer does not want to modify the\nLambda code if the table name changes.\nWhich solution will meet these requirements MOST efficiently?""]",Create a Lambda environment variable to store the table name. Use the standard method for the programming language to retrieve the variable.,Store the table name in a file. Store the file in the /tmp folder. Use the SDK for the programming language to retrieve the table name.,Create a file to store the table name. Zip the file and upload the file to the Lambda layer. Use the SDK for the programming language to retrieve the table name.,Create a global variable that is outside the handler in the Lambda function to store the table name.,"# Answer
- **Correct option:** A : Create a Lambda environment variable to store the table name. Use the standard method for the programming language to retrieve the variable.

- **Reason:** This solution allows the table name to be easily configurable without modifying the Lambda function code. Environment variables can be updated directly in the AWS Lambda console or through a configuration management tool, making it a flexible and efficient way to manage dynamic configuration data like table names.

# Example / Analogy
- Think of environment variables as labels on containers. Imagine you have several boxes labeled ""Books,"" ""Toys,"" and ""Clothes."" If you want to change the contents of ""Books"" to ""Old Books,"" you just change the label rather than redoing the whole box. Similarly, with environment variables, you can change the table name without altering the code.

# Common Mistakes / Traps
- A common trap is to use hard-coded strings or global variables, thinking they provide an easier solution, but these methods require code changes whenever a value needs to be changed, which defeats the purpose of dynamically configuring the application.

# Memory Tip
- Use environment variables for dynamic settings instead of hardcoded values to ensure flexibility and ease of maintenance."
25,"[""A developer is migrating some features from a legacy monolithic application to\nuse AWS Lambda functions instead. The application currently stores data in an\nAmazon Aurora DB cluster that runs in private subnets in a VPC. The AWS account\nhas one VPC deployed. The Lambda functions and the DB cluster are deployed in\nthe same AWS Region in the same AWS account.\nThe developer needs to ensure that the Lambda functions can securely access the\nDB cluster without crossing the public internet.\nWhich solution will meet these requirements?""]",Configure the DB cluster's public access setting to Yes.,Configure an Amazon RDS database proxy for he Lambda functions.,Configure a NAT gateway and a security group for the Lambda functions.,"Configure the VPC, subnets, and a security group for the Lambda functions.","# Answer
- **Correct option:** D : Configure the VPC, subnets, and a security group for the Lambda functions.

- **Reason:** Lambda functions can securely access resources within a VPC by being configured to run inside that VPC. By placing the Lambda functions in the same VPC as the Amazon Aurora DB cluster and setting the appropriate subnets and security groups, you ensure that the connection between the Lambda functions and the DB remains private and does not traverse the public internet.

# Example / Analogy  
- Think of it like a secure office building with locked doors. When your Lambda functions are like employees who work inside that building (VPC), they need special access (subnets and security groups) to reach the data stored in the office’s secure vault (Aurora DB cluster). By giving them access to the same building without letting them go outside (the internet), you ensure that their work remains secure.

# Common Mistakes / Traps  
- A common mistake is to assume that simply configuring public access or adding a NAT gateway allows secure access to the DB. Public access exposes the database to the internet, which is not what the question requires. Additionally, a NAT gateway is typically used for outbound internet access from private subnets, which is not necessary for this scenario.

# Memory Tip  
- Remember: Keep Lambda in the same VPC as Aurora for a secure, private connection!"
26,"[""A developer is writing an AWS Lambda function. The developer wants to log key\nevents that occur while the Lambda function runs. The developer wants to include\na unique identifier to associate the events with a specific function invocation.\nThe developer adds the following code to the Lambda function:\n[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image2.png]\nWhich solution will meet this requirement?""]",Obtain the request identifier from the AWS request ID field in the context object. Configure the application to write logs to standard output.,Obtain the request identifier from the AWS request ID field in the event object. Configure the application to write logs to a file.,Obtain the request identifier from the AWS request ID field in the event object. Configure the application to write logs to standard output.,Obtain the request identifier from the AWS request ID field in the context object. Configure the application to write logs to a file.,"# Answer
- **Correct option:** A
- **Reason:** The context object in an AWS Lambda function provides the AWS request ID, which serves as a unique identifier for the specific invocation of the function. By logging this ID to standard output, the logs become easily accessible in Amazon CloudWatch, allowing the developer to track and associate log entries with that particular function execution.

# Example / Analogy
- Think of the AWS request ID as a unique ticket number at an event. Each attendee (function invocation) gets a ticket (request ID), which they can show to identify themselves. If something happens at the event (like a lost item), the ticket helps trace back to that specific attendee, just like the request ID helps trace back to the specific invocation of the Lambda function.

# Common Mistakes / Traps
- A common mistake is opting for the event object instead of the context object to retrieve the request ID. The event object typically contains the input data for the function and may not have the unique identifier meant for logging and tracing.

# Memory Tip
- Remember, ""Context is Key"" when needing unique identifiers in AWS Lambda."
27,"[""An application uses an Amazon EC2 Auto Scaling group. A developer notices that\nEC2 instances are taking a long time to become available during scale-out\nevents. The UserData script is taking a long time to run.\nThe developer must implement a solution to decrease the time that elapses before\nan EC2 instance becomes available. The solution must make the most recent\nversion of the application available at all times and must apply all available\nsecurity updates. The solution also must minimize the number of images that are\ncreated. The images must be validated.\nWhich combination of steps should the developer take to meet these requirements?\n(Choose two.)""]",Use EC2 Image Builder to create an Amazon Machine Image (AMI). Install all the patches and agents that are needed to manage and run the application. Update the Auto Scaling group launch configuration to use the AMI.,Use EC2 Image Builder to create an Amazon Machine Image (AMI). Install the latest version of the application and all the patches and agents that are needed to manage and run the application. Update the Auto Scaling group launch configuration to use the AMI.,Set up AWS CodeDeploy to deploy the most recent version of the application at runtime.,Set up AWS CodePipeline to deploy the most recent version of the application at runtime.,"# Answer
- **Correct options:** A and B

- **Reason:** Option A focuses on using EC2 Image Builder to prepare an AMI that has all necessary patches and agents installed, streamlining the instance launch process. However, while it ensures security updates, it does not ensure the application is the latest version. Option B extends this by also ensuring that the latest version of the application is pre-installed in the AMI created by EC2 Image Builder. This reduces the time it takes for an EC2 instance to become fully operational during scale-out events since the setup that usually runs at the initial startup is handled beforehand.

# Example / Analogy
- Think of preparing a meal for guests. Instead of cooking everything from scratch when they arrive (which takes time), you can pre-cook the meal (like creating an AMI) and just reheat it when guests arrive, ensuring it's always the freshest version and meets guests’ preferences.

# Common Mistakes / Traps
- A common mistake could be assuming that simply updating the UserData script would speed up the initialization process. While this may help, it does not address the need for the most current application version or maintain security patches by itself. 

# Memory Tip
- Pre-pack your AMI like a meal prep: have everything ready so that your EC2 instances serve up the latest version quickly!"
28,"[""A developer is using an AWS Lambda function to generate avatars for profile\npictures that are uploaded to an Amazon S3 bucket. The Lambda function is\nautomatically invoked for profile pictures that are saved under the /original/\nS3 prefix. The developer notices that some pictures cause the Lambda function to\ntime out. The developer wants to implement a fallback mechanism by using another\nLambda function that resizes the profile picture.\nWhich solution will meet these requirements with the LEAST development effort?""]",Set the image resize Lambda function as a destination of the avatar generator Lambda function for the events that fail processing.,Create an Amazon Simple Queue Service (Amazon SQS) queue. Set the SQS queue as a destination with an on failure condition for the avatar generator Lambda function. Configure the image resize Lambda function to poll from the SQS queue.,Create an AWS Step Functions state machine that invokes the avatar generator Lambda function and uses the image resize Lambda function as a fallback. Create an Amazon EventBridge rule that matches events from the S3 bucket to invoke the state machine.,Create an Amazon Simple Notification Service (Amazon SNS) topic. Set the SNS topic as a destination with an on failure condition for the avatar generator Lambda function. Subscribe the image resize Lambda function to the SNS topic.,"# Answer
- **Correct option:** A 
- **Reason:** Setting the image resize Lambda function as a destination of the avatar generator Lambda function for failed events is the simplest implementation. By configuring this as a failure destination, the resize function gets invoked automatically without the need for additional polling or complex error-handling mechanisms.

# Example / Analogy
- Imagine a bakery where a chef (the avatar generator) bakes a cake (the avatar), but sometimes the cake falls (times out). Instead of throwing away the fallen cake, there’s a helper (resize Lambda) who can quickly fix and decorate it. When the chef’s attempt fails, the helper is automatically notified to step in and resize the cake without needing a complicated process to communicate.

# Common Mistakes / Traps
- A common trap is to overcomplicate the solution, like using SQS or SNS, which introduces additional components and maintenance. Understanding that a simple failure destination can directly invoke another Lambda function helps streamline error handling.

# Memory Tip
- Remember: **""Simpler is better!""** Use failure destinations for direct fallback."
29,"[""A developer needs to migrate an online retail application to AWS to handle an\nanticipated increase in traffic. The application currently runs on two servers:\none server for the web application and another server for the database. The web\nserver renders webpages and manages session state in memory. The database server\nhosts a MySQL database that contains order details. When traffic to the\napplication is heavy, the memory usage for the web server approaches 100% and\nthe application slows down considerably.\nThe developer has found that most of the memory increase and performance\ndecrease is related to the load of managing additional user sessions. For the\nweb server migration, the developer will use Amazon EC2 instances with an Auto\nScaling group behind an Application Load Balancer.\nWhich additional set of changes should the developer make to the application to\nimprove the application's performance?""]",Use an EC2 instance to host the MySQL database. Store the session data and the application data in the MySQL database.,Use Amazon ElastiCache for Memcached to store and manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.,Use Amazon ElastiCache for Memcached to store and manage the session data and the application data.,Use the EC2 instance store to manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.,"# Answer
- **Correct option:** B: Use Amazon ElastiCache for Memcached to store and manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.

- **Reason:** Storing session data in Amazon ElastiCache helps alleviate the load on the web server's memory, as it provides a scalable in-memory caching solution specifically designed for high performance. This allows the web server to focus on rendering web pages and processing requests rather than managing session state. Additionally, using Amazon RDS for MySQL offloads database management tasks, providing a managed database solution that can handle scalability, backups, and high availability, which is particularly beneficial for an online retail application with fluctuating traffic demands.

# Example / Analogy
- Think of managing sessions like running a busy restaurant. If the chef (your web server) is also responsible for taking orders (managing sessions) and cooking (serving web pages), the quality of food (application performance) will suffer when too many customers arrive. By hiring a waiter (Amazon ElastiCache) to take orders (handle sessions), the chef can focus on cooking efficiently. Meanwhile, the RDS for MySQL can be compared to a reliable supply chain that ensures all ingredients (application data) are always in stock and readily available.

# Common Mistakes / Traps
- A common mistake is to consider that session data can simply be stored in the database (option A or D), as traditional databases are not optimized for high-speed reads/writes required for session management. Additionally, opting for EC2 instance storage (option D) is risky since instance storage is ephemeral and will be lost if the instance is terminated. 

# Memory Tip
- Use caching (like ElastiCache) to offload session management from the web server for better performance!"
30,"[""A developer has a legacy application that is hosted on-premises. Other\napplications hosted on AWS depend on the on-premises application for proper\nfunctioning. In case of any application errors, the developer wants to be able\nto use Amazon CloudWatch to monitor and troubleshoot all applications from one\nplace.\nHow can the developer accomplish this?""]",Install an AWS SDK on the on-premises server to automatically send logs to CloudWatch.,Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch.,Upload log files from the on-premises server to Amazon S3 and have CloudWatch read the files.,Upload log files from the on-premises server to an Amazon EC2 instance and have the instance forward the logs to CloudWatch.,"# Answer
- **Correct option:** B : Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch.

- **Reason:** The Amazon CloudWatch Agent is specifically designed to collect monitoring and operational data from both EC2 and on-premises servers. By installing this agent on the on-premises server, the developer can have the application logs sent directly to CloudWatch, providing centralized monitoring for troubleshooting and operational oversight.

# Example / Analogy
- Imagine a security camera (CloudWatch agent) installed in a store (on-premises server) that continuously relays video footage (logs) to a central monitoring station (CloudWatch). Instead of having to go to the store to check the footage, the security personnel can simply view everything from one centralized location.

# Common Mistakes / Traps
- A common mistake could be to think that simply uploading logs to S3 (Options C and D) would allow direct integration with CloudWatch. While this approach is valid for archival, it doesn’t provide real-time monitoring capabilities like the CloudWatch agent does. Additionally, users might assume that just using an SDK (Option A) is sufficient, but the agility and functionality of the dedicated CloudWatch agent make it the preferred solution.

# Memory Tip
- **Install the CloudWatch agent on-premises for real-time log monitoring and centralized troubleshooting.**"
31,"[""An ecommerce company is using an AWS Lambda function behind Amazon API Gateway\nas its application tier. To process orders during checkout, the application\ncalls a POST API from the frontend. The POST API invokes the Lambda function\nasynchronously. In rare situations, the application has not processed orders.\nThe Lambda application logs show no errors or failures.\nWhat should a developer do to solve this problem?""]",Inspect the frontend logs for API failures. Call the POST API manually by using the requests from the log file.,Create and inspect the Lambda dead-letter queue. Troubleshoot the failed functions. Reprocess the events.,Inspect the Lambda logs in Amazon CloudWatch for possible errors. Fix the errors.,Make sure that caching is disabled for the POST API in API Gateway.,"# Answer
- **Correct option:** B
- **Reason:** Creating and inspecting the Lambda dead-letter queue (DLQ) allows you to identify any events that failed to be processed correctly. Since the logs do not show any errors or failures, it's possible that the Lambda function processed the request, but there may have been an issue with the event itself that caused it to be unprocessed. The DLQ will help capture this data.

# Example / Analogy
- Think of the dead-letter queue as a lost-and-found box. If someone orders a package, but it gets lost (in this case, the event fails), the order information is still there in the lost-and-found. By checking the box (DLQ), you can see what went wrong and re-process those orders.

# Common Mistakes / Traps
- A common mistake might be to assume that there was an error in the Lambda logs (Choice C) when no error was logged, which can lead to overlooking the possibility of unprocessed events. Another trap could be focusing on API Gateway's configurations like caching (Choice D), which isn't the issue when the Lambda function shows no errors.

# Memory Tip
- Remember: ""Use the dead-letter queue to find lost events!"""
32,"[""A company has an Amazon S3 bucket that contains sensitive data. The data must be\nencrypted in transit and at rest. The company encrypts the data in the S3 bucket\nby using an AWS Key Management Service (AWS KMS) key. A developer needs to grant\nseveral other AWS accounts the permission to use the S3 GetObject operation to\nretrieve the data from the S3 bucket.\nHow can the developer enforce that all requests to retrieve the data provide\nencryption in transit?""]",Define a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false”.,Define a resource-based policy on the S3 bucket to allow access when a request meets the condition “aws:SecureTransport”: “false”.,Define a role-based policy on the other accounts' roles to deny access when a request meets the condition of “aws:SecureTransport”: “false”.,Define a resource-based policy on the KMS key to deny access when a request meets the condition of “aws:SecureTransport”: “false”.,"# Answer
- **Correct option:** A
- **Reason:** Defining a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false” ensures that any attempt to access the S3 bucket without using HTTPS (which is secure transport) will be blocked. This effectively enforces the requirement that all requests for retrieving data must be encrypted in transit.

# Example / Analogy
- Think of the S3 bucket as a vault that only opens when the secure lock (HTTPS) is used. If someone tries to open the vault using a regular key (HTTP), the vault will stay locked, protecting the sensitive data inside.

# Common Mistakes / Traps
- A common mistake could be selecting option B, which incorrectly allows access under insecure conditions. This could lead to data being accessible over unencrypted channels, jeopardizing the data's confidentiality and integrity.

# Memory Tip
- ""S3 policies can lock the vault against unsecure keys."""
33,"[""A developer is building a web application that uses Amazon API Gateway to expose\nan AWS Lambda function to process requests from clients. During testing, the\ndeveloper notices that the API Gateway times out even though the Lambda function\nfinishes under the set time limit.\nWhich of the following API Gateway metrics in Amazon CloudWatch can help the\ndeveloper troubleshoot the issue? (Choose two.)""]",CacheHitCount,IntegrationLatency,CacheMissCount,Latency,"# Answer
- **Correct option:** B : IntegrationLatency, D : Latency

- **Reason:** 
  - **IntegrationLatency** measures the time taken by API Gateway to call the integrated AWS service (in this case, the Lambda function). It helps the developer understand if the delay is due to the API Gateway waiting on the Lambda function's response.
  - **Latency** encompasses the total time taken for a client request to be processed by API Gateway, which includes the time taken to receive the request, call the Lambda function, and return the response. It can help identify if the timeout issue is within the API Gateway itself.

# Example / Analogy
- Think of API Gateway as a restaurant waiter (API), who takes an order, passes it to the kitchen (Lambda function), and then delivers the food back to the customer. If the waiter takes a long time to return with the food, you'd want to know if it's because the kitchen is slow (IntegrationLatency) or if the waiter took a long time to take the order or deliver it (Latency).

# Common Mistakes / Traps
- A common mistake is confusing metrics that don't directly relate to the timing of Lambda execution with those that do. For instance, **CacheHitCount** and **CacheMissCount** contribute to performance analysis, but they don't help in understanding the timing of the request processing.

# Memory Tip
- Remember: **IntegrationLatency is about the connection, while Latency is about the whole journey.**"
34,"[""A company is building a web application on AWS. When a customer sends a request,\nthe application will generate reports and then make the reports available to the\ncustomer within one hour. Reports should be accessible to the customer for 8\nhours. Some reports are larger than 1 MB. Each report is unique to the customer.\nThe application should delete all reports that are older than 2 days.\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Generate the reports and then store the reports as Amazon DynamoDB items that have a specified TTL. Generate a URL that retrieves the reports from DynamoDB. Provide the URL to customers through the web application.,Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryption. Attach the reports to an Amazon Simple Notification Service (Amazon SNS) message. Subscribe the customer to email notifications from Amazon SNS.,Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryption. Generate a presigned URL that contains an expiration date Provide the URL to customers through the web application. Add S3 Lifecycle configuration rules to the S3 bucket to delete old reports.,Generate the reports and then store the reports in an Amazon RDS database with a date stamp. Generate an URL that retrieves the reports from the RDS database. Provide the URL to customers through the web application. Schedule an hourly AWS Lambda function to delete database records that have expired date stamps.,"# Answer
- **Correct option:** C
- **Reason:** Storing the reports in an Amazon S3 bucket allows for large files and simplifies expiration management with S3 lifecycle rules. Using presigned URLs ensures secure access for customers, and the lifecycle rules automate the deletion of reports older than 2 days, minimizing operational overhead.

# Example / Analogy
- Think of Amazon S3 as a secure filing cabinet where you can store large documents. When you need a document (report), you get a special key (presigned URL) that allows you to access it for a limited time. The janitor (S3 lifecycle rules) automatically removes old documents after a certain period, so you don’t have to worry about cleaning out the cabinet yourself.

# Common Mistakes / Traps
- A common mistake is to overlook the storage service's ability to handle file size—in this case, Amazon S3 is designed for large objects, unlike DynamoDB which has limitations on item size (up to 400 KB). Also, avoid thinking that pressing for a DB solution (like RDS) is always better; it incurs more overhead because of managing connection pools and read/write operations.

# Memory Tip
- **Remember:** Use S3 for scalable storage and lifecycle rules to automate cleanup."
35,"[""A company is planning to securely manage one-time fixed license keys in AWS. The\ncompany's development team needs to access the license keys in automaton scripts\nthat run in Amazon EC2 instances and in AWS CloudFormation stacks.\nWhich solution will meet these requirements MOST cost-effectively?""]",Amazon S3 with encrypted files prefixed with “config”,AWS Secrets Manager secrets with a tag that is named SecretString,AWS Systems Manager Parameter Store SecureString parameters,CloudFormation NoEcho parameters,"# Answer
- **Correct option:** C : AWS Systems Manager Parameter Store SecureString parameters

- **Reason:** AWS Systems Manager Parameter Store allows you to securely store and manage parameters, such as license keys, in a way that developers can easily access them in their automation scripts running on EC2 instances. The SecureString parameters ensure that the sensitive data is encrypted at rest and in transit, making it a cost-effective solution since Parameter Store is free for a certain number of parameters and is straightforward to integrate into both EC2 instances and CloudFormation stacks.

# Example / Analogy
- Think of Parameter Store like a secure vault for important documents. Just as you would keep keys or sensitive papers in a safe place where only trusted individuals can access them, Parameter Store keeps your license keys safe, allowing your development team to retrieve them as needed securely.

# Common Mistakes / Traps
- A common mistake is choosing AWS Secrets Manager because it’s specifically designed for secrets management, but for simple license key storage, Parameter Store is often more cost-effective. Additionally, some might overlook the need for SecureString, thinking standard strings will suffice for sensitive data.

# Memory Tip
- Use Parameter Store as your secure vault for sensitive keys in scripts!"
36,"[""A development team wants to build a continuous integration/continuous delivery\n(CI/CD) pipeline. The team is using AWS CodePipeline to automate the code build\nand deployment. The team wants to store the program code to prepare for the\nCI/CD pipeline.\nWhich AWS service should the team use to store the program code?""]",AWS CodeDeploy,AWS CodeArtifact,AWS CodeCommit,Amazon CodeGuru,"# Answer
- **Correct option:** C : AWS CodeCommit
- **Reason:** AWS CodeCommit is a fully managed source control service that hosts secure Git repositories. It is designed specifically for developers to store and manage their code, making it an ideal choice for the development team to store their program code in preparation for the CI/CD pipeline.

# Example / Analogy
- Think of CodeCommit like a digital library where you can store your books (the code). Just as a library allows you to keep your books safe, share them with others, and manage different versions, CodeCommit allows developers to store their code securely, collaborate with team members, and manage different code versions effectively.

# Common Mistakes / Traps
- A common mistake is to confuse CodeCommit with AWS CodeDeploy or AWS CodeArtifact. CodeDeploy is for deploying applications, while CodeArtifact is for managing and sharing software packages—not for storing code repositories.

# Memory Tip
- Remember: ""CodeCommit keeps code safe, just like a library for your books!"""
37,"[""A developer is designing an AWS Lambda function that creates temporary files\nthat are less than 10 MB during invocation. The temporary files will be accessed\nand modified multiple times during invocation. The developer has no need to save\nor retrieve these files in the future.\nWhere should the temporary files be stored?""]",the /tmp directory,Amazon Elastic File System (Amazon EFS),Amazon Elastic Block Store (Amazon EBS),Amazon S3,"# Answer
- **Correct option:** A : the /tmp directory
- **Reason:** AWS Lambda provides a limited amount of temporary storage in the /tmp directory with a capacity of 512 MB. This is ideal for storing temporary files needed only during the execution of the function, especially since the files are less than 10 MB and do not need to be preserved beyond the invocation.

# Example / Analogy
- Think of the /tmp directory like a temporary workspace or desk. When you're working on a project, you might use this desk to spread out your documents and tools. Once you finish the project, you can clear the desk without worrying about saving anything because you no longer need those materials. Similarly, the /tmp directory serves as a temporary space during the execution of the Lambda function.

# Common Mistakes / Traps
- A common mistake is to choose Amazon S3, thinking it’s suitable for temporary storage, but S3 is designed for persistent storage and involves additional overhead for interactions. Also, options like EBS and EFS are used for long-term storage, which is unnecessary for temporary files.

# Memory Tip
- Temporary files = /tmp directory in AWS Lambda = quick cleanup after use!"
38,"[""A developer is designing a serverless application with two AWS Lambda functions\nto process photos. One Lambda function stores objects in an Amazon S3 bucket and\nstores the associated metadata in an Amazon DynamoDB table. The other Lambda\nfunction fetches the objects from the S3 bucket by using the metadata from the\nDynamoDB table. Both Lambda functions use the same Python library to perform\ncomplex computations and are approaching the quota for the maximum size of\nzipped deployment packages.\nWhat should the developer do to reduce the size of the Lambda deployment\npackages with the LEAST operational overhead?""]",Package each Python library in its own .zip file archive. Deploy each Lambda function with its own copy of the library.,Create a Lambda layer with the required Python library. Use the Lambda layer in both Lambda functions.,Combine the two Lambda functions into one Lambda function. Deploy the Lambda function as a single .zip file archive.,Download the Python library to an S3 bucket. Program the Lambda functions to reference the object URLs.,"# Answer
- **Correct option:** B : Create a Lambda layer with the required Python library. Use the Lambda layer in both Lambda functions.
- **Reason:** Using a Lambda layer allows you to store the shared library separately from the Lambda function deployment package. This reduces the size of the individual deployment packages, as the library is maintained separately and can be reused across multiple functions, which is ideal when the same library is required in more than one function.

# Example / Analogy
- Real-life example or memorable comparison: Think of Lambda layers like a cooking class where several chefs need to use the same set of tools (e.g., knives, pans). Instead of each chef bringing their own set of tools (which would take up space), they can use a shared kitchen tool set located in one area. This way, they focus on their dishes (the main tasks of each Lambda function) while keeping their workspaces (deployment packages) less cluttered.

# Common Mistakes / Traps
- Common trap points, if any: A common mistake is assuming that each Lambda function must contain its own copy of a library. This increases deployment package size unnecessarily. Another trap is thinking that simply combining the functions or downloading libraries at runtime will resolve size issues without operational overhead or complexity.

# Memory Tip
- Use Lambda layers to share code efficiently and reduce deployment package size."
39,"[""A developer is working on a serverless application that needs to process any\nchanges to an Amazon DynamoDB table with an AWS Lambda function.\nHow should the developer configure the Lambda function to detect changes to the\nDynamoDB table?""]","Create an Amazon Kinesis data stream, and attach it to the DynamoDB table. Create a trigger to connect the data stream to the Lambda function.",Create an Amazon EventBridge rule to invoke the Lambda function on a regular schedule. Conned to the DynamoDB table from the Lambda function to detect changes.,Enable DynamoDB Streams on the table. Create a trigger to connect the DynamoDB stream to the Lambda function.,"Create an Amazon Kinesis Data Firehose delivery stream, and attach it to the DynamoDB table. Configure the delivery stream destination as the Lambda function.","# Answer
- **Correct option:** C
- **Reason:** Enabling DynamoDB Streams on the table captures changes to the items in the table, and by creating a trigger that connects the DynamoDB Streams to the Lambda function, the function can be automatically invoked whenever changes occur in the table (like inserts, updates, or deletes).

# Example / Analogy
- Think of DynamoDB Streams as a live feed of all changes happening in a store. If you imagine a café where every order placed, modified, or canceled is written down on a board, the Lambda function is like a barista who gets immediately notified whenever the board is updated, allowing them to adjust their tasks in real-time based on customer demand.

# Common Mistakes / Traps
- A common error is to confuse DynamoDB Streams with alternative data ingestion services like Kinesis. Many people might think they should set up Kinesis for streaming changes, but using DynamoDB Streams is more efficient and directly tied to tracking changes in DynamoDB.

# Memory Tip
- Remember: **DynamoDB Streams = Real-time operation updates** for triggering actions with Lambda."
40,"[""A developer is creating an AWS Lambda function that needs credentials to connect\nto an Amazon RDS for MySQL database. An Amazon S3 bucket currently stores the\ncredentials. The developer needs to improve the existing solution by\nimplementing credential rotation and secure storage. The developer also needs to\nprovide integration with the Lambda function.\nWhich solution should the developer use to store and retrieve the credentials\nwith the LEAST management overhead?""]",Store the credentials in AWS Systems Manager Parameter Store. Select the database that the parameter will access. Use the default AWS Key Management Service (AWS KMS) key to encrypt the parameter. Enable automatic rotation for the parameter. Use the parameter from Parameter Store on the Lambda function to connect to the database.,"Encrypt the credentials with the default AWS Key Management Service (AWS KMS) key. Store the credentials as environment variables for the Lambda function. Create a second Lambda function to generate new credentials and to rotate the credentials by updating the environment variables of the first Lambda function. Invoke the second Lambda function by using an Amazon EventBridge rule that runs on a schedule. Update the database to use the new credentials. On the first Lambda function, retrieve the credentials from the environment variables. Decrypt the credentials by using AWS KMS, Connect to the database.",Store the credentials in AWS Secrets Manager. Set the secret type to Credentials for Amazon RDS database. Select the database that the secret will access. Use the default AWS Key Management Service (AWS KMS) key to encrypt the secret. Enable automatic rotation for the secret. Use the secret from Secrets Manager on the Lambda function to connect to the database.,Encrypt the credentials by using AWS Key Management Service (AWS KMS). Store the credentials in an Amazon DynamoDB table. Create a second Lambda function to rotate the credentials. Invoke the second Lambda function by using an Amazon EventBridge rule that runs on a schedule. Update the DynamoDB table. Update the database to use the generated credentials. Retrieve the credentials from DynamoDB with the first Lambda function. Connect to the database.,"# Answer
- **Correct option:** C
- **Reason:** AWS Secrets Manager is specifically designed for managing and storing sensitive information like database credentials securely. It provides built-in support for automatic credential rotation without additional management overhead, simplifying integration with AWS Lambda functions.

# Example / Analogy
- Think of AWS Secrets Manager as a high-security vault that automatically changes its lock (the credentials) regularly without you having to physically change the keys (credentials) yourself. Just like a bank updates its security measures to ensure safety, Secrets Manager updates the credentials while you continue to access the vault (database) seamlessly.

# Common Mistakes / Traps
- A common mistake would be to choose options that involve more management overhead, like creating additional Lambda functions for rotation or manually handling encryption and storage in places like S3 or DynamoDB, which can lead to complexity and higher chances of error. Many might also underestimate the importance of automatic rotation and secure storage features that Secret Manager provides.

# Memory Tip
- Use Secrets Manager for secrets—it's the purpose-built tool that reduces overhead and simplifies secure management."
41,"[""A developer has written the following IAM policy to provide access to an Amazon\nS3 bucket:\n[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image3.png]\nWhich access does the policy allow regarding the s3:GetObject and s3:PutObject\nactions?""]",Access on all buckets except the “DOC-EXAMPLE-BUCKET” bucket,Access on all buckets that start with “DOC-EXAMPLE-BUCKET” except the “DOC-EXAMPLE-BUCKET/secrets” bucket,Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket along with access to all S3 actions for objects in the “DOC-EXAMPLE-BUCKET” bucket that start with “secrets”,Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket except on objects that start with “secrets”,"# Answer
- **Correct option:** D
- **Reason:** The IAM policy allows access to all objects in the ""DOC-EXAMPLE-BUCKET"" bucket except for those objects whose keys start with ""secrets"". This is specified in the policy with the Action `s3:GetObject` and `s3:PutObject` allowed for all objects in the bucket, with a condition that denies specific access for objects prefixed with ""secrets"".

# Example / Analogy
- Think of the ""DOC-EXAMPLE-BUCKET"" as a library where you can borrow any book (objects) you want, but there is a special section labeled ""Secrets"" (objects starting with ""secrets""). You have a sign that states you cannot access any books in that special section. This way, while you have access to the majority of the library (the bucket), the restriction prevents you from obtaining specific information that is confidential.

# Common Mistakes / Traps
- A common mistake is to misunderstand the effect of the S3 policy in relation to object prefixes. People might mistakenly think that the restriction on ""secrets"" applies to all actions or the entire bucket, rather than just objects starting with that prefix.

# Memory Tip
- Remember: ""All but the Secrets"" when allowing permissions for S3 objects—you're granted access, but only certain items are off-limits."
42,"[""An Amazon Kinesis Data Firehose delivery stream is receiving customer data that\ncontains personally identifiable information. A developer needs to remove\npattern-based customer identifiers from the data and store the modified data in\nan Amazon S3 bucket.\nWhat should the developer do to meet these requirements?""]",Implement Kinesis Data Firehose data transformation as an AWS Lambda function. Configure the function to remove the customer identifiers. Set an Amazon S3 bucket as the destination of the delivery stream.,Launch an Amazon EC2 instance. Set the EC2 instance as the destination of the delivery stream. Run an application on the EC2 instance to remove the customer identifiers. Store the transformed data in an Amazon S3 bucket.,Create an Amazon OpenSearch Service instance. Set the OpenSearch Service instance as the destination of the delivery stream. Use search and replace to remove the customer identifiers. Export the data to an Amazon S3 bucket.,"Create an AWS Step Functions workflow to remove the customer identifiers. As the last step in the workflow, store the transformed data in an Amazon S3 bucket. Set the workflow as the destination of the delivery stream.","# Answer
- **Correct option:** A 
- **Reason:** Implementing Kinesis Data Firehose data transformation as an AWS Lambda function allows for the on-the-fly modification of incoming data streams. The Lambda function can be configured specifically to remove customer identifiers directly before the data is sent to Amazon S3, thus effectively meeting the requirement to remove personally identifiable information.

# Example / Analogy
- Think of Kinesis Data Firehose like a conveyor belt carrying boxes of goods (the data). In this scenario, the Lambda function acts like a quality control station along the conveyor belt that inspects each box and removes any sensitive items before they reach the storage area (Amazon S3). This ensures only safe, modified boxes are stored.

# Common Mistakes / Traps
- A common pitfall is choosing solutions that involve additional, unnecessary services (like EC2 or Step Functions) that complicate the architecture. The goal is to efficiently handle stream data processing without introducing more components than needed.

# Memory Tip
- **Lambda is your data filter: remove sensitive info on-the-go!**"
43,"[""An application uses Lambda functions to extract metadata from files uploaded to\nan S3 bucket; the metadata is stored in Amazon DynamoDB. The application starts\nbehaving unexpectedly, and the developer wants to examine the logs of the Lambda\nfunction code for errors.\nBased on this system configuration, where would the developer find the logs?""]",Amazon S3,AWS CloudTrail,Amazon CloudWatch,Amazon DynamoDB,"# Answer
- **Correct option:** C : Amazon CloudWatch
- **Reason:** AWS Lambda automatically integrates with Amazon CloudWatch to log output and error messages generated by your Lambda functions. When a Lambda function fails or logs a statement, this information is sent to CloudWatch Logs, where developers can access it for debugging and monitoring.

# Example / Analogy
- Consider CloudWatch as a security camera for your Lambda function—it records everything that happens during execution (like logs) so you can review and diagnose issues later.

# Common Mistakes / Traps
- A common mistake is thinking that logs are stored in S3 or that CloudTrail (which records API activity) would provide the specific logs of the Lambda function’s execution. CloudTrail captures management events but not the runtime execution logs of Lambda.

# Memory Tip
- Think ""CloudWatch for checking what went wrong in my Lambda."""
44,"[""A company is using an AWS Lambda function to process records from an Amazon\nKinesis data stream. The company recently observed slow processing of the\nrecords. A developer notices that the iterator age metric for the function is\nincreasing and that the Lambda run duration is constantly above normal.\nWhich actions should the developer take to increase the processing speed?\n(Choose two.)""]",Increase the number of shards of the Kinesis data stream.,Decrease the timeout of the Lambda function.,Increase the memory that is allocated to the Lambda function.,Decrease the number of shards of the Kinesis data stream.,"# Answer
- **Correct options:** A, C  

- **Reason:**  
  Option A (Increase the number of shards of the Kinesis data stream) effectively allows for more parallel processing of records, enabling multiple Lambda instances to consume from the stream at the same time. The iterator age metric indicates how old the records in the stream are compared to the current processing point; if it's increasing, it suggests that Lambda is not keeping up with the incoming data. By increasing the number of shards, you can distribute the load more efficiently.  
  Option C (Increase the memory that is allocated to the Lambda function) not only increases the available memory but can also lead to a higher CPU allocation, which can reduce processing time for each function invocation. When you boost memory for Lambda, it can handle more complex processing or run faster, reducing the overall duration of function executions.

# Example / Analogy
- Think of a highway with multiple lanes (shards) where cars (data records) travel. If the highway has too few lanes, traffic slows down, leading to cars piling up (increased iterator age). Adding more lanes (shards) allows more cars to travel at once, improving overall traffic speed. Similarly, if one car (Lambda function) isn't powerful enough (limited memory), it can't accelerate effectively; giving it a more powerful engine (more memory) helps it go faster.

# Common Mistakes / Traps
- A common mistake is to assume that decreasing the timeout or the number of shards will help. Decreasing the timeout (Option B) only means the Lambda will stop processing sooner, which does not resolve the underlying problem of processing speed. Additionally, decreasing the number of shards (Option D) contradicts the goal of improving throughput.

# Memory Tip
- **More shards = more lanes; more memory = a more powerful engine.**"
45,"[""A developer is incorporating AWS X-Ray into an application that handles personal\nidentifiable information (PII). The application is hosted on Amazon EC2\ninstances. The application trace messages include encrypted PII and go to Amazon\nCloudWatch. The developer needs to ensure that no PII goes outside of the EC2\ninstances.\nWhich solution will meet these requirements?""]",Manually instrument the X-Ray SDK in the application code.,Use the X-Ray auto-instrumentation agent.,Use Amazon Macie to detect and hide PII. Call the X-Ray API from AWS Lambda.,Use AWS Distro for Open Telemetry.,"# Answer
- **Correct option:** A: Manually instrument the X-Ray SDK in the application code.

- **Reason:** Manually instrumenting the X-Ray SDK allows the developer to have complete control over what data is sent to AWS X-Ray, ensuring that encrypted PII can be excluded from trace data. By selectively including trace messages that do not contain any PII, the developer can ensure compliance with privacy requirements and prevent sensitive information from going outside the EC2 instances.

# Example / Analogy
- Think of it like manually deciding what to share on social media versus automatically sharing everything from your phone. If you manually select which photos to post (like manually instrumenting the X-Ray SDK), you ensure that no personal or sensitive images (PII) are shared publicly. Automatic sharing might include everything, potentially compromising your privacy.

# Common Mistakes / Traps
- A common trap in this type of question is assuming that auto-instrumentation (Option B) is safer. While it is quicker and easier to set up, it may capture all data, including sensitive PII, unless carefully configured. Similarly, using services like AWS Lambda to process data (Option C) does not inherently prevent PII from being sent outside EC2 instances, as the data received may still include sensitive information.

# Memory Tip
- Manually control your data flow to keep your PII personal and protected!"
46,"[""A company needs to harden its container images before the images are in a\nrunning state. The company's application uses Amazon Elastic Container Registry\n(Amazon ECR) as an image registry. Amazon Elastic Kubernetes Service (Amazon\nEKS) for compute, and an AWS CodePipeline pipeline that orchestrates a\ncontinuous integration and continuous delivery (CI/CD) workflow.\nDynamic application security testing occurs in the final stage of the pipeline\nafter a new image is deployed to a development namespace in the EKS cluster. A\ndeveloper needs to place an analysis stage before this deployment to analyze the\ncontainer image earlier in the CI/CD pipeline.\nWhich solution will meet these requirements with the MOST operational\nefficiency?""]",Build the container image and run the docker scan command locally. Mitigate any findings before pushing changes to the source code repository. Write a pre-commit hook that enforces the use of this workflow before commit.,Create a new CodePipeline stage that occurs after the container image is built. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.,Create a new CodePipeline stage that occurs after source code has been retrieved from its repository. Run a security scanner on the latest revision of the source code. Fail the pipeline if there are findings.,Add an action to the deployment stage of the pipeline so that the action occurs before the deployment to the EKS cluster. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.,"# Answer
- **Correct option:** B
- **Reason:** This option creates a dedicated stage in the CodePipeline that scans the container image immediately after it is built and before it is deployed. By utilizing ECR's built-in image scanning capability, the solution ensures that container vulnerabilities are identified early. The Lambda function can automate the process of checking the scan results and stopping the pipeline if any issues are detected, providing a robust security check without manual intervention.

# Example / Analogy
- Think of this as a safety inspection for a vehicle before it hits the road. Just like a thorough inspection can identify issues with the brakes or engines before the car is used, the image scanning step identifies security vulnerabilities before the container is deployed. This proactive approach prevents problematic containers from running in a production environment.

# Common Mistakes / Traps
- A common mistake could be thinking that scanning after the deployment is sufficient, or assuming local scanning is enough for a CI/CD pipeline. Both of these overlook the efficiencies and automation provided by integrating the scan before deployment in the pipeline.

# Memory Tip
- Always scan your images *before* deployment for early detection of vulnerabilities—secure from the start!"
47,"[""A developer is testing a new file storage application that uses an Amazon\nCloudFront distribution to serve content from an Amazon S3 bucket. The\ndistribution accesses the S3 bucket by using an origin access identity (OAI).\nThe S3 bucket's permissions explicitly deny access to all other users.\nThe application prompts users to authenticate on a login page and then uses\nsigned cookies to allow users to access their personal storage directories. The\ndeveloper has configured the distribution to use its default cache behavior with\nrestricted viewer access and has set the origin to point to the S3 bucket.\nHowever, when the developer tries to navigate to the login page, the developer\nreceives a 403 Forbidden error.\nThe developer needs to implement a solution to allow unauthenticated access to\nthe login page. The solution also must keep all private content secure.\nWhich solution will meet these requirements?""]","Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to the path of the login page, and make viewer access unrestricted. Keep the default cache behavior's settings unchanged.","Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to *, and make viewer access restricted. Change the default cache behavior's path pattern to the path of the login page, and make viewer access unrestricted.","Add a second origin as a failover origin to the default cache behavior. Point the failover origin to the S3 bucket. Set the path pattern for the primary origin to *, and make viewer access restricted. Set the path pattern for the failover origin to the path of the login page, and make viewer access unrestricted.",Add a bucket policy to the S3 bucket to allow read access. Set the resource on the policy to the Amazon Resource Name (ARN) of the login page object in the S3 bucket. Add a CloudFront function to the default cache behavior to redirect unauthorized requests to the login page's S3 URL.,"# Answer
- **Correct option:** A
- **Reason:** Option A suggests adding a second cache behavior to the CloudFront distribution that points specifically to the login page with unrestricted viewer access. This allows anyone to access the login page while keeping the default cache behavior (which is restricted) for all other content, ensuring that private content remains secure.

# Example / Analogy
- Imagine you run a secure building with a main entrance that requires keycard access (like the restricted access for private content). However, you also have a public lobby area (the login page) where anyone can enter without restriction. By setting up a separate entrance just for the lobby (the second cache behavior), you allow unauthenticated users to access that area while keeping the main building secure.

# Common Mistakes / Traps
- A common trap is to think that simply adjusting the default cache behavior will solve the problem. However, that would not grant access to the login page for unauthenticated users, leading to a 403 error. It's essential to identify that a targeted solution (like a separate cache behavior) is needed for public access.

# Memory Tip
- **One behavior for access, another for security!**"
48,"[""A developer is using AWS Amplify Hosting to build and deploy an application. The\ndeveloper is receiving an increased number of bug reports from users. The\ndeveloper wants to add end-to-end testing to the application to eliminate as\nmany bugs as possible before the bugs reach production.\nWhich solution should the developer implement to meet these requirements?""]",Run the amplify add test command in the Amplify CLI.,Create unit tests in the application. Deploy the unit tests by using the amplify push command in the Amplify CLI.,Add a test phase to the amplify.yml build settings for the application.,Add a test phase to the aws-exports.js file for the application.,"# Answer
- **Correct option:** C
- **Reason:** Adding a test phase to the `amplify.yml` build settings allows you to automate end-to-end testing during the build process on AWS Amplify Hosting. This ensures that tests are run each time you deploy the application, helping to catch bugs before the application reaches production.

# Example / Analogy
- Think of `amplify.yml` build settings like a chef's recipe that includes a baking step (test phase) at the end. If the chef doesn't taste the dish (run tests) before serving it, they might deliver a flawed dish (application) to the customers (users). Having a test phase ensures that the dish is perfect before it gets to the table.

# Common Mistakes / Traps
- A common mistake is to think that adding unit tests (Choice B) is sufficient on its own. While unit tests are important, end-to-end tests cover broader functionality and integration, ensuring the entire application works well together.

# Memory Tip
- ""Test early, test often"" to remember that integrating testing into build settings is essential for quality."
49,"[""A company has deployed an application on AWS Elastic Beanstalk. The company has\nconfigured the Auto Scaling group that is associated with the Elastic Beanstalk\nenvironment to have five Amazon EC2 instances. If the capacity is fewer than\nfour EC2 instances during the deployment, application performance degrades. The\ncompany is using the all-at-once deployment policy.\nWhat is the MOST cost-effective way to solve the deployment issue?""]",Change the Auto Scaling group to six desired instances.,Change the deployment policy to traffic splitting. Specify an evaluation time of 1 hour.,Change the deployment policy to rolling with additional batch. Specify a batch size of 1.,Change the deployment policy to rolling. Specify a batch size of 2.,"# Answer
- **Correct option:** C : Change the deployment policy to rolling with additional batch. Specify a batch size of 1.

- **Reason:** Changing the deployment policy to ""rolling with additional batch"" and setting the batch size to 1 allows one instance to be updated at a time while maintaining a minimum of four instances available. This ensures that the application has enough capacity to handle traffic without degrading performance during deployments, thus addressing the issue of application performance degradation.

# Example / Analogy
- Imagine a bakery that gets busy every day. If the bakery decides to renovate all its counters at once (like the all-at-once deployment), it might not have enough space to serve customers, resulting in long lines and unhappy customers. Instead, if the bakery updates one counter at a time (similar to rolling deployment with batch size of 1), it can continue serving customers and won't cause a bottleneck, ensuring happy customers and continued sales.

# Common Mistakes / Traps
- A common mistake is choosing the ""all-at-once"" deployment because it sounds quicker but leads to the performance issue. Additionally, some may mistakenly think that increasing the number of instances is the only solution without realizing that the deployment strategy is also critical for maintaining performance.

# Memory Tip
- Aim to ""roll"" one at a time for smoother transitions during deployments."
50,"[""A company is building a serverless application that uses AWS Lambda functions.\nThe company needs to create a set of test events to test Lambda functions in a\ndevelopment environment. The test events will be created once and then will be\nused by all the developers in an IAM developer group. The test events must be\neditable by any of the IAM users in the IAM developer group.\n\nWhich solution will meet these requirements?""]",Create and store the test events in Amazon S3 as JSON objects. Allow S3 bucket access to all IAM users.,Create the test events. Configure the event sharing settings to make the test events shareable.,Create and store the test events in Amazon DynamoDB. Allow access to DynamoDB by using IAM roles.,Create the test events. Configure the event sharing settings to make the test events private.,"# Answer
- **Correct option:** A

- **Reason:** Storing the test events in an Amazon S3 bucket as JSON objects allows all IAM users in the developer group to access and edit the test events as needed. S3 is designed for easy sharing and storage of files and provides fine-grained access control through IAM policies. Once the events are stored, any developer in the appropriate group can read, modify, and reuse these events without conflicts.

# Example / Analogy
- Think of S3 as a shared drive where all developers store and collaborate on project documents. By placing your test events there (much like filing docs in a shared folder), everyone can retrieve, edit, and share their updates seamlessly, ensuring everyone is working with the same information.

# Common Mistakes / Traps
- A common mistake might be thinking that event sharing settings (as mentioned in options B and D) can facilitate direct collaboration like S3 does. However, Lambda test events do not have built-in sharing features like S3. Additionally, confusing DynamoDB (option C) as an appropriate place for test event storage may lead to unnecessary complexity and cost since S3 is simpler and more suited for this use case.

# Memory Tip
- ""For collaborative storage and easy access, think S3 – the shared drive of AWS."""
51,"[""An engineer created an A/B test of a new feature on an Amazon CloudWatch\nEvidently project. The engineer configured two variations of the feature\n(Variation A and Variation B) for the test. The engineer wants to work\nexclusively with Variation A. The engineer needs to make updates so that\nVariation A is the only variation that appears when the engineer hits the\napplication's endpoint.\n\nWhich solution will meet this requirement?""]",Add an override to the feature. Set the identifier of the override to the engineer's user ID. Set the variation to Variation A.,Add an override to the feature. Set the identifier of the override to Variation A. Set the variation to 100%.,Add an experiment to the project. Set the identifier of the experiment to Variation B. Set the variation to 0%.,Add an experiment to the project. Set the identifier of the experiment to the AWS account's account ISet the variation to Variation A.,"# Answer
- **Correct option:** A
- **Reason:** Adding an override with the engineer's user ID ensures that the application will always serve Variation A to that specific user, effectively bypassing the A/B test for them. This approach allows testing variations without impacting other users by forcing the desired variation for a designated user.

# Example / Analogy
- Think of it like a restaurant where you’re conducting a taste test for two new dishes (Variation A and Variation B). If you want to ensure one specific customer always receives their favorite dish (Variation A), you could create a note in the kitchen that says, ""Always serve dish A to this customer."" This way, they get what they prefer while you continue to experiment with others.

# Common Mistakes / Traps
- A common mistake is to confuse the variations with user identifiers. Option B suggests setting the variation to 100%, which might seem like a good solution, but it does not target specific users and might confuse the overall testing framework, reducing the integrity of the A/B test.

# Memory Tip
- **Override with user ID** to give specific users their desired variation without disrupting the A/B test for others."