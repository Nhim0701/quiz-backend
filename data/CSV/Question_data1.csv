No,Content,A,B,C,D,Answered
2,"[""A company wants to share information with a third party. The third party has an\nHTTP API endpoint that the company can use to share the information. The company\nhas the required API key to access the HTTP API.\nThe company needs a way to manage the API key by using code. The integration of\nthe API key with the application code cannot affect application performance.\nWhich solution will meet these requirements MOST securely?""]",Store the API credentials in AWS Secrets Manager. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,Store the API credentials in a local code variable. Push the code to a secure Git repository. Use the local code variable at runtime to make the API call.,Store the API credentials as an object in a private Amazon S3 bucket. Restrict access to the S3 object by using IAM policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,Store the API credentials in an Amazon DynamoDB table. Restrict access to the table by using resource-based policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,"# Answer
- **Correct option:** A

- **Reason:** Storing the API credentials in AWS Secrets Manager allows for secure management and retrieval of sensitive information like API keys. Secrets Manager automatically handles encryption, access control, and key rotation, while also providing easy access via the AWS SDK, ensuring that the application performance remains unaffected at runtime. This approach is also in line with best practices for security and compliance.

# Example / Analogy
- Think of AWS Secrets Manager like a secure vault where you can store your valuable jewels (API keys). Just like you wouldn’t want to carry your expensive jewelry around in the open, secrets should not be hardcoded in your application. Instead, they should be stored securely and accessed only when needed.

# Common Mistakes / Traps
- A common mistake is to think that storing sensitive information directly in application code (like choice B) or simply using a less secure method (like S3 or DynamoDB without added security features) is sufficient. This goes against security best practices and can expose sensitive information to unintended access.

# Memory Tip
- Always store secrets securely, not in code; use AWS Secrets Manager for best security practices!"
3,"[""A company wants to share information with a third party. The third party has an\nHTTP API endpoint that the company can use to share the information. The company\nhas the required API key to access the HTTP API.\nThe company needs a way to manage the API key by using code. The integration of\nthe API key with the application code cannot affect application performance.\nWhich solution will meet these requirements MOST securely?""]",Store the API credentials in AWS Secrets Manager. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,Store the API credentials in a local code variable. Push the code to a secure Git repository. Use the local code variable at runtime to make the API call.,Store the API credentials as an object in a private Amazon S3 bucket. Restrict access to the S3 object by using IAM policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,Store the API credentials in an Amazon DynamoDB table. Restrict access to the table by using resource-based policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.,"# Answer
- **Correct option:** A
- **Reason:** Storing the API key in AWS Secrets Manager provides a secure and controlled way to manage sensitive information. This method allows for automatic retrieval of the credentials at runtime without compromising application performance. Secrets Manager also offers built-in encryption and fine-grained access control, ensuring that only authorized entities can access the API key.

# Example / Analogy
- Think of AWS Secrets Manager as a secure vault where you can store your valuables (like API keys) along with instructions on how to access them. Just as you wouldn’t keep your valuables in plain sight at home, you shouldn’t hardcode sensitive data (like API keys) in your application code. Instead, you can ""check out"" what you need from the vault when you need it, ensuring only the right people can access it.

# Common Mistakes / Traps
- A common trap is to consider local variables or configuration files for storing API keys, as this can lead to unintentional exposure or hardcoding of credentials in the source code. Many developers might think pushing code to a secure Git repository keeps it safe, but repositories can be inadvertently accessed by unauthorized users or leaked.

# Memory Tip
- **""Use the vault for your secrets: store them securely in Secrets Manager!""**"
4,"[""A developer is deploying a new application to Amazon Elastic Container Service\n(Amazon ECS). The developer needs to securely store and retrieve different types\nof variables. These variables include authentication information for a remote\nAPI, the URL for the API, and credentials. The authentication information and\nAPI URL must be available to all current and future deployed versions of the\napplication across development, testing, and production environments.\nHow should the developer retrieve the variables with the FEWEST application\nchanges?""]",Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.,Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment.,Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment.,Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process.,"# Answer
- **Correct option:** A
- **Reason:** Option A suggests using AWS Systems Manager Parameter Store for application variables and AWS Secrets Manager for credentials. This approach allows for secure storage and retrieval of sensitive information with minimal application code changes across different environments (development, testing, and production). Parameter Store offers straightforward integration with ECS and maintains a single source of truth for all environments, ensuring consistency.

# Example / Analogy
- Think of Parameter Store like a secure vault for your app's keys. Just as a hotel might have a single lock system for all rooms, you keep all environment variables in the same “vault”. You simply call out to that vault when you need a key (variable), and it provides the right one based on what room (environment) you are currently in.

# Common Mistakes / Traps
- A common mistake is assuming that each environment needs separate code or hardcoding values into the application, which complicates deployments and can lead to security risks. Option B might seem appealing with AWS KMS but is not really designed for easy retrieval of config values like Parameter Store or Secrets Manager.

# Memory Tip
- AWS Parameter Store is the vault for variables; use it to avoid hardcoding and control access securely."
5,"[""A developer is creating an application that will give users the ability to store\nphotos from their cellphones in the cloud. The application needs to support tens\nof thousands of users. The application uses an Amazon API Gateway REST API that\nis integrated with AWS Lambda functions to process the photos. The application\nstores details about the photos in Amazon DynamoDB.\nUsers need to create an account to access the application. In the application,\nusers must be able to upload photos and retrieve previously uploaded photos. The\nphotos will range in size from 300 KB to 5 MB.\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos and details in the DynamoDB table. Retrieve previously uploaded photos directly from the DynamoDB table.,Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.,Create an IAM user for each user of the application during the sign-up process. Use IAM authentication to access the API Gateway API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.,Create a users table in DynamoDB. Use the table to manage user accounts. Create a Lambda authorizer that validates user credentials against the users table. Integrate the Lambda authorizer with API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as par of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.,"# Answer
- **Correct option:** B
- **Reason:** Option B uses Amazon Cognito user pools to manage user accounts, which simplifies user management and authentication, especially for applications that need to scale to tens of thousands of users. It also leverages Amazon S3 for storing photos, which is the ideal solution for handling large file uploads efficiently. Storing the S3 key in DynamoDB minimizes the operational overhead while enabling quick retrieval of the photos.

# Example / Analogy
- Think of Amazon S3 as a warehouse where all your photos are stored, and DynamoDB as an indexed catalog that tells you where each photo is located in the warehouse. Instead of digging through the warehouse (S3) to find a photo, you just look it up in the catalog (DynamoDB) for quick access.

# Common Mistakes / Traps
- A common trap is to try to store the photos in DynamoDB directly, which is not efficient given DynamoDB's limitations on item size and costs related to storage. Also, relying on IAM user creation for each individual can lead to immense management overhead and loss of scalability.

# Memory Tip
- ""Cognito for accounts, S3 for storage, DynamoDB for indexing."""
6,"[""A company receives food orders from multiple partners. The company has a\nmicroservices application that uses Amazon API Gateway APIs with AWS Lambda\nintegration. Each partner sends orders by calling a customized API that is\nexposed through API Gateway. The API call invokes a shared Lambda function to\nprocess the orders.\nPartners need to be notified after the Lambda function processes the orders.\nEach partner must receive updates for only the partner's own orders. The company\nwants to add new partners in the future with the fewest code changes possible.\nWhich solution will meet these requirements in the MOST scalable way?""]",Create a different Amazon Simple Notification Service (Amazon SNS) topic for each partner. Configure the Lambda function to publish messages for each partner to the partner's SNS topic.,Create a different Lambda function for each partner. Configure the Lambda function to notify each partner's service endpoint directly.,Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure the Lambda function to publish messages with specific attributes to the SNS topic. Subscribe each partner to the SNS topic. Apply the appropriate filter policy to the topic subscriptions.,Create one Amazon Simple Notification Service (Amazon SNS) topic. Subscribe all partners to the SNS topic.,"# Answer
- **Correct option:** C

- **Reason:** This solution allows for scalability by using a single Amazon Simple Notification Service (SNS) topic that can accommodate many partners easily. By publishing messages with specific attributes, the Lambda function can differentiate notifications for each partner. Each partner can subscribe to the SNS topic with a filter policy that ensures they only receive updates relevant to their own orders. This setup reduces the need for multiple resources and code changes when adding new partners.

# Example / Analogy
- Imagine a post office where each partner is a house that receives mail. Instead of building separate mailboxes for each house (which would be cumbersome), the post office has one central mailbox (the SNS topic). Letters dispatched from the post office have specific addresses (attributes). When a new house moves in (a new partner), the post office simply adds their address to the mailing list without needing a new mailbox or significant changes to the system.

# Common Mistakes / Traps
- A common mistake would be to choose option A and create separate SNS topics for each partner. While this meets the requirement, it does not scale well since every new partner would necessitate additional topics, complicating configuration and management. Options B and D also fall short as they either lead to increased complex configurations or all partners receiving every notification without filtering.

# Memory Tip
- Think of SNS like a central post office: one topic, many recipients, filtered mail."
7,"[""A developer has written an AWS Lambda function. The function is CPU-bound. The\ndeveloper wants to ensure that the function returns responses quickly.\nHow can the developer improve the function's performance?""]",Increase the function's CPU core count.,Increase the function's memory.,Increase the function's reserved concurrency.,Increase the function's timeout.,"# Answer
- **Correct option:** B : Increase the function's memory.

- **Reason:** In AWS Lambda, the CPU and memory allocation are linked. By increasing the memory for a Lambda function, you also increase the CPU resources allocated to it. For CPU-bound tasks, having more memory means that the function will have more CPU power available, allowing it to process tasks more quickly and efficiently.

# Example / Analogy
- Think of a chef in a kitchen where the number of stoves (CPU) is limited by the kitchen's size (memory). If the chef has access to more space, they can set up more stoves, cook more dishes at once, and thus serve the customers faster. By increasing the memory allocated to the Lambda function, you're effectively allowing it more ""stoves"" to work with, speeding up the processing time.

# Common Mistakes / Traps
- A common mistake is selecting option A (Increase the function's CPU core count). While it seems logical to want more CPU power, AWS Lambda does not allow you to directly adjust the number of CPU cores; it allows you to adjust memory, which indirectly increases CPU allocation based on the memory size.

# Memory Tip
- **More memory = more CPU power in Lambda!**"
8,"[""A company is building a serverless application on AWS. The application uses an\nAWS Lambda function to process customer orders 24 hours a day, 7 days a week.\nThe Lambda function calls an external vendor's HTTP API to process payments.\nDuring load tests, a developer discovers that the external vendor payment\nprocessing API occasionally times out and returns errors. The company expects\nthat some payment processing API calls will return errors.\nThe company wants the support team to receive notifications in near real time\nonly when the payment processing external API error rate exceed 5% of the total\nnumber of transactions in an hour. Developers need to use an existing Amazon\nSimple Notification Service (Amazon SNS) topic that is configured to notify the\nsupport team.\nWhich solution will meet these requirements?""]",Write the results of payment processing API calls to Amazon CloudWatch. Use Amazon CloudWatch Logs Insights to query the CloudWatch logs. Schedule the Lambda function to check the CloudWatch logs and notify the existing SNS topic.,Publish custom metrics to CloudWatch that record the failures of the external payment processing API calls. Configure a CloudWatch alarm to notify the existing SNS topic when error rate exceeds the specified rate.,Publish the results of the external payment processing API calls to a new Amazon SNS topic. Subscribe the support team members to the new SNS topic.,Write the results of the external payment processing API calls to Amazon S3. Schedule an Amazon Athena query to run at regular intervals. Configure Athena to send notifications to the existing SNS topic when the error rate exceeds the specified rate.,"# Answer
- **Correct option:** B
- **Reason:** Publishing custom metrics to Amazon CloudWatch allows the application to track error rates by capturing failures specifically from the payment processing API calls. A CloudWatch alarm can be configured to monitor these metrics and can trigger a notification via the existing SNS topic if the error rate exceeds 5%, meeting the company's requirements for near real-time alerts based on defined thresholds.

# Example / Analogy
- Imagine you're managing a restaurant's orders, and you set up a system that logs all customer complaints. If more than 5% of the customers report issues with orders in an hour, you want to be notified immediately to address the problem. By continuously tracking complaints (similar to CloudWatch metrics), you can swiftly react if the business's performance dips.

# Common Mistakes / Traps
- A common mistake is to think that simply logging the results (as in option A) or storing results in S3 (as in option D) is adequate for monitoring error rates. While these methods can provide data, they don’t facilitate immediate alerting and monitoring of key metrics, which is critical for timely response.

# Memory Tip
- Track key metrics with CloudWatch to alert swiftly; don't rely on logs or delayed queries."
9,"[""A company is offering APIs as a service over the internet to provide\nunauthenticated read access to statistical information that is updated daily.\nThe company uses Amazon API Gateway and AWS Lambda to develop the APIs. The\nservice has become popular, and the company wants to enhance the responsiveness\nof the APIs.\nWhich action can help the company achieve this goal?""]",Enable API caching in API Gateway.,Configure API Gateway to use an interface VPC endpoint.,Enable cross-origin resource sharing (CORS) for the APIs.,Configure usage plans and API keys in API Gateway.,"# Answer
- **Correct option:** A : Enable API caching in API Gateway.
- **Reason:** Enabling API caching allows responses from the API to be stored temporarily, which can significantly reduce the latency for subsequent requests, as these requests can be served from the cache rather than invoking the backend services (like Lambda) each time. This is particularly effective for read access APIs where the data does not change frequently, such as daily statistics.

# Example / Analogy
- Think of API caching like a library that allows you to borrow books. Instead of ordering a new book every time someone wants to read it, the library keeps popular books on a shelf ready for quick access. This reduces waiting time for readers (similar to reducing response time for users) since they can get the book immediately without having to wait for it to be ordered and delivered again.

# Common Mistakes / Traps
- A common mistake is to assume that enabling CORS or configuring usage plans directly improves API performance. While these are useful features, they do not directly enhance responsiveness in terms of reducing latency for read access operations.

# Memory Tip
- ""Cache to dash: speed up APIs with cached responses!"""
10,"[""A developer wants to store information about movies. Each movie has a title,\nrelease year, and genre. The movie information also can include additional\nproperties about the cast and production crew. This additional information is\ninconsistent across movies. For example, one movie might have an assistant\ndirector, and another movie might have an animal trainer.\nThe developer needs to implement a solution to support the following use cases:\nFor a given title and release year, get all details about the movie that has\nthat title and release year.\nFor a given title, get all details about all movies that have that title.\nFor a given genre, get all details about all movies in that genre.\nWhich data store configuration will meet these requirements?""]",Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the title as the partition key and the release year as the sort key. Create a global secondary index that uses the genre as the partition key and the title as the sort key.,Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the genre as the partition key and the release year as the sort key. Create a global secondary index that uses the title as the partition key.,"On an Amazon RDS DB instance, create a table that contains columns for title, release year, and genre. Configure the title as the primary key.","On an Amazon RDS DB instance, create a table where the primary key is the title and all other data is encoded into JSON format as one additional column.","# Answer
- **Correct option:** A
- **Reason:** Option A proposes a suitable design using Amazon DynamoDB with a primary key composed of the title as the partition key and the release year as the sort key. This allows for efficient querying of a specific movie detail using both the title and release year. Moreover, the global secondary index (GSI) on genre enables querying all movies by their genre. This structure effectively meets all the specified use cases.

# Example / Analogy
- Real-life example: Think of a library where books are organized by both title and year of publication. If you know the title and year, you can easily find the book, much like querying in DynamoDB with the primary key. Additionally, if you want to find all books of a specific genre, a separate shelf for genres acts like the global secondary index in DynamoDB.

# Common Mistakes / Traps
- A common mistake is assuming that a single primary key can satisfy all query requirements. Some options might seem simpler but could limit querying flexibility. For instance, Option C confining title as the primary key would not allow querying by genre or release year effectively, as it lacks sufficient granularity.

# Memory Tip
- **Remember:** ""DynamoDB needs the right keys for the queries you want to perform."""
11,"[""A company is migrating an on-premises database to Amazon RDS for MySQL. The\ncompany has read-heavy workloads. The company wants to refactor the code to\nachieve optimum read performance for queries.\nWhich solution will meet this requirement with LEAST current and future effort?""]",Use a multi-AZ Amazon RDS deployment. Increase the number of connections that the code makes to the database or increase the connection pool size if a connection pool is in use.,Use a multi-AZ Amazon RDS deployment. Modify the code so that queries access the secondary RDS instance.,Deploy Amazon RDS with one or more read replicas. Modify the application code so that queries use the URL for the read replicas.,Use open source replication software to create a copy of the MySQL database on an Amazon EC2 instance. Modify the application code so that queries use the IP address of the EC2 instance.,"# Answer
- **Correct option:** C : Deploy Amazon RDS with one or more read replicas. Modify the application code so that queries use the URL for the read replicas.
- **Reason:** Deploying read replicas allows you to offload read traffic from the primary database. This is optimal for read-heavy workloads, as it enhances performance while reducing latency for read queries. Each read replica can handle read queries independently, freeing up resources on the primary instance for write operations. This solution requires minimal changes to the code, involving only the modification of the connection strings to direct read queries to the replicas.

# Example / Analogy
- Think of a primary database like a busy restaurant kitchen. If all customers (queries) are directed to that kitchen, the chefs (the database) can get overwhelmed, leading to long wait times. By setting up read replicas as additional kitchens, you can channel diners (read queries) to these smaller kitchens, allowing them to be served faster without overloading the main kitchen.

# Common Mistakes / Traps
- A common mistake is to assume that a multi-AZ deployment will increase read performance. Multi-AZ is primarily designed for high availability and failover, not for handling read-heavy workloads. Another mistake is overlooking the need for application code modifications, as simply improving the database configuration isn't enough.

# Memory Tip
- **Tip:** Remember that read replicas boost read performance by splitting the workload—like having multiple kitchen setups for a busy restaurant."
12,"[""A developer has an application that stores data in an Amazon S3 bucket. The\napplication uses an HTTP API to store and retrieve objects. When the PutObject\nAPI operation adds objects to the S3 bucket the developer must encrypt these\nobjects at rest by using server-side encryption with Amazon S3 managed keys\n(SSE-S3).\nWhich solution will meet this requirement?""]",Create an AWS Key Management Service (AWS KMS) key. Assign the KMS key to the S3 bucket.,Set the x-amz-server-side-encryption header when invoking the PutObject API operation.,Provide the encryption key in the HTTP header of every request.,Apply TLS to encrypt the traffic to the S3 bucket.,"# Answer
- **Correct option:** B : Set the x-amz-server-side-encryption header when invoking the PutObject API operation.

- **Reason:** When using server-side encryption with Amazon S3 managed keys (SSE-S3), the developer needs to specify this encryption method during the `PutObject` API operation. This is done by setting the `x-amz-server-side-encryption` HTTP header to `AES256`. This instructs S3 to automatically encrypt the object at rest using its managed keys.

# Example / Analogy
- Think of it like checking a box to request a special wrapping when sending a gift. Just as you need to indicate that you want the gift wrapped upon sending it (in this case, by setting up a header), when uploading an object to S3, you specify that it should be encrypted using the SSE-S3 method with a header. 

# Common Mistakes / Traps
- A common mistake is to confuse SSE-S3 with AWS KMS. Option A mentions creating an AWS KMS key, which is not necessary for SSE-S3, as SSE-S3 uses its own Amazon-managed keys. Additionally, some might think that encryption should be applied at the transport layer (like TLS in option D), but that does not equate to encrypting data at rest.

# Memory Tip
- Remember: ""Set the header to encrypt, just like checking the box for gift wrapping!"""
13,"[""A developer needs to perform geographic load testing of an API. The developer\nmust deploy resources to multiple AWS Regions to support the load testing of the\nAPI.\nHow can the developer meet these requirements without additional application\ncode?""]",Create and deploy an AWS Lambda function in each desired Region. Configure the Lambda function to create a stack from an AWS CloudFormation template in that Region when the function is invoked.,Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI create-stack-set command to create a stack set in the desired Regions.,Create an AWS Systems Manager document that defines the resources. Use the document to create the resources in the desired Regions.,Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI deploy command to create a stack from the template in each Region.,"# Answer
- **Correct option:** B : Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI create-stack-set command to create a stack set in the desired Regions.

- **Reason:** The AWS CloudFormation stack set functionality allows you to easily deploy a single CloudFormation template across multiple AWS Regions simultaneously. This eliminates the need for additional application code and enables efficient resource deployment tailored to geographic load testing.

# Example / Analogy
- Imagine you are a chef preparing the same dish in multiple restaurants located in different cities. Instead of going to each restaurant and cooking the meal manually, you create a recipe (CloudFormation template). With stack sets, you can hand the recipe to a manager (the AWS CLI) at each restaurant, who can then execute it, ensuring each location prepares the dish consistently and simultaneously without additional work from you.

# Common Mistakes / Traps
- One common mistake is thinking that simply deploying resources in each region with separate commands or templates is sufficient, which does not leverage the efficiency and centralized management that stack sets offer. Some may also consider manually invoking resources in each region using Lambda or other methods, which is not as efficient or manageable.

# Memory Tip
- Use stack sets for efficient multi-region resource deployment; simplify your process."
14,"[""A developer is creating a template that uses AWS CloudFormation to deploy an\napplication. The application is serverless and uses Amazon API Gateway, Amazon\nDynamoDB, and AWS Lambda.\nWhich AWS service or tool should the developer use to define serverless\nresources in YAML?""]",CloudFormation serverless intrinsic functions,AWS Elastic Beanstalk,AWS Serverless Application Model (AWS SAM),AWS Cloud Development Kit (AWS CDK),"# Answer
- **Correct option:** C : AWS Serverless Application Model (AWS SAM)
- **Reason:** AWS SAM is specifically designed to simplify the creation and management of serverless applications. It extends AWS CloudFormation with a simplified syntax for defining serverless resources, allowing developers to define APIs, Lambda functions, and DynamoDB tables in a YAML format.

# Example / Analogy
- Think of AWS SAM like a recipe book for cooking a meal. Just as a recipe simplifies the cooking process by detailing the ingredients and steps, AWS SAM simplifies the deployment of serverless applications by providing a structured format to define the necessary resources and their configurations.

# Common Mistakes / Traps
- A common mistake is to confuse AWS SAM with AWS Cloud Development Kit (AWS CDK). While CDK allows you to define infrastructure using familiar programming languages and could also be used for serverless architectures, SAM is tailored specifically for serverless applications and offers an easier path with its simplified abstractions.

# Memory Tip
- Remember: ""SAM simplifies serverless."""
15,"[""A developer wants to insert a record into an Amazon DynamoDB table as soon as a\nnew file is added to an Amazon S3 bucket.\nWhich set of steps would be necessary to achieve this?""]",Create an event with Amazon EventBridge that will monitor the S3 bucket and then insert the records into DynamoDB.,Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.,Create an AWS Lambda function that will poll the S3 bucket and then insert the records into DynamoDB.,Create a cron job that will run at a scheduled time and insert the records into DynamoDB.,"# Answer
- **Correct option:** B : Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.

- **Reason:** This option allows for a direct and efficient way to respond to events in S3. By configuring an event notification on the S3 bucket, you can trigger a Lambda function automatically whenever a new file is added. This function can then handle the logic to insert a record into the DynamoDB table without the need for continuous polling or additional event management services.

# Example / Analogy
- Imagine you have a smart doorbell that notifies you when someone rings the bell (S3 event), and you have a second service (Lambda) that performs an action when it receives that notification, like recording the visitor’s name (inserting a record into DynamoDB). This direct trigger-response model is efficient and automated.

# Common Mistakes / Traps
- A common mistake is to overlook alternatives, like using EventBridge or polling methods. However, those methods introduce unnecessary complexity for this straightforward use case. The question specifically asks about actions as soon as a file is added, which the Lambda trigger handles perfectly.

# Memory Tip
- Remember: ""S3 events are like triggers; they call Lambda to take action instantly."""
16,"[""A development team maintains a web application by using a single AWS\nCloudFormation template. The template defines web servers and an Amazon RDS\ndatabase. The team uses the Cloud Formation template to deploy the Cloud\nFormation stack to different environments.\nDuring a recent application deployment, a developer caused the primary\ndevelopment database to be dropped and recreated. The result of this incident\nwas a loss of data. The team needs to avoid accidental database deletion in the\nfuture.\nWhich solutions will meet these requirements? (Choose two.)""]",Add a CloudFormation Deletion Policy attribute with the Retain value to the database resource.,Update the CloudFormation stack policy to prevent updates to the database.,Modify the database to use a Multi-AZ deployment.,Create a CloudFormation stack set for the web application and database deployments.,"# Answer
- **Correct options:** A and B

- **Reason:**  
Option A is correct because adding a CloudFormation Deletion Policy attribute with the Retain value to the database resource will ensure that if the stack is deleted, the database resource will not be deleted, thus preserving the data. Option B is also correct because updating the CloudFormation stack policy to prevent updates to the database will lock it down from being modified or deleted unexpectedly during stack updates, thereby protecting it from accidental deletion.

# Example / Analogy
- Imagine you have a precious family heirloom (the database) kept in a secure display cabinet (the CloudFormation stack). If you put a ""do not touch"" sign (the stack policy) on the cabinet, people know they can't disturb what's inside. Further, if you put a safeguard in place that says ""keep this inside even if you dismantle the cabinet"" (the retain policy), your heirloom stays safe regardless of what happens to the cabinet.

# Common Mistakes / Traps
- A common mistake is confusing the Deletion Policy with stack policies. The Deletion Policy specifically addresses how resources are treated upon stack deletion, while stack policies control what can be modified within the stack's lifecycle. Additionally, some might be tempted to consider options like Multi-AZ deployments (Option C) as a solution, but that primarily enhances availability rather than preventing deletion.

# Memory Tip
- Remember: **""Retain to sustain, policy to protect!""** (Deletion Policy for retainment; stack policy for protection)."
17,"[""An application that is hosted on an Amazon EC2 instance needs access to files\nthat are stored in an Amazon S3 bucket. The application lists the objects that\nare stored in the S3 bucket and displays a table to the user. During testing, a\ndeveloper discovers that the application does not show any objects in the list.\nWhat is the MOST secure way to resolve this issue?""]",Update the IAM instance profile that is attached to the EC2 instance to include the S3:* permission for the S3 bucket.,Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket.,Update the developer's user permissions to include the S3:ListBucket permission for the S3 bucket.,Update the S3 bucket policy by including the S3:ListBucket permission and by setting the Principal element to specify the account number of the EC2 instance.,"# Answer
- **Correct option:** B : Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket.

- **Reason:** The application running on the EC2 instance needs the `S3:ListBucket` permission to retrieve the list of objects in the specified S3 bucket. This permission allows the application to access the bucket's metadata and display the content. Granting broader permissions like `S3:*` is less secure and should be avoided when only `S3:ListBucket` is needed.

# Example / Analogy
- Imagine a library where you can only see the book titles on the shelves (metadata) if you have a specific library card (permission) that allows you to look at the index. The `S3:ListBucket` permission is like that library card—it lets the application see what is available, without giving full access to check out the books.

# Common Mistakes / Traps
- A common mistake is to assume that a user’s permissions are the same as those needed by an application. The application runs under the EC2 instance's IAM role, not under the developer's permissions. Also, choosing overly broad permissions can lead to security vulnerabilities.

# Memory Tip
- ""Grant only what’s necessary: List permissions to see, but don’t give full access!"""
18,"[""A company has deployed infrastructure on AWS. A development team wants to create\nan AWS Lambda function that will retrieve data from an Amazon Aurora database.\nThe Amazon Aurora database is in a private subnet in company's VPC. The VPC is\nnamed VPC1. The data is relational in nature. The Lambda function needs to\naccess the data securely.\nWhich solution will meet these requirements?""]",Create the Lambda function. Configure VPC1 access for the function. Attach a security group named SG1 to both the Lambda function and the database. Configure the security group inbound and outbound rules to allow TCP traffic on Port 3306.,Create and launch a Lambda function in a new public subnet that is in a new VPC named VPC2. Create a peering connection between VPC1 and VPC2.,Create the Lambda function. Configure VPC1 access for the function. Assign a security group named SG1 to the Lambda function. Assign a second security group named SG2 to the database. Add an inbound rule to SG1 to allow TCP traffic from Port 3306.,Export the data from the Aurora database to Amazon S3. Create and launch a Lambda function in VPC1. Configure the Lambda function query the data from Amazon S3.,"# Answer
- **Correct option:** A
- **Reason:** This option correctly implements VPC access for the Lambda function while maintaining the security requirements. By configuring the Lambda function to run within VPC1 and attaching a security group (SG1) that allows inbound and outbound TCP traffic on Port 3306 (the standard MySQL port used by Amazon Aurora), the Lambda function can securely communicate with the Aurora database.

# Example / Analogy
- Think of the Lambda function as a guest who needs special permission to enter a restricted area (the private subnet where the Aurora database resides). By giving the guest a pass (security group SG1) that allows access to specific areas (inbound and outbound rules for TCP on Port 3306), they can safely access the resources (the database) they need without exposing it to the entire internet.

# Common Mistakes / Traps
- A common mistake might be to assume that creating a new public subnet or a new VPC (as in option B) is necessary for a Lambda function to communicate with a private database. This would expose the setup to unnecessary complexities like VPC peering and would not adhere to the requirement for secure access to a private database.

# Memory Tip
- Remember: “Keep Lambda in the same VPC as the database for secure access!”"
19,"[""A developer is creating a mobile app that calls a backend service by using an\nAmazon API Gateway REST API. For integration testing during the development\nphase, the developer wants to simulate different backend responses without\ninvoking the backend service.\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Create an AWS Lambda function. Use API Gateway proxy integration to return constant HTTP responses.,Create an Amazon EC2 instance that serves the backend REST API by using an AWS CloudFormation template.,Customize the API Gateway stage to select a response type based on the request.,Use a request mapping template to select the mock integration response.,"# Answer
- **Correct option:** D : Use a request mapping template to select the mock integration response.

- **Reason:** This method allows you to define mock responses directly in API Gateway without any additional infrastructure or resources. By using request mapping templates, you can simulate various backend responses, which is efficient and requires the least operational overhead compared to setting up Lambda functions or EC2 instances.

# Example / Analogy
- Imagine you're a chef preparing a new dish in your restaurant. Instead of sourcing every ingredient and cooking it, you decide to create a sample dish just using what you have in your pantry. This way, you can quickly adjust flavors and portions without the need for extra supplies. Similarly, using mapping templates in API Gateway lets you quickly simulate backend responses without needing additional resources.

# Common Mistakes / Traps
- A common mistake is to choose options that require setting up additional infrastructure (like options A and B) thinking they may provide more flexibility, but they, in fact, introduce unnecessary complexity and operational overhead. 

# Memory Tip
- Think ""mock responses"" in API Gateway for quick tests—just like a chef trying flavors before the final dish!"
20,"[""A developer is creating an application that will store personal health\ninformation (PHI). The PHI needs to be encrypted at all times. An encrypted\nAmazon RDS for MySQL DB instance is storing the data. The developer wants to\nincrease the performance of the application by caching frequently accessed data\nwhile adding the ability to sort or rank the cached datasets.\nWhich solution will meet these requirements?""]",Create an Amazon ElastiCache for Redis instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.,Create an Amazon ElastiCache for Memcached instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.,Create an Amazon RDS for MySQL read replica. Connect to the read replica by using SSL. Configure the read replica to store frequently accessed data.,Create an Amazon DynamoDB table and a DynamoDB Accelerator (DAX) cluster for the table. Store frequently accessed data in the DynamoDB table.,"# Answer
- **Correct option:** A : Create an Amazon ElastiCache for Redis instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.

- **Reason:** Amazon ElastiCache for Redis supports advanced caching features, such as sorting and ranking capabilities, in addition to offering options for data encryption both at rest and in transit. This makes it suitable for handling personal health information (PHI) while enhancing application performance by caching frequently accessed data.

# Example / Analogy
- Think of ElastiCache for Redis like a high-speed filing system. When you need to retrieve important documents quickly (frequently accessed data), you keep them handy in a drawer (cache) where you can easily access them instead of going to the long-term archive (RDS). Additionally, ensuring that the drawer is locked (encryption) secures the important information inside.

# Common Mistakes / Traps
- A common mistake might be to choose Amazon ElastiCache for Memcached (Option B) instead of Redis, as Memcached does not support data persistence and lacks the advanced data structure features that Redis provides, which are necessary for sorting and ranking the cached data.

# Memory Tip
- Reflect: ""Use Redis for speed (cache) and encryption (security) of valuable data."""
21,"[""A company wants to deploy and maintain static websites on AWS. Each website's\nsource code is hosted in one of several version control systems, including AWS\nCodeCommit, Bitbucket, and GitHub.\nThe company wants to implement phased releases by using development, staging,\nuser acceptance testing, and production environments in the AWS Cloud.\nDeployments to each environment must be started by code merges on the relevant\nGit branch. The company wants to use HTTPS for all data exchange. The company\nneeds a solution that does not require servers to run continuously.\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Host each website by using AWS Amplify with a serverless backend. Conned the repository branches that correspond to each of the desired environments. Start deployments by merging code changes to a desired branch.,Host each website in AWS Elastic Beanstalk with multiple environments. Use the EB CLI to link each repository branch. Integrate AWS CodePipeline to automate deployments from version control code merges.,Host each website in different Amazon S3 buckets for each environment. Configure AWS CodePipeline to pull source code from version control. Add an AWS CodeBuild stage to copy source code to Amazon S3.,Host each website on its own Amazon EC2 instance. Write a custom deployment script to bundle each website's static assets. Copy the assets to Amazon EC2. Set up a workflow to run the script when code is merged.,"# Answer
- **Correct option:** A

- **Reason:** Hosting each website using AWS Amplify allows the company to easily connect repository branches that correspond to different environments, such as development, staging, user acceptance testing, and production. AWS Amplify provides a fully managed service that automatically deploys the website when code changes are merged into the relevant Git branch, and it handles HTTPS automatically. This solution requires no servers to run continuously, ensuring minimal operational overhead.

# Example / Analogy
- Think of AWS Amplify like a restaurant that offers a seamless take-out service. You simply call in your order (merge code changes), and the restaurant (Amplify) prepares your meal (deploys the website) without needing you to worry about the kitchen (servers) every time you want to order. Each time you want to try a new dish (deploy to an environment), you just call to place your order.

# Common Mistakes / Traps
- A common mistake is choosing options that require more management and operational overhead, such as EC2 instances or Elastic Beanstalk, which involve more configuration and ongoing management.

# Memory Tip
- **Remember AWS Amplify for serverless web deployment and effortless environment branching!**"
22,"[""A developer is creating an application that will be deployed on IoT devices. The\napplication will send data to a RESTful API that is deployed as an AWS Lambda\nfunction. The application will assign each API request a unique identifier. The\nvolume of API requests from the application can randomly increase at any given\ntime of day.\nDuring periods of request throttling, the application might need to retry\nrequests. The API must be able to handle duplicate requests without\ninconsistencies or data loss.\nWhich solution will meet these requirements?""]",Create an Amazon RDS for MySQL DB instance. Store the unique identifier for each request in a database table. Modify the Lambda function to check the table for the identifier before processing the request.,Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to check the table for the identifier before processing the request.,Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to return a client error response when the function receives a duplicate request.,Create an Amazon ElastiCache for Memcached instance. Store the unique identifier for each request in the cache. Modify the Lambda function to check the cache for the identifier before processing the request.,"# Answer
- **Correct option:** B 

- **Reason:** 
  Using an Amazon DynamoDB table to store the unique identifier for each API request allows the Lambda function to effectively check for and handle duplicate requests. DynamoDB provides high availability and can handle bursts of requests, making it suitable for IoT applications with fluctuating workloads. By storing incoming request identifiers, the Lambda function can prevent processing the same request multiple times, ensuring data integrity and consistency.

# Example / Analogy
- Think of it like a ticketing system at a concert where each ticket has a unique serial number. If someone tries to enter with a duplicate ticket number, the venue can reference a master list (like DynamoDB) to check for duplicates and deny entry, ensuring each person only experiences the concert once, just as the application ensures no duplicate data is processed.

# Common Mistakes / Traps
- A common mistake could be selecting option A instead of B, assuming relational databases like RDS would work efficiently with high-velocity requests. However, DynamoDB's design is inherently better suited for fast, scalable key-value lookups necessary for this use case.

# Memory Tip
- **Think “DynamoDB for data integrity” to handle duplicates effectively!**"
23,"[""A developer is creating an application that includes an Amazon API Gateway REST\nAPI in the us-east-2 Region. The developer wants to use Amazon CloudFront and a\ncustom domain name for the API. The developer has acquired an SSL/TLS\ncertificate for the domain from a third-party provider.\nHow should the developer configure the custom domain for the application?""]",Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS A record for the custom domain.,Import the SSL/TLS certificate into CloudFront. Create a DNS CNAME record for the custom domain.,Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS CNAME record for the custom domain.,Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. Create a DNS CNAME record for the custom domain.,"# Answer
- **Correct option:** D
- **Reason:** The developer should import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Region because API Gateway requires the certificate to be in that region when using a custom domain with CloudFront. After the certificate is imported, the developer can create a DNS CNAME record for the custom domain pointing to the CloudFront distribution.

# Example / Analogy
- Think of it like needing a special permit (the SSL certificate) that is issued only from a specific office (us-east-1) in a city (AWS). Even if your business (API) is located in a different part of the city (us-east-2), you still need that permit to operate legally. You’ll then need to let people know about your new location (creating a DNS record).

# Common Mistakes / Traps
- A common mistake is to think that the SSL/TLS certificate can be in the same region as the API (us-east-2) when in fact it must be imported to us-east-1 for the API Gateway custom domain setup. Developers might also misinterpret how to link the DNS records, mixing up A records and CNAME records.

# Memory Tip
- Remember: ""SSL in us-east-1, CNAME to disguise the endpoint!"""
24,"[""A developer is building a new application on AWS. The application uses an AWS\nLambda function that retrieves information from an Amazon DynamoDB table. The\ndeveloper hard coded the DynamoDB table name into the Lambda function code. The\ntable name might change over time. The developer does not want to modify the\nLambda code if the table name changes.\nWhich solution will meet these requirements MOST efficiently?""]",Create a Lambda environment variable to store the table name. Use the standard method for the programming language to retrieve the variable.,Store the table name in a file. Store the file in the /tmp folder. Use the SDK for the programming language to retrieve the table name.,Create a file to store the table name. Zip the file and upload the file to the Lambda layer. Use the SDK for the programming language to retrieve the table name.,Create a global variable that is outside the handler in the Lambda function to store the table name.,"# Answer
- **Correct option:** A : Create a Lambda environment variable to store the table name. Use the standard method for the programming language to retrieve the variable.

- **Reason:** This solution allows the table name to be easily configurable without modifying the Lambda function code. Environment variables can be updated directly in the AWS Lambda console or through a configuration management tool, making it a flexible and efficient way to manage dynamic configuration data like table names.

# Example / Analogy
- Think of environment variables as labels on containers. Imagine you have several boxes labeled ""Books,"" ""Toys,"" and ""Clothes."" If you want to change the contents of ""Books"" to ""Old Books,"" you just change the label rather than redoing the whole box. Similarly, with environment variables, you can change the table name without altering the code.

# Common Mistakes / Traps
- A common trap is to use hard-coded strings or global variables, thinking they provide an easier solution, but these methods require code changes whenever a value needs to be changed, which defeats the purpose of dynamically configuring the application.

# Memory Tip
- Use environment variables for dynamic settings instead of hardcoded values to ensure flexibility and ease of maintenance."
25,"[""A developer is migrating some features from a legacy monolithic application to\nuse AWS Lambda functions instead. The application currently stores data in an\nAmazon Aurora DB cluster that runs in private subnets in a VPC. The AWS account\nhas one VPC deployed. The Lambda functions and the DB cluster are deployed in\nthe same AWS Region in the same AWS account.\nThe developer needs to ensure that the Lambda functions can securely access the\nDB cluster without crossing the public internet.\nWhich solution will meet these requirements?""]",Configure the DB cluster's public access setting to Yes.,Configure an Amazon RDS database proxy for he Lambda functions.,Configure a NAT gateway and a security group for the Lambda functions.,"Configure the VPC, subnets, and a security group for the Lambda functions.","# Answer
- **Correct option:** D : Configure the VPC, subnets, and a security group for the Lambda functions.

- **Reason:** Lambda functions can securely access resources within a VPC by being configured to run inside that VPC. By placing the Lambda functions in the same VPC as the Amazon Aurora DB cluster and setting the appropriate subnets and security groups, you ensure that the connection between the Lambda functions and the DB remains private and does not traverse the public internet.

# Example / Analogy  
- Think of it like a secure office building with locked doors. When your Lambda functions are like employees who work inside that building (VPC), they need special access (subnets and security groups) to reach the data stored in the office’s secure vault (Aurora DB cluster). By giving them access to the same building without letting them go outside (the internet), you ensure that their work remains secure.

# Common Mistakes / Traps  
- A common mistake is to assume that simply configuring public access or adding a NAT gateway allows secure access to the DB. Public access exposes the database to the internet, which is not what the question requires. Additionally, a NAT gateway is typically used for outbound internet access from private subnets, which is not necessary for this scenario.

# Memory Tip  
- Remember: Keep Lambda in the same VPC as Aurora for a secure, private connection!"
26,"[""A developer is writing an AWS Lambda function. The developer wants to log key\nevents that occur while the Lambda function runs. The developer wants to include\na unique identifier to associate the events with a specific function invocation.\nThe developer adds the following code to the Lambda function:\n[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image2.png]\nWhich solution will meet this requirement?""]",Obtain the request identifier from the AWS request ID field in the context object. Configure the application to write logs to standard output.,Obtain the request identifier from the AWS request ID field in the event object. Configure the application to write logs to a file.,Obtain the request identifier from the AWS request ID field in the event object. Configure the application to write logs to standard output.,Obtain the request identifier from the AWS request ID field in the context object. Configure the application to write logs to a file.,"# Answer
- **Correct option:** A
- **Reason:** The context object in an AWS Lambda function provides the AWS request ID, which serves as a unique identifier for the specific invocation of the function. By logging this ID to standard output, the logs become easily accessible in Amazon CloudWatch, allowing the developer to track and associate log entries with that particular function execution.

# Example / Analogy
- Think of the AWS request ID as a unique ticket number at an event. Each attendee (function invocation) gets a ticket (request ID), which they can show to identify themselves. If something happens at the event (like a lost item), the ticket helps trace back to that specific attendee, just like the request ID helps trace back to the specific invocation of the Lambda function.

# Common Mistakes / Traps
- A common mistake is opting for the event object instead of the context object to retrieve the request ID. The event object typically contains the input data for the function and may not have the unique identifier meant for logging and tracing.

# Memory Tip
- Remember, ""Context is Key"" when needing unique identifiers in AWS Lambda."
27,"[""An application uses an Amazon EC2 Auto Scaling group. A developer notices that\nEC2 instances are taking a long time to become available during scale-out\nevents. The UserData script is taking a long time to run.\nThe developer must implement a solution to decrease the time that elapses before\nan EC2 instance becomes available. The solution must make the most recent\nversion of the application available at all times and must apply all available\nsecurity updates. The solution also must minimize the number of images that are\ncreated. The images must be validated.\nWhich combination of steps should the developer take to meet these requirements?\n(Choose two.)""]",Use EC2 Image Builder to create an Amazon Machine Image (AMI). Install all the patches and agents that are needed to manage and run the application. Update the Auto Scaling group launch configuration to use the AMI.,Use EC2 Image Builder to create an Amazon Machine Image (AMI). Install the latest version of the application and all the patches and agents that are needed to manage and run the application. Update the Auto Scaling group launch configuration to use the AMI.,Set up AWS CodeDeploy to deploy the most recent version of the application at runtime.,Set up AWS CodePipeline to deploy the most recent version of the application at runtime.,"# Answer
- **Correct options:** A and B

- **Reason:** Option A focuses on using EC2 Image Builder to prepare an AMI that has all necessary patches and agents installed, streamlining the instance launch process. However, while it ensures security updates, it does not ensure the application is the latest version. Option B extends this by also ensuring that the latest version of the application is pre-installed in the AMI created by EC2 Image Builder. This reduces the time it takes for an EC2 instance to become fully operational during scale-out events since the setup that usually runs at the initial startup is handled beforehand.

# Example / Analogy
- Think of preparing a meal for guests. Instead of cooking everything from scratch when they arrive (which takes time), you can pre-cook the meal (like creating an AMI) and just reheat it when guests arrive, ensuring it's always the freshest version and meets guests’ preferences.

# Common Mistakes / Traps
- A common mistake could be assuming that simply updating the UserData script would speed up the initialization process. While this may help, it does not address the need for the most current application version or maintain security patches by itself. 

# Memory Tip
- Pre-pack your AMI like a meal prep: have everything ready so that your EC2 instances serve up the latest version quickly!"
28,"[""A developer is using an AWS Lambda function to generate avatars for profile\npictures that are uploaded to an Amazon S3 bucket. The Lambda function is\nautomatically invoked for profile pictures that are saved under the /original/\nS3 prefix. The developer notices that some pictures cause the Lambda function to\ntime out. The developer wants to implement a fallback mechanism by using another\nLambda function that resizes the profile picture.\nWhich solution will meet these requirements with the LEAST development effort?""]",Set the image resize Lambda function as a destination of the avatar generator Lambda function for the events that fail processing.,Create an Amazon Simple Queue Service (Amazon SQS) queue. Set the SQS queue as a destination with an on failure condition for the avatar generator Lambda function. Configure the image resize Lambda function to poll from the SQS queue.,Create an AWS Step Functions state machine that invokes the avatar generator Lambda function and uses the image resize Lambda function as a fallback. Create an Amazon EventBridge rule that matches events from the S3 bucket to invoke the state machine.,Create an Amazon Simple Notification Service (Amazon SNS) topic. Set the SNS topic as a destination with an on failure condition for the avatar generator Lambda function. Subscribe the image resize Lambda function to the SNS topic.,"# Answer
- **Correct option:** A 
- **Reason:** Setting the image resize Lambda function as a destination of the avatar generator Lambda function for failed events is the simplest implementation. By configuring this as a failure destination, the resize function gets invoked automatically without the need for additional polling or complex error-handling mechanisms.

# Example / Analogy
- Imagine a bakery where a chef (the avatar generator) bakes a cake (the avatar), but sometimes the cake falls (times out). Instead of throwing away the fallen cake, there’s a helper (resize Lambda) who can quickly fix and decorate it. When the chef’s attempt fails, the helper is automatically notified to step in and resize the cake without needing a complicated process to communicate.

# Common Mistakes / Traps
- A common trap is to overcomplicate the solution, like using SQS or SNS, which introduces additional components and maintenance. Understanding that a simple failure destination can directly invoke another Lambda function helps streamline error handling.

# Memory Tip
- Remember: **""Simpler is better!""** Use failure destinations for direct fallback."
29,"[""A developer needs to migrate an online retail application to AWS to handle an\nanticipated increase in traffic. The application currently runs on two servers:\none server for the web application and another server for the database. The web\nserver renders webpages and manages session state in memory. The database server\nhosts a MySQL database that contains order details. When traffic to the\napplication is heavy, the memory usage for the web server approaches 100% and\nthe application slows down considerably.\nThe developer has found that most of the memory increase and performance\ndecrease is related to the load of managing additional user sessions. For the\nweb server migration, the developer will use Amazon EC2 instances with an Auto\nScaling group behind an Application Load Balancer.\nWhich additional set of changes should the developer make to the application to\nimprove the application's performance?""]",Use an EC2 instance to host the MySQL database. Store the session data and the application data in the MySQL database.,Use Amazon ElastiCache for Memcached to store and manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.,Use Amazon ElastiCache for Memcached to store and manage the session data and the application data.,Use the EC2 instance store to manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.,"# Answer
- **Correct option:** B: Use Amazon ElastiCache for Memcached to store and manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.

- **Reason:** Storing session data in Amazon ElastiCache helps alleviate the load on the web server's memory, as it provides a scalable in-memory caching solution specifically designed for high performance. This allows the web server to focus on rendering web pages and processing requests rather than managing session state. Additionally, using Amazon RDS for MySQL offloads database management tasks, providing a managed database solution that can handle scalability, backups, and high availability, which is particularly beneficial for an online retail application with fluctuating traffic demands.

# Example / Analogy
- Think of managing sessions like running a busy restaurant. If the chef (your web server) is also responsible for taking orders (managing sessions) and cooking (serving web pages), the quality of food (application performance) will suffer when too many customers arrive. By hiring a waiter (Amazon ElastiCache) to take orders (handle sessions), the chef can focus on cooking efficiently. Meanwhile, the RDS for MySQL can be compared to a reliable supply chain that ensures all ingredients (application data) are always in stock and readily available.

# Common Mistakes / Traps
- A common mistake is to consider that session data can simply be stored in the database (option A or D), as traditional databases are not optimized for high-speed reads/writes required for session management. Additionally, opting for EC2 instance storage (option D) is risky since instance storage is ephemeral and will be lost if the instance is terminated. 

# Memory Tip
- Use caching (like ElastiCache) to offload session management from the web server for better performance!"
30,"[""A developer has a legacy application that is hosted on-premises. Other\napplications hosted on AWS depend on the on-premises application for proper\nfunctioning. In case of any application errors, the developer wants to be able\nto use Amazon CloudWatch to monitor and troubleshoot all applications from one\nplace.\nHow can the developer accomplish this?""]",Install an AWS SDK on the on-premises server to automatically send logs to CloudWatch.,Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch.,Upload log files from the on-premises server to Amazon S3 and have CloudWatch read the files.,Upload log files from the on-premises server to an Amazon EC2 instance and have the instance forward the logs to CloudWatch.,"# Answer
- **Correct option:** B : Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch.

- **Reason:** The Amazon CloudWatch Agent is specifically designed to collect monitoring and operational data from both EC2 and on-premises servers. By installing this agent on the on-premises server, the developer can have the application logs sent directly to CloudWatch, providing centralized monitoring for troubleshooting and operational oversight.

# Example / Analogy
- Imagine a security camera (CloudWatch agent) installed in a store (on-premises server) that continuously relays video footage (logs) to a central monitoring station (CloudWatch). Instead of having to go to the store to check the footage, the security personnel can simply view everything from one centralized location.

# Common Mistakes / Traps
- A common mistake could be to think that simply uploading logs to S3 (Options C and D) would allow direct integration with CloudWatch. While this approach is valid for archival, it doesn’t provide real-time monitoring capabilities like the CloudWatch agent does. Additionally, users might assume that just using an SDK (Option A) is sufficient, but the agility and functionality of the dedicated CloudWatch agent make it the preferred solution.

# Memory Tip
- **Install the CloudWatch agent on-premises for real-time log monitoring and centralized troubleshooting.**"
31,"[""An ecommerce company is using an AWS Lambda function behind Amazon API Gateway\nas its application tier. To process orders during checkout, the application\ncalls a POST API from the frontend. The POST API invokes the Lambda function\nasynchronously. In rare situations, the application has not processed orders.\nThe Lambda application logs show no errors or failures.\nWhat should a developer do to solve this problem?""]",Inspect the frontend logs for API failures. Call the POST API manually by using the requests from the log file.,Create and inspect the Lambda dead-letter queue. Troubleshoot the failed functions. Reprocess the events.,Inspect the Lambda logs in Amazon CloudWatch for possible errors. Fix the errors.,Make sure that caching is disabled for the POST API in API Gateway.,"# Answer
- **Correct option:** B
- **Reason:** Creating and inspecting the Lambda dead-letter queue (DLQ) allows you to identify any events that failed to be processed correctly. Since the logs do not show any errors or failures, it's possible that the Lambda function processed the request, but there may have been an issue with the event itself that caused it to be unprocessed. The DLQ will help capture this data.

# Example / Analogy
- Think of the dead-letter queue as a lost-and-found box. If someone orders a package, but it gets lost (in this case, the event fails), the order information is still there in the lost-and-found. By checking the box (DLQ), you can see what went wrong and re-process those orders.

# Common Mistakes / Traps
- A common mistake might be to assume that there was an error in the Lambda logs (Choice C) when no error was logged, which can lead to overlooking the possibility of unprocessed events. Another trap could be focusing on API Gateway's configurations like caching (Choice D), which isn't the issue when the Lambda function shows no errors.

# Memory Tip
- Remember: ""Use the dead-letter queue to find lost events!"""
32,"[""A company has an Amazon S3 bucket that contains sensitive data. The data must be\nencrypted in transit and at rest. The company encrypts the data in the S3 bucket\nby using an AWS Key Management Service (AWS KMS) key. A developer needs to grant\nseveral other AWS accounts the permission to use the S3 GetObject operation to\nretrieve the data from the S3 bucket.\nHow can the developer enforce that all requests to retrieve the data provide\nencryption in transit?""]",Define a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false”.,Define a resource-based policy on the S3 bucket to allow access when a request meets the condition “aws:SecureTransport”: “false”.,Define a role-based policy on the other accounts' roles to deny access when a request meets the condition of “aws:SecureTransport”: “false”.,Define a resource-based policy on the KMS key to deny access when a request meets the condition of “aws:SecureTransport”: “false”.,"# Answer
- **Correct option:** A
- **Reason:** Defining a resource-based policy on the S3 bucket to deny access when a request meets the condition “aws:SecureTransport”: “false” ensures that any attempt to access the S3 bucket without using HTTPS (which is secure transport) will be blocked. This effectively enforces the requirement that all requests for retrieving data must be encrypted in transit.

# Example / Analogy
- Think of the S3 bucket as a vault that only opens when the secure lock (HTTPS) is used. If someone tries to open the vault using a regular key (HTTP), the vault will stay locked, protecting the sensitive data inside.

# Common Mistakes / Traps
- A common mistake could be selecting option B, which incorrectly allows access under insecure conditions. This could lead to data being accessible over unencrypted channels, jeopardizing the data's confidentiality and integrity.

# Memory Tip
- ""S3 policies can lock the vault against unsecure keys."""
33,"[""A developer is building a web application that uses Amazon API Gateway to expose\nan AWS Lambda function to process requests from clients. During testing, the\ndeveloper notices that the API Gateway times out even though the Lambda function\nfinishes under the set time limit.\nWhich of the following API Gateway metrics in Amazon CloudWatch can help the\ndeveloper troubleshoot the issue? (Choose two.)""]",CacheHitCount,IntegrationLatency,CacheMissCount,Latency,"# Answer
- **Correct option:** B : IntegrationLatency, D : Latency

- **Reason:** 
  - **IntegrationLatency** measures the time taken by API Gateway to call the integrated AWS service (in this case, the Lambda function). It helps the developer understand if the delay is due to the API Gateway waiting on the Lambda function's response.
  - **Latency** encompasses the total time taken for a client request to be processed by API Gateway, which includes the time taken to receive the request, call the Lambda function, and return the response. It can help identify if the timeout issue is within the API Gateway itself.

# Example / Analogy
- Think of API Gateway as a restaurant waiter (API), who takes an order, passes it to the kitchen (Lambda function), and then delivers the food back to the customer. If the waiter takes a long time to return with the food, you'd want to know if it's because the kitchen is slow (IntegrationLatency) or if the waiter took a long time to take the order or deliver it (Latency).

# Common Mistakes / Traps
- A common mistake is confusing metrics that don't directly relate to the timing of Lambda execution with those that do. For instance, **CacheHitCount** and **CacheMissCount** contribute to performance analysis, but they don't help in understanding the timing of the request processing.

# Memory Tip
- Remember: **IntegrationLatency is about the connection, while Latency is about the whole journey.**"
34,"[""A company is building a web application on AWS. When a customer sends a request,\nthe application will generate reports and then make the reports available to the\ncustomer within one hour. Reports should be accessible to the customer for 8\nhours. Some reports are larger than 1 MB. Each report is unique to the customer.\nThe application should delete all reports that are older than 2 days.\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Generate the reports and then store the reports as Amazon DynamoDB items that have a specified TTL. Generate a URL that retrieves the reports from DynamoDB. Provide the URL to customers through the web application.,Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryption. Attach the reports to an Amazon Simple Notification Service (Amazon SNS) message. Subscribe the customer to email notifications from Amazon SNS.,Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryption. Generate a presigned URL that contains an expiration date Provide the URL to customers through the web application. Add S3 Lifecycle configuration rules to the S3 bucket to delete old reports.,Generate the reports and then store the reports in an Amazon RDS database with a date stamp. Generate an URL that retrieves the reports from the RDS database. Provide the URL to customers through the web application. Schedule an hourly AWS Lambda function to delete database records that have expired date stamps.,"# Answer
- **Correct option:** C
- **Reason:** Storing the reports in an Amazon S3 bucket allows for large files and simplifies expiration management with S3 lifecycle rules. Using presigned URLs ensures secure access for customers, and the lifecycle rules automate the deletion of reports older than 2 days, minimizing operational overhead.

# Example / Analogy
- Think of Amazon S3 as a secure filing cabinet where you can store large documents. When you need a document (report), you get a special key (presigned URL) that allows you to access it for a limited time. The janitor (S3 lifecycle rules) automatically removes old documents after a certain period, so you don’t have to worry about cleaning out the cabinet yourself.

# Common Mistakes / Traps
- A common mistake is to overlook the storage service's ability to handle file size—in this case, Amazon S3 is designed for large objects, unlike DynamoDB which has limitations on item size (up to 400 KB). Also, avoid thinking that pressing for a DB solution (like RDS) is always better; it incurs more overhead because of managing connection pools and read/write operations.

# Memory Tip
- **Remember:** Use S3 for scalable storage and lifecycle rules to automate cleanup."
35,"[""A company is planning to securely manage one-time fixed license keys in AWS. The\ncompany's development team needs to access the license keys in automaton scripts\nthat run in Amazon EC2 instances and in AWS CloudFormation stacks.\nWhich solution will meet these requirements MOST cost-effectively?""]",Amazon S3 with encrypted files prefixed with “config”,AWS Secrets Manager secrets with a tag that is named SecretString,AWS Systems Manager Parameter Store SecureString parameters,CloudFormation NoEcho parameters,"# Answer
- **Correct option:** C : AWS Systems Manager Parameter Store SecureString parameters

- **Reason:** AWS Systems Manager Parameter Store allows you to securely store and manage parameters, such as license keys, in a way that developers can easily access them in their automation scripts running on EC2 instances. The SecureString parameters ensure that the sensitive data is encrypted at rest and in transit, making it a cost-effective solution since Parameter Store is free for a certain number of parameters and is straightforward to integrate into both EC2 instances and CloudFormation stacks.

# Example / Analogy
- Think of Parameter Store like a secure vault for important documents. Just as you would keep keys or sensitive papers in a safe place where only trusted individuals can access them, Parameter Store keeps your license keys safe, allowing your development team to retrieve them as needed securely.

# Common Mistakes / Traps
- A common mistake is choosing AWS Secrets Manager because it’s specifically designed for secrets management, but for simple license key storage, Parameter Store is often more cost-effective. Additionally, some might overlook the need for SecureString, thinking standard strings will suffice for sensitive data.

# Memory Tip
- Use Parameter Store as your secure vault for sensitive keys in scripts!"
36,"[""A development team wants to build a continuous integration/continuous delivery\n(CI/CD) pipeline. The team is using AWS CodePipeline to automate the code build\nand deployment. The team wants to store the program code to prepare for the\nCI/CD pipeline.\nWhich AWS service should the team use to store the program code?""]",AWS CodeDeploy,AWS CodeArtifact,AWS CodeCommit,Amazon CodeGuru,"# Answer
- **Correct option:** C : AWS CodeCommit
- **Reason:** AWS CodeCommit is a fully managed source control service that hosts secure Git repositories. It is designed specifically for developers to store and manage their code, making it an ideal choice for the development team to store their program code in preparation for the CI/CD pipeline.

# Example / Analogy
- Think of CodeCommit like a digital library where you can store your books (the code). Just as a library allows you to keep your books safe, share them with others, and manage different versions, CodeCommit allows developers to store their code securely, collaborate with team members, and manage different code versions effectively.

# Common Mistakes / Traps
- A common mistake is to confuse CodeCommit with AWS CodeDeploy or AWS CodeArtifact. CodeDeploy is for deploying applications, while CodeArtifact is for managing and sharing software packages—not for storing code repositories.

# Memory Tip
- Remember: ""CodeCommit keeps code safe, just like a library for your books!"""
37,"[""A developer is designing an AWS Lambda function that creates temporary files\nthat are less than 10 MB during invocation. The temporary files will be accessed\nand modified multiple times during invocation. The developer has no need to save\nor retrieve these files in the future.\nWhere should the temporary files be stored?""]",the /tmp directory,Amazon Elastic File System (Amazon EFS),Amazon Elastic Block Store (Amazon EBS),Amazon S3,"# Answer
- **Correct option:** A : the /tmp directory
- **Reason:** AWS Lambda provides a limited amount of temporary storage in the /tmp directory with a capacity of 512 MB. This is ideal for storing temporary files needed only during the execution of the function, especially since the files are less than 10 MB and do not need to be preserved beyond the invocation.

# Example / Analogy
- Think of the /tmp directory like a temporary workspace or desk. When you're working on a project, you might use this desk to spread out your documents and tools. Once you finish the project, you can clear the desk without worrying about saving anything because you no longer need those materials. Similarly, the /tmp directory serves as a temporary space during the execution of the Lambda function.

# Common Mistakes / Traps
- A common mistake is to choose Amazon S3, thinking it’s suitable for temporary storage, but S3 is designed for persistent storage and involves additional overhead for interactions. Also, options like EBS and EFS are used for long-term storage, which is unnecessary for temporary files.

# Memory Tip
- Temporary files = /tmp directory in AWS Lambda = quick cleanup after use!"
38,"[""A developer is designing a serverless application with two AWS Lambda functions\nto process photos. One Lambda function stores objects in an Amazon S3 bucket and\nstores the associated metadata in an Amazon DynamoDB table. The other Lambda\nfunction fetches the objects from the S3 bucket by using the metadata from the\nDynamoDB table. Both Lambda functions use the same Python library to perform\ncomplex computations and are approaching the quota for the maximum size of\nzipped deployment packages.\nWhat should the developer do to reduce the size of the Lambda deployment\npackages with the LEAST operational overhead?""]",Package each Python library in its own .zip file archive. Deploy each Lambda function with its own copy of the library.,Create a Lambda layer with the required Python library. Use the Lambda layer in both Lambda functions.,Combine the two Lambda functions into one Lambda function. Deploy the Lambda function as a single .zip file archive.,Download the Python library to an S3 bucket. Program the Lambda functions to reference the object URLs.,"# Answer
- **Correct option:** B : Create a Lambda layer with the required Python library. Use the Lambda layer in both Lambda functions.
- **Reason:** Using a Lambda layer allows you to store the shared library separately from the Lambda function deployment package. This reduces the size of the individual deployment packages, as the library is maintained separately and can be reused across multiple functions, which is ideal when the same library is required in more than one function.

# Example / Analogy
- Real-life example or memorable comparison: Think of Lambda layers like a cooking class where several chefs need to use the same set of tools (e.g., knives, pans). Instead of each chef bringing their own set of tools (which would take up space), they can use a shared kitchen tool set located in one area. This way, they focus on their dishes (the main tasks of each Lambda function) while keeping their workspaces (deployment packages) less cluttered.

# Common Mistakes / Traps
- Common trap points, if any: A common mistake is assuming that each Lambda function must contain its own copy of a library. This increases deployment package size unnecessarily. Another trap is thinking that simply combining the functions or downloading libraries at runtime will resolve size issues without operational overhead or complexity.

# Memory Tip
- Use Lambda layers to share code efficiently and reduce deployment package size."
39,"[""A developer is working on a serverless application that needs to process any\nchanges to an Amazon DynamoDB table with an AWS Lambda function.\nHow should the developer configure the Lambda function to detect changes to the\nDynamoDB table?""]","Create an Amazon Kinesis data stream, and attach it to the DynamoDB table. Create a trigger to connect the data stream to the Lambda function.",Create an Amazon EventBridge rule to invoke the Lambda function on a regular schedule. Conned to the DynamoDB table from the Lambda function to detect changes.,Enable DynamoDB Streams on the table. Create a trigger to connect the DynamoDB stream to the Lambda function.,"Create an Amazon Kinesis Data Firehose delivery stream, and attach it to the DynamoDB table. Configure the delivery stream destination as the Lambda function.","# Answer
- **Correct option:** C
- **Reason:** Enabling DynamoDB Streams on the table captures changes to the items in the table, and by creating a trigger that connects the DynamoDB Streams to the Lambda function, the function can be automatically invoked whenever changes occur in the table (like inserts, updates, or deletes).

# Example / Analogy
- Think of DynamoDB Streams as a live feed of all changes happening in a store. If you imagine a café where every order placed, modified, or canceled is written down on a board, the Lambda function is like a barista who gets immediately notified whenever the board is updated, allowing them to adjust their tasks in real-time based on customer demand.

# Common Mistakes / Traps
- A common error is to confuse DynamoDB Streams with alternative data ingestion services like Kinesis. Many people might think they should set up Kinesis for streaming changes, but using DynamoDB Streams is more efficient and directly tied to tracking changes in DynamoDB.

# Memory Tip
- Remember: **DynamoDB Streams = Real-time operation updates** for triggering actions with Lambda."
40,"[""A developer is creating an AWS Lambda function that needs credentials to connect\nto an Amazon RDS for MySQL database. An Amazon S3 bucket currently stores the\ncredentials. The developer needs to improve the existing solution by\nimplementing credential rotation and secure storage. The developer also needs to\nprovide integration with the Lambda function.\nWhich solution should the developer use to store and retrieve the credentials\nwith the LEAST management overhead?""]",Store the credentials in AWS Systems Manager Parameter Store. Select the database that the parameter will access. Use the default AWS Key Management Service (AWS KMS) key to encrypt the parameter. Enable automatic rotation for the parameter. Use the parameter from Parameter Store on the Lambda function to connect to the database.,"Encrypt the credentials with the default AWS Key Management Service (AWS KMS) key. Store the credentials as environment variables for the Lambda function. Create a second Lambda function to generate new credentials and to rotate the credentials by updating the environment variables of the first Lambda function. Invoke the second Lambda function by using an Amazon EventBridge rule that runs on a schedule. Update the database to use the new credentials. On the first Lambda function, retrieve the credentials from the environment variables. Decrypt the credentials by using AWS KMS, Connect to the database.",Store the credentials in AWS Secrets Manager. Set the secret type to Credentials for Amazon RDS database. Select the database that the secret will access. Use the default AWS Key Management Service (AWS KMS) key to encrypt the secret. Enable automatic rotation for the secret. Use the secret from Secrets Manager on the Lambda function to connect to the database.,Encrypt the credentials by using AWS Key Management Service (AWS KMS). Store the credentials in an Amazon DynamoDB table. Create a second Lambda function to rotate the credentials. Invoke the second Lambda function by using an Amazon EventBridge rule that runs on a schedule. Update the DynamoDB table. Update the database to use the generated credentials. Retrieve the credentials from DynamoDB with the first Lambda function. Connect to the database.,"# Answer
- **Correct option:** C
- **Reason:** AWS Secrets Manager is specifically designed for managing and storing sensitive information like database credentials securely. It provides built-in support for automatic credential rotation without additional management overhead, simplifying integration with AWS Lambda functions.

# Example / Analogy
- Think of AWS Secrets Manager as a high-security vault that automatically changes its lock (the credentials) regularly without you having to physically change the keys (credentials) yourself. Just like a bank updates its security measures to ensure safety, Secrets Manager updates the credentials while you continue to access the vault (database) seamlessly.

# Common Mistakes / Traps
- A common mistake would be to choose options that involve more management overhead, like creating additional Lambda functions for rotation or manually handling encryption and storage in places like S3 or DynamoDB, which can lead to complexity and higher chances of error. Many might also underestimate the importance of automatic rotation and secure storage features that Secret Manager provides.

# Memory Tip
- Use Secrets Manager for secrets—it's the purpose-built tool that reduces overhead and simplifies secure management."
41,"[""A developer has written the following IAM policy to provide access to an Amazon\nS3 bucket:\n[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image3.png]\nWhich access does the policy allow regarding the s3:GetObject and s3:PutObject\nactions?""]",Access on all buckets except the “DOC-EXAMPLE-BUCKET” bucket,Access on all buckets that start with “DOC-EXAMPLE-BUCKET” except the “DOC-EXAMPLE-BUCKET/secrets” bucket,Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket along with access to all S3 actions for objects in the “DOC-EXAMPLE-BUCKET” bucket that start with “secrets”,Access on all objects in the “DOC-EXAMPLE-BUCKET” bucket except on objects that start with “secrets”,"# Answer
- **Correct option:** D
- **Reason:** The IAM policy allows access to all objects in the ""DOC-EXAMPLE-BUCKET"" bucket except for those objects whose keys start with ""secrets"". This is specified in the policy with the Action `s3:GetObject` and `s3:PutObject` allowed for all objects in the bucket, with a condition that denies specific access for objects prefixed with ""secrets"".

# Example / Analogy
- Think of the ""DOC-EXAMPLE-BUCKET"" as a library where you can borrow any book (objects) you want, but there is a special section labeled ""Secrets"" (objects starting with ""secrets""). You have a sign that states you cannot access any books in that special section. This way, while you have access to the majority of the library (the bucket), the restriction prevents you from obtaining specific information that is confidential.

# Common Mistakes / Traps
- A common mistake is to misunderstand the effect of the S3 policy in relation to object prefixes. People might mistakenly think that the restriction on ""secrets"" applies to all actions or the entire bucket, rather than just objects starting with that prefix.

# Memory Tip
- Remember: ""All but the Secrets"" when allowing permissions for S3 objects—you're granted access, but only certain items are off-limits."
42,"[""An Amazon Kinesis Data Firehose delivery stream is receiving customer data that\ncontains personally identifiable information. A developer needs to remove\npattern-based customer identifiers from the data and store the modified data in\nan Amazon S3 bucket.\nWhat should the developer do to meet these requirements?""]",Implement Kinesis Data Firehose data transformation as an AWS Lambda function. Configure the function to remove the customer identifiers. Set an Amazon S3 bucket as the destination of the delivery stream.,Launch an Amazon EC2 instance. Set the EC2 instance as the destination of the delivery stream. Run an application on the EC2 instance to remove the customer identifiers. Store the transformed data in an Amazon S3 bucket.,Create an Amazon OpenSearch Service instance. Set the OpenSearch Service instance as the destination of the delivery stream. Use search and replace to remove the customer identifiers. Export the data to an Amazon S3 bucket.,"Create an AWS Step Functions workflow to remove the customer identifiers. As the last step in the workflow, store the transformed data in an Amazon S3 bucket. Set the workflow as the destination of the delivery stream.","# Answer
- **Correct option:** A 
- **Reason:** Implementing Kinesis Data Firehose data transformation as an AWS Lambda function allows for the on-the-fly modification of incoming data streams. The Lambda function can be configured specifically to remove customer identifiers directly before the data is sent to Amazon S3, thus effectively meeting the requirement to remove personally identifiable information.

# Example / Analogy
- Think of Kinesis Data Firehose like a conveyor belt carrying boxes of goods (the data). In this scenario, the Lambda function acts like a quality control station along the conveyor belt that inspects each box and removes any sensitive items before they reach the storage area (Amazon S3). This ensures only safe, modified boxes are stored.

# Common Mistakes / Traps
- A common pitfall is choosing solutions that involve additional, unnecessary services (like EC2 or Step Functions) that complicate the architecture. The goal is to efficiently handle stream data processing without introducing more components than needed.

# Memory Tip
- **Lambda is your data filter: remove sensitive info on-the-go!**"
43,"[""An application uses Lambda functions to extract metadata from files uploaded to\nan S3 bucket; the metadata is stored in Amazon DynamoDB. The application starts\nbehaving unexpectedly, and the developer wants to examine the logs of the Lambda\nfunction code for errors.\nBased on this system configuration, where would the developer find the logs?""]",Amazon S3,AWS CloudTrail,Amazon CloudWatch,Amazon DynamoDB,"# Answer
- **Correct option:** C : Amazon CloudWatch
- **Reason:** AWS Lambda automatically integrates with Amazon CloudWatch to log output and error messages generated by your Lambda functions. When a Lambda function fails or logs a statement, this information is sent to CloudWatch Logs, where developers can access it for debugging and monitoring.

# Example / Analogy
- Consider CloudWatch as a security camera for your Lambda function—it records everything that happens during execution (like logs) so you can review and diagnose issues later.

# Common Mistakes / Traps
- A common mistake is thinking that logs are stored in S3 or that CloudTrail (which records API activity) would provide the specific logs of the Lambda function’s execution. CloudTrail captures management events but not the runtime execution logs of Lambda.

# Memory Tip
- Think ""CloudWatch for checking what went wrong in my Lambda."""
44,"[""A company is using an AWS Lambda function to process records from an Amazon\nKinesis data stream. The company recently observed slow processing of the\nrecords. A developer notices that the iterator age metric for the function is\nincreasing and that the Lambda run duration is constantly above normal.\nWhich actions should the developer take to increase the processing speed?\n(Choose two.)""]",Increase the number of shards of the Kinesis data stream.,Decrease the timeout of the Lambda function.,Increase the memory that is allocated to the Lambda function.,Decrease the number of shards of the Kinesis data stream.,"# Answer
- **Correct options:** A, C  

- **Reason:**  
  Option A (Increase the number of shards of the Kinesis data stream) effectively allows for more parallel processing of records, enabling multiple Lambda instances to consume from the stream at the same time. The iterator age metric indicates how old the records in the stream are compared to the current processing point; if it's increasing, it suggests that Lambda is not keeping up with the incoming data. By increasing the number of shards, you can distribute the load more efficiently.  
  Option C (Increase the memory that is allocated to the Lambda function) not only increases the available memory but can also lead to a higher CPU allocation, which can reduce processing time for each function invocation. When you boost memory for Lambda, it can handle more complex processing or run faster, reducing the overall duration of function executions.

# Example / Analogy
- Think of a highway with multiple lanes (shards) where cars (data records) travel. If the highway has too few lanes, traffic slows down, leading to cars piling up (increased iterator age). Adding more lanes (shards) allows more cars to travel at once, improving overall traffic speed. Similarly, if one car (Lambda function) isn't powerful enough (limited memory), it can't accelerate effectively; giving it a more powerful engine (more memory) helps it go faster.

# Common Mistakes / Traps
- A common mistake is to assume that decreasing the timeout or the number of shards will help. Decreasing the timeout (Option B) only means the Lambda will stop processing sooner, which does not resolve the underlying problem of processing speed. Additionally, decreasing the number of shards (Option D) contradicts the goal of improving throughput.

# Memory Tip
- **More shards = more lanes; more memory = a more powerful engine.**"
45,"[""A developer is incorporating AWS X-Ray into an application that handles personal\nidentifiable information (PII). The application is hosted on Amazon EC2\ninstances. The application trace messages include encrypted PII and go to Amazon\nCloudWatch. The developer needs to ensure that no PII goes outside of the EC2\ninstances.\nWhich solution will meet these requirements?""]",Manually instrument the X-Ray SDK in the application code.,Use the X-Ray auto-instrumentation agent.,Use Amazon Macie to detect and hide PII. Call the X-Ray API from AWS Lambda.,Use AWS Distro for Open Telemetry.,"# Answer
- **Correct option:** A: Manually instrument the X-Ray SDK in the application code.

- **Reason:** Manually instrumenting the X-Ray SDK allows the developer to have complete control over what data is sent to AWS X-Ray, ensuring that encrypted PII can be excluded from trace data. By selectively including trace messages that do not contain any PII, the developer can ensure compliance with privacy requirements and prevent sensitive information from going outside the EC2 instances.

# Example / Analogy
- Think of it like manually deciding what to share on social media versus automatically sharing everything from your phone. If you manually select which photos to post (like manually instrumenting the X-Ray SDK), you ensure that no personal or sensitive images (PII) are shared publicly. Automatic sharing might include everything, potentially compromising your privacy.

# Common Mistakes / Traps
- A common trap in this type of question is assuming that auto-instrumentation (Option B) is safer. While it is quicker and easier to set up, it may capture all data, including sensitive PII, unless carefully configured. Similarly, using services like AWS Lambda to process data (Option C) does not inherently prevent PII from being sent outside EC2 instances, as the data received may still include sensitive information.

# Memory Tip
- Manually control your data flow to keep your PII personal and protected!"
46,"[""A company needs to harden its container images before the images are in a\nrunning state. The company's application uses Amazon Elastic Container Registry\n(Amazon ECR) as an image registry. Amazon Elastic Kubernetes Service (Amazon\nEKS) for compute, and an AWS CodePipeline pipeline that orchestrates a\ncontinuous integration and continuous delivery (CI/CD) workflow.\nDynamic application security testing occurs in the final stage of the pipeline\nafter a new image is deployed to a development namespace in the EKS cluster. A\ndeveloper needs to place an analysis stage before this deployment to analyze the\ncontainer image earlier in the CI/CD pipeline.\nWhich solution will meet these requirements with the MOST operational\nefficiency?""]",Build the container image and run the docker scan command locally. Mitigate any findings before pushing changes to the source code repository. Write a pre-commit hook that enforces the use of this workflow before commit.,Create a new CodePipeline stage that occurs after the container image is built. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.,Create a new CodePipeline stage that occurs after source code has been retrieved from its repository. Run a security scanner on the latest revision of the source code. Fail the pipeline if there are findings.,Add an action to the deployment stage of the pipeline so that the action occurs before the deployment to the EKS cluster. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.,"# Answer
- **Correct option:** B
- **Reason:** This option creates a dedicated stage in the CodePipeline that scans the container image immediately after it is built and before it is deployed. By utilizing ECR's built-in image scanning capability, the solution ensures that container vulnerabilities are identified early. The Lambda function can automate the process of checking the scan results and stopping the pipeline if any issues are detected, providing a robust security check without manual intervention.

# Example / Analogy
- Think of this as a safety inspection for a vehicle before it hits the road. Just like a thorough inspection can identify issues with the brakes or engines before the car is used, the image scanning step identifies security vulnerabilities before the container is deployed. This proactive approach prevents problematic containers from running in a production environment.

# Common Mistakes / Traps
- A common mistake could be thinking that scanning after the deployment is sufficient, or assuming local scanning is enough for a CI/CD pipeline. Both of these overlook the efficiencies and automation provided by integrating the scan before deployment in the pipeline.

# Memory Tip
- Always scan your images *before* deployment for early detection of vulnerabilities—secure from the start!"
47,"[""A developer is testing a new file storage application that uses an Amazon\nCloudFront distribution to serve content from an Amazon S3 bucket. The\ndistribution accesses the S3 bucket by using an origin access identity (OAI).\nThe S3 bucket's permissions explicitly deny access to all other users.\nThe application prompts users to authenticate on a login page and then uses\nsigned cookies to allow users to access their personal storage directories. The\ndeveloper has configured the distribution to use its default cache behavior with\nrestricted viewer access and has set the origin to point to the S3 bucket.\nHowever, when the developer tries to navigate to the login page, the developer\nreceives a 403 Forbidden error.\nThe developer needs to implement a solution to allow unauthenticated access to\nthe login page. The solution also must keep all private content secure.\nWhich solution will meet these requirements?""]","Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to the path of the login page, and make viewer access unrestricted. Keep the default cache behavior's settings unchanged.","Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to *, and make viewer access restricted. Change the default cache behavior's path pattern to the path of the login page, and make viewer access unrestricted.","Add a second origin as a failover origin to the default cache behavior. Point the failover origin to the S3 bucket. Set the path pattern for the primary origin to *, and make viewer access restricted. Set the path pattern for the failover origin to the path of the login page, and make viewer access unrestricted.",Add a bucket policy to the S3 bucket to allow read access. Set the resource on the policy to the Amazon Resource Name (ARN) of the login page object in the S3 bucket. Add a CloudFront function to the default cache behavior to redirect unauthorized requests to the login page's S3 URL.,"# Answer
- **Correct option:** A
- **Reason:** Option A suggests adding a second cache behavior to the CloudFront distribution that points specifically to the login page with unrestricted viewer access. This allows anyone to access the login page while keeping the default cache behavior (which is restricted) for all other content, ensuring that private content remains secure.

# Example / Analogy
- Imagine you run a secure building with a main entrance that requires keycard access (like the restricted access for private content). However, you also have a public lobby area (the login page) where anyone can enter without restriction. By setting up a separate entrance just for the lobby (the second cache behavior), you allow unauthenticated users to access that area while keeping the main building secure.

# Common Mistakes / Traps
- A common trap is to think that simply adjusting the default cache behavior will solve the problem. However, that would not grant access to the login page for unauthenticated users, leading to a 403 error. It's essential to identify that a targeted solution (like a separate cache behavior) is needed for public access.

# Memory Tip
- **One behavior for access, another for security!**"
48,"[""A developer is using AWS Amplify Hosting to build and deploy an application. The\ndeveloper is receiving an increased number of bug reports from users. The\ndeveloper wants to add end-to-end testing to the application to eliminate as\nmany bugs as possible before the bugs reach production.\nWhich solution should the developer implement to meet these requirements?""]",Run the amplify add test command in the Amplify CLI.,Create unit tests in the application. Deploy the unit tests by using the amplify push command in the Amplify CLI.,Add a test phase to the amplify.yml build settings for the application.,Add a test phase to the aws-exports.js file for the application.,"# Answer
- **Correct option:** C
- **Reason:** Adding a test phase to the `amplify.yml` build settings allows you to automate end-to-end testing during the build process on AWS Amplify Hosting. This ensures that tests are run each time you deploy the application, helping to catch bugs before the application reaches production.

# Example / Analogy
- Think of `amplify.yml` build settings like a chef's recipe that includes a baking step (test phase) at the end. If the chef doesn't taste the dish (run tests) before serving it, they might deliver a flawed dish (application) to the customers (users). Having a test phase ensures that the dish is perfect before it gets to the table.

# Common Mistakes / Traps
- A common mistake is to think that adding unit tests (Choice B) is sufficient on its own. While unit tests are important, end-to-end tests cover broader functionality and integration, ensuring the entire application works well together.

# Memory Tip
- ""Test early, test often"" to remember that integrating testing into build settings is essential for quality."
49,"[""A company has deployed an application on AWS Elastic Beanstalk. The company has\nconfigured the Auto Scaling group that is associated with the Elastic Beanstalk\nenvironment to have five Amazon EC2 instances. If the capacity is fewer than\nfour EC2 instances during the deployment, application performance degrades. The\ncompany is using the all-at-once deployment policy.\nWhat is the MOST cost-effective way to solve the deployment issue?""]",Change the Auto Scaling group to six desired instances.,Change the deployment policy to traffic splitting. Specify an evaluation time of 1 hour.,Change the deployment policy to rolling with additional batch. Specify a batch size of 1.,Change the deployment policy to rolling. Specify a batch size of 2.,"# Answer
- **Correct option:** C : Change the deployment policy to rolling with additional batch. Specify a batch size of 1.

- **Reason:** Changing the deployment policy to ""rolling with additional batch"" and setting the batch size to 1 allows one instance to be updated at a time while maintaining a minimum of four instances available. This ensures that the application has enough capacity to handle traffic without degrading performance during deployments, thus addressing the issue of application performance degradation.

# Example / Analogy
- Imagine a bakery that gets busy every day. If the bakery decides to renovate all its counters at once (like the all-at-once deployment), it might not have enough space to serve customers, resulting in long lines and unhappy customers. Instead, if the bakery updates one counter at a time (similar to rolling deployment with batch size of 1), it can continue serving customers and won't cause a bottleneck, ensuring happy customers and continued sales.

# Common Mistakes / Traps
- A common mistake is choosing the ""all-at-once"" deployment because it sounds quicker but leads to the performance issue. Additionally, some may mistakenly think that increasing the number of instances is the only solution without realizing that the deployment strategy is also critical for maintaining performance.

# Memory Tip
- Aim to ""roll"" one at a time for smoother transitions during deployments."
50,"[""A company is building a serverless application that uses AWS Lambda functions.\nThe company needs to create a set of test events to test Lambda functions in a\ndevelopment environment. The test events will be created once and then will be\nused by all the developers in an IAM developer group. The test events must be\neditable by any of the IAM users in the IAM developer group.\n\nWhich solution will meet these requirements?""]",Create and store the test events in Amazon S3 as JSON objects. Allow S3 bucket access to all IAM users.,Create the test events. Configure the event sharing settings to make the test events shareable.,Create and store the test events in Amazon DynamoDB. Allow access to DynamoDB by using IAM roles.,Create the test events. Configure the event sharing settings to make the test events private.,"# Answer
- **Correct option:** A

- **Reason:** Storing the test events in an Amazon S3 bucket as JSON objects allows all IAM users in the developer group to access and edit the test events as needed. S3 is designed for easy sharing and storage of files and provides fine-grained access control through IAM policies. Once the events are stored, any developer in the appropriate group can read, modify, and reuse these events without conflicts.

# Example / Analogy
- Think of S3 as a shared drive where all developers store and collaborate on project documents. By placing your test events there (much like filing docs in a shared folder), everyone can retrieve, edit, and share their updates seamlessly, ensuring everyone is working with the same information.

# Common Mistakes / Traps
- A common mistake might be thinking that event sharing settings (as mentioned in options B and D) can facilitate direct collaboration like S3 does. However, Lambda test events do not have built-in sharing features like S3. Additionally, confusing DynamoDB (option C) as an appropriate place for test event storage may lead to unnecessary complexity and cost since S3 is simpler and more suited for this use case.

# Memory Tip
- ""For collaborative storage and easy access, think S3 – the shared drive of AWS."""
51,"[""An engineer created an A/B test of a new feature on an Amazon CloudWatch\nEvidently project. The engineer configured two variations of the feature\n(Variation A and Variation B) for the test. The engineer wants to work\nexclusively with Variation A. The engineer needs to make updates so that\nVariation A is the only variation that appears when the engineer hits the\napplication's endpoint.\n\nWhich solution will meet this requirement?""]",Add an override to the feature. Set the identifier of the override to the engineer's user ID. Set the variation to Variation A.,Add an override to the feature. Set the identifier of the override to Variation A. Set the variation to 100%.,Add an experiment to the project. Set the identifier of the experiment to Variation B. Set the variation to 0%.,Add an experiment to the project. Set the identifier of the experiment to the AWS account's account ISet the variation to Variation A.,"# Answer
- **Correct option:** A
- **Reason:** Adding an override with the engineer's user ID ensures that the application will always serve Variation A to that specific user, effectively bypassing the A/B test for them. This approach allows testing variations without impacting other users by forcing the desired variation for a designated user.

# Example / Analogy
- Think of it like a restaurant where you’re conducting a taste test for two new dishes (Variation A and Variation B). If you want to ensure one specific customer always receives their favorite dish (Variation A), you could create a note in the kitchen that says, ""Always serve dish A to this customer."" This way, they get what they prefer while you continue to experiment with others.

# Common Mistakes / Traps
- A common mistake is to confuse the variations with user identifiers. Option B suggests setting the variation to 100%, which might seem like a good solution, but it does not target specific users and might confuse the overall testing framework, reducing the integrity of the A/B test.

# Memory Tip
- **Override with user ID** to give specific users their desired variation without disrupting the A/B test for others."
52,"[""A developer is working on an existing application that uses Amazon DynamoDB as\nits data store. The DynamoDB table has the following attributes: partNumber\n(partition key), vendor (sort key), description, productFamily, and productType.\nWhen the developer analyzes the usage patterns, the developer notices that there\nare application modules that frequently look for a list of products based on the\nproductFamily and productType attributes.\n\nThe developer wants to make changes to the application to improve performance of\nthe query operations.\n\nWhich solution will meet these requirements?""]",Create a global secondary index (GSI) with productFamily as the partition key and productType as the sort key.,Create a local secondary index (LSI) with productFamily as the partition key and productType as the sort key.,"Recreate the table. Add partNumber as the partition key and vendor as the sort key. During table creation, add a local secondary index (LSI) with productFamily as the partition key and productType as the sort key.",Update the queries to use Scan operations with productFamily as the partition key and productType as the sort key.,"# Answer
- **Correct option:** A: Create a global secondary index (GSI) with productFamily as the partition key and productType as the sort key.
  
- **Reason:** A Global Secondary Index (GSI) allows you to query the DynamoDB table using different key attributes. By creating a GSI with `productFamily` as the partition key and `productType` as the sort key, the application will be able to efficiently retrieve items based on both of these attributes. This improves performance compared to scanning the full table.

# Example / Analogy
- Imagine you have a library organized solely by the author's name (like a DynamoDB table with only one index). If you wanted to find all books in a specific genre, you would need to search through every book, which is very time-consuming (akin to a Scan operation). However, if the library had a separate catalog (GSI) organized by genre and then sub-organized by title, you could quickly find them, saving time (just like the GSI allows for efficient querying based on `productFamily` and `productType`).

# Common Mistakes / Traps
- A common mistake is choosing a Local Secondary Index (LSI) when you actually need an index that allows queries based on different partition keys across the entire dataset. An LSI requires the same partition key as the base table, making it unsuitable in this scenario.

# Memory Tip
- **Tip:** Use GSIs for alternative querying patterns to efficiently retrieve data beyond primary keys."
53,"[""A developer creates a VPC named VPC-A that has public and private subnets. The\ndeveloper also creates an Amazon RDS database inside the private subnet of\nVPC-A. To perform some queries, the developer creates an AWS Lambda function in\nthe default VPC. The Lambda function has code to access the RDS database. When\nthe Lambda function runs, an error message indicates that the function cannot\nconnect to the RDS database.\n\nHow can the developer solve this problem?""]",Modify the RDS security group. Add a rule to allow traffic from all the ports from the VPC CIDR block.,Redeploy the Lambda function in the same subnet as the RDS instance. Ensure that the RDS security group allows traffic from the Lambda function.,Create a security group for the Lambda function. Add a new rule in the RDS security group to allow traffic from the new Lambda security group.,Create an IAM role. Attach a policy that allows access to the RDS database. Attach the role to the Lambda function.,"# Answer
- **Correct option:** B
- **Reason:** The Lambda function is created in the default VPC, which has no direct connection to the private subnet of VPC-A where the RDS database resides. By redeploying the Lambda function within the same VPC (specifically the same subnet), it will have network access to the RDS database. Additionally, ensuring that the RDS's security group allows traffic from the Lambda function's IP or security group is necessary for a successful connection.

# Example / Analogy
- Imagine trying to make a phone call to someone who is out of your service area; no matter how good your phone is (the Lambda function), you won't be able to connect (access the RDS) unless you are both in the same service area (VPC and subnet). By moving your phone to a location with service (redeploying Lambda), you can successfully connect with the person on the other end (RDS database).

# Common Mistakes / Traps
- A common mistake is focusing only on security group modifications (as suggested in options A and C) without considering that a network path must exist. Simply modifying security groups does not allow the Lambda function in a different VPC to communicate with the RDS database.

# Memory Tip
- **Always deploy Lambda functions in the same VPC as the resources they need to access.**"
54,"[""A company runs an application on AWS. The company deployed the application on\nAmazon EC2 instances. The application stores data on Amazon Aurora.\n\nThe application recently logged multiple application-specific custom\nDECRYP_ERROR errors to Amazon CloudWatch logs. The company did not detect the\nissue until the automated tests that run every 30 minutes failed. A developer\nmust implement a solution that will monitor for the custom errors and alert a\ndevelopment team in real time when these errors occur in the production\nenvironment.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Configure the application to create a custom metric and to push the metric to CloudWatch. Create an AWS CloudTrail alarm. Configure the CloudTrail alarm to use an Amazon Simple Notification Service (Amazon SNS) topic to send notifications.,Create an AWS Lambda function to run every 5 minutes to scan the CloudWatch logs for the keyword DECRYP_ERROR. Configure the Lambda function to use Amazon Simple Notification Service (Amazon SNS) to send a notification.,Use Amazon CloudWatch Logs to create a metric filter that has a filter pattern for DECRYP_ERROR. Create a CloudWatch alarm on this metric for a threshold >=1. Configure the alarm to send Amazon Simple Notification Service (Amazon SNS) notifications.,Install the CloudWatch unified agent on the EC2 instance. Configure the application to generate a metric for the keyword DECRYP_ERROR errors. Configure the agent to send Amazon Simple Notification Service (Amazon SNS) notifications.,"# Answer
- **Correct option:** C  
- **Reason:** This option utilizes Amazon CloudWatch Logs to create a metric filter specifically for the DECRYP_ERROR logs. By creating a metric filter, the errors can be monitored in real time, and a CloudWatch alarm can be set up to trigger notifications through Amazon Simple Notification Service (SNS) when the error count meets or exceeds a threshold of 1. This solution is efficient as it directly leverages existing tools and services without requiring additional compute resources or complex configurations.

# Example / Analogy
- Imagine you are in a large office building with multiple floors. Instead of assigning someone to walk around and check if any lights are malfunctioning every few minutes (like the Lambda solution), you install automatic sensors (metric filters) that will alert the maintenance team (SNS) the instant a light goes out (DECRYP_ERROR occurs). This way, you get immediate feedback with minimal effort.

# Common Mistakes / Traps
- One common mistake is overcomplicating the solution when simpler native AWS functionalities can suffice. For example, options that require creating custom metrics or using AWS Lambda solutions might seem attractive but introduce unnecessary complexity and overhead compared to CloudWatch Logs capabilities.

# Memory Tip
- For real-time monitoring of logs, always think **""filter and alert""** with CloudWatch Logs."
55,"[""A developer created an AWS Lambda function that accesses resources in a VPC. The\nLambda function polls an Amazon Simple Queue Service (Amazon SQS) queue for new\nmessages through a VPC endpoint. Then the function calculates a rolling average\nof the numeric values that are contained in the messages. After initial tests of\nthe Lambda function, the developer found that the value of the rolling average\nthat the function returned was not accurate.\n\nHow can the developer ensure that the function calculates an accurate rolling\naverage?""]",Set the function's reserved concurrency to 1. Calculate the rolling average in the function. Store the calculated rolling average in Amazon ElastiCache.,"Modify the function to store the values in Amazon ElastiCache. When the function initializes, use the previous values from the cache to calculate the rolling average.",Set the function's provisioned concurrency to 1. Calculate the rolling average in the function. Store the calculated rolling average in Amazon ElastiCache.,"Modify the function to store the values in the function's layers. When the function initializes, use the previously stored values to calculate the rolling average.","# Answer
- **Correct option:** B
- **Reason:** By modifying the function to store the values in Amazon ElastiCache, the developer can ensure that the function has access to the historical data needed for calculating the rolling average. This allows the Lambda function to retrieve previously stored values whenever it initializes, thus ensuring that the rolling average calculation incorporates all relevant data, leading to an accurate output.

# Example / Analogy
- Think of it like a chef who wants to make a consistent soup every day. Instead of starting from scratch every time, the chef keeps a notebook (Amazon ElastiCache) with the recipes and adjustments from the previous days. This helps the chef create a consistent flavor by using past modifications instead of relying solely on memory.

# Common Mistakes / Traps
- A common mistake is to think that setting concurrency limits (as in options A and C) directly impacts the accuracy of calculations. While controlling concurrency is important for resource management, it does not address the fundamental issue of historical value access needed for accurate calculations.

# Memory Tip
- Use ElastiCache to store past values for consistent rolling calculations!"
56,"[""A developer is writing unit tests for a new application that will be deployed on\nAWS. The developer wants to validate all pull requests with unit tests and merge\nthe code with the main branch only when all tests pass.\n\nThe developer stores the code in AWS CodeCommit and sets up AWS CodeBuild to run\nthe unit tests. The developer creates an AWS Lambda function to start the\nCodeBuild task. The developer needs to identify the CodeCommit events in an\nAmazon EventBridge event that can invoke the Lambda function when a pull request\nis created or updated.\n\nWhich CodeCommit event will meet these requirements?""]",[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image4.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image5.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image6.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image7.png],"# Answer
- **Correct option:** C

- **Reason:** The CodeCommit event that triggers on pull request creation or update is specifically designed to notify changes related to pull requests. This event would capture scenarios where a new pull request is created, or an existing one is updated, thus allowing the developer's Lambda function to invoke CodeBuild to run unit tests.

# Example / Analogy
- Think of this scenario like a gatekeeper at a concert. The gatekeeper only allows entry (the merge of the code) when a ticket is shown (all tests pass) for each new arrival (pull request). If someone tries to change their ticket (update the pull request), the gatekeeper checks it again before granting access.

# Common Mistakes / Traps
- A common mistake is to confuse pull request events with commits. Pull request actions are different from standard commit actions, and only specific events related to pull requests should be considered for triggering the Lambda function in this scenario.

# Memory Tip
- **Remember “PR Events for Tests”**: Pull Request events in CodeCommit are your key to validating code before it merges."
57,"[""A company hosts a batch processing application on AWS Elastic Beanstalk with\ninstances that run the most recent version of Amazon Linux. The application\nsorts and processes large datasets.\n\nIn recent weeks, the application's performance has decreased significantly\nduring a peak period for traffic. A developer suspects that the application\nissues are related to the memory usage. The developer checks the Elastic\nBeanstalk console and notices that memory usage is not being tracked.\n\nHow should the developer gather more information about the application\nperformance issues?""]",Configure the Amazon CloudWatch agent to push logs to Amazon CloudWatch Logs by using port 443.,Configure the Elastic Beanstalk .ebextensions directory to track the memory usage of the instances.,Configure the Amazon CloudWatch agent to track the memory usage of the instances.,Configure an Amazon CloudWatch dashboard to track the memory usage of the instances.,"# Answer
- **Correct option:** C : Configure the Amazon CloudWatch agent to track the memory usage of the instances.  
- **Reason:** Configuring the Amazon CloudWatch agent allows you to collect detailed metrics, including memory usage, from the EC2 instances running within the Elastic Beanstalk environment. This is essential for diagnosing performance issues related to memory.

# Example / Analogy
- Think of the Amazon CloudWatch agent like a health monitor for your application—just as a doctor would need detailed information about your vital signs (blood pressure, heart rate) to diagnose potential health issues, your application needs memory metrics to identify performance bottlenecks.

# Common Mistakes / Traps
- A common mistake is to think that just configuring CloudWatch alarms or logs (like options A and D) will give insight into performance metrics. However, these will not track actual memory usage unless the agent is configured specifically for that purpose, as mentioned in option C.

# Memory Tip
- Remember: **CloudWatch agent = detailed metrics, including memory!**"
58,"[""A developer is implementing an AWS Cloud Development Kit (AWS CDK) serverless\napplication. The developer will provision several AWS Lambda functions and\nAmazon API Gateway APIs during AWS CloudFormation stack creation. The\ndeveloper's workstation has the AWS Serverless Application Model (AWS SAM) and\nthe AWS CDK installed locally.\n\nHow can the developer test a specific Lambda function locally?""]",Run the sam package and sam deploy commands. Create a Lambda test event from the AWS Management Console. Test the Lambda function.,Run the cdk synth and cdk deploy commands. Create a Lambda test event from the AWS Management Console. Test the Lambda function.,Run the cdk synth and sam local invoke commands with the function construct identifier and the path to the synthesized CloudFormation template.,Run the cdk synth and sam local start-lambda commands with the function construct identifier and the path to the synthesized CloudFormation template.,"# Answer
- **Correct option:** C 
- **Reason:** This option correctly involves using the `cdk synth` command to synthesize the CloudFormation template generated by AWS CDK, followed by `sam local invoke` which allows testing of a specific Lambda function locally using the generated template context. This approach enables the developer to run the function with their local development environment set up properly.

# Example / Analogy
- Think of it like baking a cake from a recipe: you first need to write down the recipe (synthesizing the CloudFormation template with `cdk synth`), and then you can cook a slice of that cake (testing the Lambda function with `sam local invoke`).

# Common Mistakes / Traps
- A common mistake would be to assume that `sam local start-lambda` is required for testing a single function. In fact, `sam local invoke` is more appropriate for executing and testing individual Lambda functions while also allowing event simulation.

# Memory Tip
- **Remember:** Synthesize first, then invoke—CDK creates, SAM executes!"
59,"[""A developer is building an application that uses Amazon DynamoDB. The developer\nwants to retrieve multiple specific items from the database with a single API\ncall.\n\nWhich DynamoDB API call will meet these requirements with the MINIMUM impact on\nthe database?""]",BatchGetItem,GetItem,Scan,Query,"# Answer
- **Correct option:** A : BatchGetItem
- **Reason:** The `BatchGetItem` API call is specifically designed to retrieve multiple items from one or more tables in a single request. This method minimizes the number of requests sent to the database, thus reducing the overall impact on read throughput and improving efficiency.

# Example / Analogy
- Imagine you are at a grocery store and you need to buy ingredients for a recipe. Instead of going to different aisles and checking out separately, you take a shopping list with all the items you need and go through the store once, picking everything up and checking out in a single transaction. Similarly, `BatchGetItem` allows you to pull multiple items from DynamoDB in one API call rather than making separate requests for each item.

# Common Mistakes / Traps
- A common mistake is to confuse `BatchGetItem` with `GetItem`, which retrieves a single item per call. Additionally, some might consider using `Scan` or `Query`, but those methods do not specifically target multiple individual items efficiently and can lead to higher read costs and performance issues.

# Memory Tip
- Remember: ""Batch for multiple, Single for one – save time with BatchGetItem."""
60,"[""A developer is creating a service that uses an Amazon S3 bucket for image\nuploads. The service will use an AWS Lambda function to create a thumbnail of\neach image. Each time an image is uploaded, the service needs to send an email\nnotification and create the thumbnail. The developer needs to configure the\nimage processing and email notifications setup.\n\nWhich solution will meet these requirements?""]",Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure S3 event notifications with a destination of the SNS topic. Subscribe the Lambda function to the SNS topic. Create an email notification subscription to the SNS topic.,Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure S3 event notifications with a destination of the SNS topic. Subscribe the Lambda function to the SNS topic. Create an Amazon Simple Queue Service (Amazon SQS) queue. Subscribe the SQS queue to the SNS topic. Create an email notification subscription to the SQS queue.,Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure S3 event notifications with a destination of the SQS queue. Subscribe the Lambda function to the SQS queue. Create an email notification subscription to the SQS queue.,Create an Amazon Simple Queue Service (Amazon SQS) queue. Send S3 event notifications to Amazon EventBridge. Create an EventBridge rule that runs the Lambda function when images are uploaded to the S3 bucket. Create an EventBridge rule that sends notifications to the SQS queue. Create an email notification subscription to the SQS queue.,"# Answer
- **Correct option:** A

- **Reason:** Option A is the most straightforward and efficient solution to meet the requirements of processing images and sending email notifications. By creating an Amazon SNS topic and configuring S3 event notifications to trigger when an object is uploaded to the S3 bucket, you can easily notify multiple subscribers. The Lambda function can then be subscribed to the SNS topic to process the image and create thumbnails, while an email notification subscription will ensure that notifications are sent whenever an image is uploaded.

# Example / Analogy
- Think of it like a notification system in a workplace. When a new project file (image) is uploaded to the shared drive (S3 bucket), everyone is alerted (SNS topic) about it, while a specific team (Lambda function) immediately starts working on it (creating a thumbnail). At the same time, a receptionist (email subscription) informs everyone that a new file is available.

# Common Mistakes / Traps
- A common mistake is to overlook the direct relationship between S3 and SNS notifications. Some might assume they need to add unnecessary layers (like SQS) when the simpler SNS solution is available. Additionally, people may confuse duties between SNS and SQS, not realizing that SNS can directly handle the email notifications.

# Memory Tip
- **Remember:** SNS = Simple Notifications for Subscriptions; it's the go-to for alerts and awareness of what's happening with your data."
61,"[""A developer has designed an application to store incoming data as JSON files in\nAmazon S3 objects. Custom business logic in an AWS Lambda function then\ntransforms the objects, and the Lambda function loads the data into an Amazon\nDynamoDB table. Recently, the workload has experienced sudden and significant\nchanges in traffic. The flow of data to the DynamoDB table is becoming\nthrottled.\n\nThe developer needs to implement a solution to eliminate the throttling and load\nthe data into the DynamoDB table more consistently.\n\nWhich solution will meet these requirements?""]",Refactor the Lambda function into two functions. Configure one function to transform the data and one function to load the data into the DynamoDB table. Create an Amazon Simple Queue Service (Amazon SQS) queue in between the functions to hold the items as messages and to invoke the second function.,Turn on auto scaling for the DynamoDB table. Use Amazon CloudWatch to monitor the table's read and write capacity metrics and to track consumed capacity.,Create an alias for the Lambda function. Configure provisioned concurrency for the application to use.,Refactor the Lambda function into two functions. Configure one function to store the data in the DynamoDB table. Configure the second function to process the data and update the items after the data is stored in DynamoDB. Create a DynamoDB stream to invoke the second function after the data is stored.,"# Answer
- **Correct option:** A

- **Reason:** Refactoring the Lambda function into two separate functions allows for better management of workloads and reduces throttling issues. By introducing an Amazon SQS queue, the first Lambda function can handle data transformation and place the messages in SQS, while the second function can consume messages from the queue and handle the loading into DynamoDB at its own pace. This decouples the processing and loading functions, thereby smoothing out the peaks in traffic and preventing DynamoDB from becoming overwhelmed.

# Example / Analogy
- Think of a restaurant where the kitchen (Lambda function) receives orders (data) to prepare meals (transform data) and serve them to the customers (DynamoDB). If too many customers arrive at once, the kitchen could get backed up (DynamoDB throttling). By adding a conveyor belt (SQS), the kitchen can prepare meals at its own pace and place them on the belt, while another staff member takes orders from the belt to serve to the customers. This way, the kitchen can focus on preparation without getting overwhelmed.

# Common Mistakes / Traps
- A common trap is to assume that simply increasing capacity (like auto-scaling for DynamoDB) will solve all performance issues. However, if the Lambda function is directly invoking DynamoDB at a high frequency, it could still lead to throttling, so breaking the workload into manageable parts through SQS is essential.

# Memory Tip
- ""Decouple processes with queues to manage peaks and avoid throttling."""
62,"[""A developer is creating an application. New users of the application must be\nable to create an account and register by using their own social media accounts.\n\nWhich AWS service or resource should the developer use to meet these\nrequirements?""]",IAM role,Amazon Cognito identity pools,Amazon Cognito user pools,AWS Directory Service,"# Answer
- **Correct option:** B : Amazon Cognito identity pools  

- **Reason:** Amazon Cognito identity pools allow users to authenticate using social identity providers (like Facebook, Google, etc.) and obtain AWS credentials, enabling them to access AWS services. This is exactly what is needed for an application where new users can register and create accounts using their social media accounts.

# Example / Analogy
- Think of Cognito identity pools like a VIP entrance at a concert where people can show their social media passes (like Facebook or Google) to get access. Instead of creating a new ticket for each person (like traditional user registration), they show their existing pass to enter.

# Common Mistakes / Traps
- A common mistake is confusing identity pools with user pools. While identity pools enable federated authentication using social providers, user pools are more about user registration and management without social identity integration.

# Memory Tip
- Remember: ""Identity pools for social sign-ins; User pools for managing users."""
63,"[""A social media application uses the AWS SDK for JavaScript on the frontend to\nget user credentials from AWS Security Token Service (AWS STS). The application\nstores its assets in an Amazon S3 bucket. The application serves its content by\nusing an Amazon CloudFront distribution with the origin set to the S3 bucket.\n\nThe credentials for the role that the application assumes to make the SDK calls\nare stored in plaintext in a JSON file within the application code. The\ndeveloper needs to implement a solution that will allow the application to get\nuser credentials without having any credentials hardcoded in the application\ncode.\n\nWhich solution will meet these requirements?""]",Add a Lambda@Edge function to the distribution. Invoke the function on viewer request. Add permissions to the function's execution role to allow the function to access AWS STS. Move all SDK calls from the frontend into the function.,Add a CloudFront function to the distribution. Invoke the function on viewer request. Add permissions to the function's execution role to allow the function to access AWS STS. Move all SDK calls from the frontend into the function.,Add a Lambda@Edge function to the distribution. Invoke the function on viewer request. Move the credentials from the JSON file into the function. Move all SDK calls from the frontend into the function.,Add a CloudFront function to the distribution. Invoke the function on viewer request. Move the credentials from the JSON file into the function. Move all SDK calls from the frontend into the function.,"# Answer
- **Correct option:** A

- **Reason:** 
  The correct solution is to add a Lambda@Edge function to the CloudFront distribution that will invoke on viewer requests. This function can assume a role to access AWS STS, allowing it to generate temporary, secure credentials without hardcoding any sensitive information in the frontend application. By moving all SDK calls from the frontend to the Lambda@Edge function, you eliminate the exposure of credentials.

# Example / Analogy
- Think of Lambda@Edge as a security guard at the entrance of a concert (your application). Instead of allowing every attendee (the frontend code) to access the backstage (AWS STS) where sensitive operations happen, the guard (Lambda@Edge) checks the tickets and only lets authorized personnel in to interact with the backstage securely.

# Common Mistakes / Traps
- A common mistake is to think that simply moving credentials to a function makes it secure. Storing credentials within the function can still pose security risks if not carefully managed. Always ensure that the code does not expose sensitive information.

# Memory Tip
- **Lambda@Edge: Your secure gatekeeper for handling sensitive AWS interactions on behalf of your frontend.**"
64,"[""An ecommerce website uses an AWS Lambda function and an Amazon RDS for MySQL\ndatabase for an order fulfillment service. The service needs to return order\nconfirmation immediately.\n\nDuring a marketing campaign that caused an increase in the number of orders, the\nwebsite's operations team noticed errors for “too many connections” from Amazon\nRDS. However, the RDS DB cluster metrics are healthy. CPU and memory capacity\nare still available.\n\nWhat should a developer do to resolve the errors?""]",Initialize the database connection outside the handler function. Increase the max_user_connections value on the parameter group of the DB cluster. Restart the DB cluster.,Initialize the database connection outside the handler function. Use RDS Proxy instead of connecting directly to the DB cluster.,Use Amazon Simple Queue Service (Amazon SQS) FIFO queues to queue the orders. Ingest the orders into the database. Set the Lambda function's concurrency to a value that equals the number of available database connections.,Use Amazon Simple Queue Service (Amazon SQS) FIFO queues to queue the orders. Ingest the orders into the database. Set the Lambda function's concurrency to a value that is less than the number of available database connections.,"# Answer
- **Correct option:** B
- **Reason:** Initializing the database connection outside the handler function allows for connection reuse across Lambda invocations, reducing the overhead of establishing a new connection for each request. Additionally, using RDS Proxy helps to manage connections efficiently and scale the authorized connections without hitting the database directly, mitigating ""too many connections"" errors.

# Example / Analogy
- Imagine trying to enter a busy restaurant where there’s a line at the door. If every customer had to talk to the manager to get a table every time they walked in, it would slow things down. Instead, if you had a receptionist (like RDS Proxy) handling the flow, she could manage the guests and ensure that customers could get seated without overwhelming the manager (the RDS database).

# Common Mistakes / Traps
- A common mistake is assuming that simply increasing the max_user_connections will solve the problem without addressing connection management or reusability.
- Another trap is initializing connections inside the handler function, leading to inefficient use of database connections and potential exhaustion during peaks.

# Memory Tip
- ""Optimize connections: Use RDS Proxy for efficient database access!"""
65,"[""A company stores its data in data tables in a series of Amazon S3 buckets. The\ncompany received an alert that customer credit card information might have been\nexposed in a data table on one of the company's public applications. A developer\nneeds to identify all potential exposures within the application environment.\n\nWhich solution will meet these requirements?""]",Use Amazon Athena to run a job on the S3 buckets that contain the affected data. Filter the findings by using the SensitiveData:S3Object/Personal finding type.,Use Amazon Macie to run a job on the S3 buckets that contain the affected data. Filter the findings by using the SensitiveData:S3Object/Financial finding type.,Use Amazon Macie to run a job on the S3 buckets that contain the affected data. Filter the findings by using the SensitiveData:S3Object/Personal finding type.,Use Amazon Athena to run a job on the S3 buckets that contain the affected data. Filter the findings by using the SensitiveData:S3Object/Financial finding type.,"# Answer
- **Correct option:** B  
- **Reason:** Amazon Macie is specifically designed for discovering and protecting sensitive data, including financial data like credit card information. By filtering with the SensitiveData:S3Object/Financial finding type, Macie can help identify all occurrences of this type of sensitive information in your S3 buckets, meeting the company's requirement to pinpoint potential exposures.

# Example / Analogy
- Think of Amazon Macie as a security guard for your data; just like a guard who inspects bags for cash or credit cards, Macie scans your S3 buckets for sensitive data, ensuring nothing is overlooked. If you suspect cash might be missing, you’d want the guard checking for that specific item—just as you want Macie filtering specifically for financial data when there’s a risk of credit card exposure.

# Common Mistakes / Traps
- A common mistake is confusing Amazon Macie with Amazon Athena. While Athena is great for querying data in S3, it does not specialize in identifying sensitive information. Users might incorrectly assume they can use Athena for this purpose when they need Macie's specialized capabilities.

# Memory Tip
- **Remember:** Use **Macie** for **sensitive data** discovery; it’s your **data security expert** in the AWS ecosystem!"
66,"[""A software company is launching a multimedia application. The application will\nallow guest users to access sample content before the users decide if they want\nto create an account to gain full access. The company wants to implement an\nauthentication process that can identify users who have already created an\naccount. The company also needs to keep track of the number of guest users who\neventually create an account.\n\nWhich combination of steps will meet these requirements? (Choose two.)""]",Create an Amazon Cognito user pool. Configure the user pool to allow unauthenticated users. Exchange user tokens for temporary credentials that allow authenticated users to assume a role.,Create an Amazon Cognito identity pool. Configure the identity pool to allow unauthenticated users. Exchange unique identity for temporary credentials that allow all users to assume a role.,Create an Amazon CloudFront distribution. Configure the distribution to allow unauthenticated users. Exchange user tokens for temporary credentials that allow all users to assume a role.,Create a role for authenticated users that allows access to all content. Create a role for unauthenticated users that allows access to only the sample content.,"# Answer
- **Correct option:** A and B
- **Reason:** Option A involves using an Amazon Cognito user pool which can authenticate users and recognize existing accounts. Option B complements this by using an Amazon Cognito identity pool which allows both authenticated and unauthenticated users to access temporary credentials. This setup lets the application track guest users turning into registered users while providing a seamless experience.

# Example / Analogy
- Think of a library where anyone can enter (guest access) and browse a shelf of sample books (sample content). Once they decide they want to check out a book (create an account), they sign up and get a library card (authentication). The library can now keep track of how many visitors went from browsing to becoming members.

# Common Mistakes / Traps
- A common mistake is assuming that using only user pools or only identity pools will suffice. In this scenario, both are necessary to handle guest access while allowing for authenticated identity verification.

# Memory Tip
- Remember: **Cognito = Guest Browsing + User Authentication = Seamless User Experience!**"
67,"[""A company is updating an application to move the backend of the application from\nAmazon EC2 instances to a serverless model. The application uses an Amazon RDS\nfor MySQL DB instance and runs in a single VPC on AWS. The application and the\nDB instance are deployed in a private subnet in the VPC.\n\nThe company needs to connect AWS Lambda functions to the DB instance.\n\nWhich solution will meet these requirements?""]",Create Lambda functions inside the VPC with the AWSLambdaBasicExecutionRole policy attached to the Lambda execution role. Modify the RDS security group to allow inbound access from the Lambda security group.,Create Lambda functions inside the VPC with the AWSLambdaVPCAccessExecutionRole policy attached to the Lambda execution role. Modify the RDS security group to allow inbound access from the Lambda security group.,Create Lambda functions with the AWSLambdaBasicExecutionRole policy attached to the Lambda execution role. Create an interface VPC endpoint for the Lambda functions. Configure the interface endpoint policy to allow the lambda:InvokeFunclion action for each Lambda function's Amazon Resource Name (ARN).,Create Lambda functions with the AWSLambdaVPCAccessExecutionRole policy attached to the Lambda execution role. Create an interface VPC endpoint for the Lambda functions. Configure the interface endpoint policy to allow the lambda:InvokeFunction action for each Lambda function's Amazon Resource Name (ARN).,"# Answer
- **Correct option:** B
- **Reason:** To allow AWS Lambda functions to access resources within a VPC, the Lambda functions must be configured to run inside the VPC by selecting appropriate subnets and security groups. The AWSLambdaVPCAccessExecutionRole policy grants the necessary permissions for Lambda to access resources in the VPC, including the Amazon RDS instance. Additionally, modifying the RDS security group to allow inbound access from the Lambda security group ensures that traffic from the Lambda functions to the RDS database is permitted.

# Example / Analogy
- Think of AWS Lambda functions as guests in a private club (VPC), and the Amazon RDS instance as a specific room in that club. The AWSLambdaVPCAccessExecutionRole policy is like a special invitation (membership access) that allows the guests (Lambda functions) to enter the club and access the room (RDS instance). If the room (RDS) doesn’t allow guests from the club (via security group rules), they won't be able to access it even if they have the right invitation.

# Common Mistakes / Traps
- A common mistake is to choose options that do not include the AWSLambdaVPCAccessExecutionRole policy, believing that basic execution permissions are sufficient. Another potential trap is misunderstanding how the security group rules work; it's crucial to allow inbound access from the Lambda function's security group to the RDS instance.

# Memory Tip
- Remember: ""VPC access needs the right role (AWSLambdaVPCAccessExecutionRole) AND correct security groups!"""
68,"[""A company has a web application that runs on Amazon EC2 instances with a custom\nAmazon Machine Image (AMI). The company uses AWS CloudFormation to provision the\napplication. The application runs in the us-east-1 Region, and the company needs\nto deploy the application to the us-west-1 Region.\n\nAn attempt to create the AWS CloudFormation stack in us-west-1 fails. An error\nmessage states that the AMI ID does not exist. A developer must resolve this\nerror with a solution that uses the least amount of operational overhead.\n\nWhich solution meets these requirements?""]",Change the AWS CloudFormation templates for us-east-1 and us-west-1 to use an AWS AMI. Relaunch the stack for both Regions.,Copy the custom AMI from us-east-1 to us-west-1. Update the AWS CloudFormation template for us-west-1 to refer to AMI ID for the copied AMI. Relaunch the stack.,Build the custom AMI in us-west-1. Create a new AWS CloudFormation template to launch the stack in us-west-1 with the new AMI ID.,Manually deploy the application outside AWS CloudFormation in us-west-1.,"# Answer
- **Correct option:** B

- **Reason:** Copying the custom AMI from us-east-1 to us-west-1 allows you to retain the configured image for use in the new region. Updating the AWS CloudFormation template to reference the new AMI ID ensures that the stack can be launched successfully in us-west-1. This approach minimizes operational overhead because it leverages the existing settings and infrastructure while only requiring a few straightforward updates.

# Example / Analogy
- Imagine you have a favorite recipe for a cake (the custom AMI) that you typically bake in one kitchen (us-east-1). If you want to bake the same cake in another kitchen (us-west-1), you wouldn't start from scratch; instead, you would copy your recipe to that kitchen, slightly adjust it to fit the new environment, and then bake. This is similar to copying the AMI to the new region and modifying the CloudFormation template.

# Common Mistakes / Traps
- A common pitfall is to choose option A, which suggests using a generic AMI instead of the custom one. This might not satisfy specific configurations or dependencies that the original custom AMI provided.
- Another mistake could be thinking that updating the CloudFormation template alone (without the AMI copy) would work, which is incorrect since the specified AMI ID will not exist in the target region initially.

# Memory Tip
- ""Copy your recipe (AMI) to the new kitchen (region) before baking (deploying)!"""
69,"[""A developer is updating several AWS Lambda functions and notices that all the\nLambda functions share the same custom libraries. The developer wants to\ncentralize all the libraries, update the libraries in a convenient way, and keep\nthe libraries versioned.\n\nWhich solution will meet these requirements with the LEAST development effort?""]",Create an AWS CodeArtifact repository that contains all the custom libraries.,Create a custom container image for the Lambda functions to save all the custom libraries.,Create a Lambda layer that contains all the custom libraries.,Create an Amazon Elastic File System (Amazon EFS) file system to store all the custom libraries.,"# Answer
- **Correct option:** C : Create a Lambda layer that contains all the custom libraries.
- **Reason:** Lambda layers allow you to package and manage shared libraries separately from your Lambda function code. This means you can easily update the library versions, share them among multiple functions, and keep the libraries organized. Using layers is a built-in feature of AWS Lambda that minimizes additional development effort while ensuring version control and centralized management.

# Example / Analogy
- Consider a school where every student (Lambda function) needs to use the same textbooks (custom libraries) for different subjects. Instead of each student carrying their own heavy textbooks (bloating each function), the school provides a central library (Lambda layer) where all the textbooks are stored. Students can easily borrow the books they need, and if a new edition comes out, the school simply updates it in the library, making it available for all students instantly.

# Common Mistakes / Traps
- A common mistake is to think that packaging libraries in a container (Option B) is necessary. While this works, it introduces more complexity compared to using layers. Another mistake could be underestimating Lambda layers' ability to manage shared code efficiently and not leveraging this feature when it’s ideal for the scenario.

# Memory Tip
- **Think of Lambda layers as a centralized library for your Lambda functions’ shared resources.**"
70,"[""A company is providing read access to objects in an Amazon S3 bucket for\ndifferent customers. The company uses IAM permissions to restrict access to the\nS3 bucket. The customers can access only their own files.\n\nDue to a regulation requirement, the company needs to enforce encryption in\ntransit for interactions with Amazon S3.\n\nWhich solution will meet these requirements?""]",Add a bucket policy to the S3 bucket to deny S3 actions when the aws:SecureTransport condition is equal to false.,Add a bucket policy to the S3 bucket to deny S3 actions when the s3:x-amz-acl condition is equal to public-read.,Add an IAM policy to the IAM users to enforce the usage of the AWS SDK.,Add an IAM policy to the IAM users that allows S3 actions when the s3:x-amz-acl condition is equal to bucket-owner-read.,"# Answer
- **Correct option:** A
- **Reason:** Adding a bucket policy to the S3 bucket that denies S3 actions when the `aws:SecureTransport` condition is equal to `false` ensures that all interactions with the S3 bucket are encrypted in transit. This means that any requests made over non-secure (HTTP) connections will be denied, thereby enforcing secure interaction with the S3 resources.

# Example / Analogy
- Think of it like a secure building that requires all people entering to pass through a metal detector (encryption) to ensure safety. If someone tries to enter without going through the detector (non-secure connection), they are turned away. Similarly, the bucket policy acts like that security measure, only allowing access via secure methods.

# Common Mistakes / Traps
- A common trap is misunderstanding the specific conditions that allow or restrict access. Focusing on other options that do not directly relate to enforcing encryption in transit can lead to selecting the wrong answer. Always check if the option is directly aligned with the requirement of enforcing secure connections.

# Memory Tip
- Remember: ""Secure Transport = Allowed; Non-Secure = Denied."""
71,"[""A company has an image storage web application that runs on AWS. The company\nhosts the application on Amazon EC2 instances in an Auto Scaling group. The Auto\nScaling group acts as the target group for an Application Load Balancer (ALB)\nand uses an Amazon S3 bucket to store the images for sale.\n\nThe company wants to develop a feature to test system requests. The feature will\ndirect requests to a separate target group that hosts a new beta version of the\napplication.\n\nWhich solution will meet this requirement with the LEAST effort?""]",Create a new Auto Scaling group and target group for the beta version of the application. Update the ALB routing rule with a condition that looks for a cookie named version that has a value of beta. Update the test system code to use this cookie to test the beta version of the application.,"Create a new ALB, Auto Scaling group, and target group for the beta version of the application. Configure an alternate Amazon Route 53 record for the new ALB endpoint. Use the alternate Route 53 endpoint in the test system requests to test the beta version of the application.","Create a new ALB, Auto Scaling group, and target group for the beta version of the application. Use Amazon CloudFront with Lambda@Edge to determine which specific request will go to the new ALB. Use the CloudFront endpoint to send the test system requests to test the beta version of the application.",Create a new Auto Scaling group and target group for the beta version of the application. Update the ALB routing rule with a condition that looks for a cookie named version that has a value of beta. Use Amazon CloudFront with Lambda@Edge to update the test system requests to add the required cookie when the requests go to the ALB.,"# Answer
- **Correct option:** A

- **Reason:** This option involves creating a new Auto Scaling group and target group for the beta version of the application which is the least effort solution. It suggests updating the existing Application Load Balancer (ALB) routing rule to direct traffic based on a specific cookie named ""version"" with a value of ""beta"". This allows the company to easily control which requests go to the beta version without needing significant infrastructure changes, like setting up a new ALB or using more complex solutions.

# Example / Analogy
- Think of it like a restaurant offering a special seasonal menu. Instead of creating a separate restaurant (like a new ALB), they could simply have a waiter ask customers if they’d like to try dishes from the special menu (using a cookie to identify the request) while still operating under the same establishment (the existing ALB).

# Common Mistakes / Traps
- A common mistake might be to overcomplicate the solution by assuming that a completely new system (like setting up additional infrastructure) is necessary instead of leveraging existing components. 

# Memory Tip
- Keep it simple: Use cookies to direct traffic rather than overengineering with new infrastructure."
72,"[""A team is developing an application that is deployed on Amazon EC2 instances.\nDuring testing, the team receives an error. The EC2 instances are unable to\naccess an Amazon S3 bucket.\n\nWhich steps should the team take to troubleshoot this issue? (Choose two.)""]",Check whether the policy that is assigned to the IAM role that is attached to the EC2 instances grants access to Amazon S3.,Check the S3 bucket policy to validate the access permissions for the S3 bucket.,Check whether the policy that is assigned to the IAM user that is attached to the EC2 instances grants access to Amazon S3.,Check the S3 Lifecycle policy to validate the permissions that are assigned to the S3 bucket.,"# Answer
- **Correct option:** A, B
- **Reason:**  
  To troubleshoot the inability of EC2 instances to access an S3 bucket, the team should first confirm that the IAM role attached to the EC2 instances has the correct permissions (Option A). This involves checking the policies to ensure that they grant the necessary access to Amazon S3. Additionally, the team should check the S3 bucket policy (Option B) to verify that it allows the EC2 instances (via their IAM role) to perform the required actions (like Read or Write). Both of these policies work together to control access.

# Example / Analogy
- Imagine a library (S3 bucket) that requires a library card (IAM role) to borrow books. If the library card does not have permission to borrow books (the IAM role lacks S3 permissions), or if the library enforces rules that only allow certain cardholders to borrow (the S3 bucket policy), the person won’t be able to borrow books. Checking both the card and the library's borrowing rules is essential to solving the issue.

# Common Mistakes / Traps
- A common mistake is to focus only on either the IAM role or the S3 bucket policy, neglecting to check both. Understanding that both policies interact is key to identifying access issues.

# Memory Tip
- Always check IAM roles and S3 bucket policies for access issues."
73,"[""A developer is working on an ecommerce website. The developer wants to review\nserver logs without logging in to each of the application servers individually.\nThe website runs on multiple Amazon EC2 instances, is written in Python, and\nneeds to be highly available.\n\nHow can the developer update the application to meet these requirements with\nMINIMUM changes?""]","Rewrite the application to be cloud native and to run on AWS Lambda, where the logs can be reviewed in Amazon CloudWatch.","Set up centralized logging by using Amazon OpenSearch Service, Logstash, and OpenSearch Dashboards.",Scale down the application to one larger EC2 instance where only one instance is recording logs.,Install the unified Amazon CloudWatch agent on the EC2 instances. Configure the agent to push the application logs to CloudWatch.,"# Answer
- **Correct option:** D : Install the unified Amazon CloudWatch agent on the EC2 instances. Configure the agent to push the application logs to CloudWatch.

- **Reason:** This option allows the developer to centrally manage and review logs from multiple EC2 instances without significant changes to the existing application architecture. By installing the CloudWatch agent, logs can be automatically pushed to CloudWatch, where they can be aggregated, searched, and analyzed, thus meeting the requirement for easy log review.

# Example / Analogy
- Think of the application servers as individual book authors working in different rooms (EC2 instances). Instead of going to each author's room to read their notes (logs), you could set up a central library (CloudWatch) where each author submits their notes for everyone to read and review in one place. 

# Common Mistakes / Traps
- A common mistake is to assume that rewriting the application to be cloud-native or changing the architecture completely (like option A) is necessary. This can lead to unnecessary complexity and resource expenditure. Additionally, options that limit log access (like option C) do not fulfill the requirement for reviewing logs easily.

# Memory Tip
- Remember: ""CloudWatch gathers logs, so you don't have to visit each server's logs."""
74,"[""A company is creating an application that processes .csv files from Amazon S3. A\ndeveloper has created an S3 bucket. The developer has also created an AWS Lambda\nfunction to process the .csv files from the S3 bucket.\n\nWhich combination of steps will invoke the Lambda function when a .csv file is\nuploaded to Amazon S3? (Choose two.)""]",Create an Amazon EventBridge rule. Configure the rule with a pattern to match the S3 object created event.,Schedule an Amazon EventBridge rule to run a new Lambda function to scan the S3 bucket.,Add a trigger to the existing Lambda function. Set the trigger type to EventBridge. Select the Amazon EventBridge rule.,Create a new Lambda function to scan the S3 bucket for recently added S3 objects.,"# Answer
- **Correct option:** A and C
- **Reason:** To set up a trigger for the Lambda function to invoke when a .csv file is uploaded to an S3 bucket, you need to create an EventBridge rule that matches the S3 object created event (option A). In addition, you can directly add an S3 bucket trigger to the existing Lambda function (option C) without needing to use EventBridge. However, since the question requires two steps, the combination of these two options (A and C) effectively accomplishes the goal of invoking the Lambda function.

# Example / Analogy
- Think of it like setting up a doorbell and then telling a friend that when the doorbell rings (a new .csv file is uploaded), they should come over and take care of the visitor (invoke the Lambda function). The doorbell (EventBridge rule) alerts you, and you might also directly tell your friend to watch for visitors (the direct S3 trigger).

# Common Mistakes / Traps
- A common mistake is to think that only the EventBridge rule (option A) is needed for this task. However, the S3 bucket can directly trigger the Lambda function, making option C an essential step as well. Also, avoid confusing EventBridge with other services like scheduled events when it comes to S3 object events.

# Memory Tip
- Remember: ""S3 triggers Lambda directly and can also notify EventBridge."""
75,"[""A developer needs to build an AWS CloudFormation template that self-populates\nthe AWS Region variable that deploys the CloudFormation template.\n\nWhat is the MOST operationally efficient way to determine the Region in which\nthe template is being deployed?""]",Use the AWS::Region pseudo parameter.,Require the Region as a CloudFormation parameter.,Find the Region from the AWS::StackId pseudo parameter by using the Fn::Split intrinsic function.,Dynamically import the Region by referencing the relevant parameter in AWS Systems Manager Parameter Store.,"# Answer
- **Correct option:** A : Use the AWS::Region pseudo parameter.
- **Reason:** The AWS::Region pseudo parameter is specifically designed to return the Region in which the CloudFormation stack is being created. It is a built-in feature that automatically populates the value without requiring any additional parameters or complex logic. This makes it the most operationally efficient option.

# Example / Analogy
- Imagine you’re sending a package and you want to label it with the destination city. Instead of writing the city name yourself, you could use a smart address label that automatically detects your current location and fills it in for you. The AWS::Region pseudo parameter does exactly that for the CloudFormation template—automatically populating the region based on where it's being deployed.

# Common Mistakes / Traps
- A common mistake is assuming that requiring the region as a parameter (Option B) is viable. However, it adds unnecessary complexity and increases the chance of user error since it relies on manual input. Similarly, relying on the AWS::StackId (Option C) or Systems Manager Parameter Store (Option D) introduces additional steps and potential for error when the AWS::Region parameter is readily available.

# Memory Tip
- ""AWS::Region: the easiest way to know where you are!"""
76,"[""A company has hundreds of AWS Lambda functions that the company's QA team needs\nto test by using the Lambda function URLs. A developer needs to configure the\nauthentication of the Lambda functions to allow access so that the QA IAM group\ncan invoke the Lambda functions by using the public URLs.\n\nWhich solution will meet these requirements?""]",Create a CLI script that loops on the Lambda functions to add a Lambda function URL with the AWS_IAM auth type. Run another script to create an IAM identity-based policy that allows the lambda:InvokeFunctionUrl action to all the Lambda function Amazon Resource Names (ARNs). Attach the policy to the QA IAM group.,Create a CLI script that loops on the Lambda functions to add a Lambda function URL with the NONE auth type. Run another script to create an IAM resource-based policy that allows the lambda:InvokeFunctionUrl action to all the Lambda function Amazon Resource Names (ARNs). Attach the policy to the QA IAM group.,Create a CLI script that loops on the Lambda functions to add a Lambda function URL with the AWS_IAM auth type. Run another script to loop on the Lambda functions to create an IAM identity-based policy that allows the lambda:InvokeFunctionUrl action from the QA IAM group's Amazon Resource Name (ARN).,Create a CLI script that loops on the Lambda functions to add a Lambda function URL with the NONE auth type. Run another script to loop on the Lambda functions to create an IAM resource-based policy that allows the lambda:InvokeFunctionUrl action from the QA IAM group's Amazon Resource Name (ARN).,"# Answer
- **Correct option:** A

- **Reason:** Option A correctly specifies to use the AWS_IAM authentication type for Lambda function URLs, which ensures that only users or groups with valid IAM permissions (in this case, the QA IAM group) can access the Lambda functions via their public URLs. The proposed solution also includes creating an IAM identity-based policy that explicitly allows the necessary action (lambda:InvokeFunctionUrl) for the Lambda function ARNs and attaching it to the QA IAM group, which is essential for managing permissions effectively.

# Example / Analogy
- Think of it like granting access to a secure building (the Lambda function) that can only be entered by employees with valid identification (AWS IAM permissions). The LDAP function URLs with AWS_IAM authentication act like security doors that only open for those who show their ID (valid IAM credentials), ensuring that only the authorized QA team can access the building.

# Common Mistakes / Traps
- A common mistake could be selecting an option that uses the NONE authentication type (as seen in options B and D), which would allow anyone with the URL to invoke the Lambda function, completely bypassing the IAM permissions and risking unauthorized access.

# Memory Tip
- Always remember: ""AWS_IAM for safe access—control who can invoke!"""
77,"[""A developer maintains a critical business application that uses Amazon DynamoDB\nas the primary data store. The DynamoDB table contains millions of documents and\nreceives 30-60 requests each minute. The developer needs to perform processing\nin near-real time on the documents when they are added or updated in the\nDynamoDB table.\n\nHow can the developer implement this feature with the LEAST amount of change to\nthe existing application code?""]",Set up a cron job on an Amazon EC2 instance. Run a script every hour to query the table for changes and process the documents.,Enable a DynamoDB stream on the table. Invoke an AWS Lambda function to process the documents.,Update the application to send a PutEvents request to Amazon EventBridge. Create an EventBridge rule to invoke an AWS Lambda function to process the documents.,Update the application to synchronously process the documents directly after the DynamoDB write.,"# Answer
- **Correct option:** B
- **Reason:** Enabling a DynamoDB stream allows the application to capture changes (inserts, updates, deletes) in real-time and trigger an AWS Lambda function to process those changes without needing extensive modifications to the existing application code.

# Example / Analogy
- Think of DynamoDB Streams like a security camera that records everything happening in a room (your DynamoDB table). When something changes in the room (like adding or editing a document), the camera captures that event and can alert someone (a Lambda function) to take action right away, all without needing to change the existing furniture (your application code).

# Common Mistakes / Traps
- A common mistake is choosing options that involve significant changes to the application architecture, such as using a cron job or synchronous processing. These methods are less efficient and would entail more development effort.

# Memory Tip
- Remember: ""Streams allow us to react in real-time while keeping our app's core unchanged.” "
78,"[""A developer is writing an application for a company. The application will be\ndeployed on Amazon EC2 and will use an Amazon RDS for Microsoft SQL Server\ndatabase. The company's security team requires that database credentials are\nrotated at least weekly.\n\nHow should the developer configure the database credentials for this\napplication?""]",Create a database user. Store the user name and password in an AWS Systems Manager Parameter Store secure string parameter. Enable rotation of the AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter.,Enable IAM authentication for the database. Create a database user for use with IAM authentication. Enable password rotation.,Create a database user. Store the user name and password in an AWS Secrets Manager secret that has daily rotation enabled.,Use the EC2 user data to create a database user. Provide the user name and password in environment variables to the application.,"# Answer
- **Correct option:** C
- **Reason:** Storing the database user name and password in AWS Secrets Manager allows for automatic and secure handling of credentials with built-in support for rotation. Secrets Manager can rotate the secrets on a schedule (like daily), which complies with the requirement of rotating credentials at least weekly.

# Example / Analogy
- Think of AWS Secrets Manager like a secure vault for your application credentials. Just like a safe that can automatically change the combination lock every day to keep your valuables secure, Secrets Manager keeps your database credentials safe and rotates them automatically per your specifications.

# Common Mistakes / Traps
- A common mistake is to underestimate the importance of using a dedicated secrets management service. Options like using AWS Systems Manager Parameter Store or storing credentials in environment variables could lead to security vulnerabilities or complicate the rotation requirements. It's essential to use services that automatically handle the security and rotation of sensitive data.

# Memory Tip
- Use a ""secret vault"" like Secrets Manager to keep credentials safe and rotated automatically."
79,"[""A real-time messaging application uses Amazon API Gateway WebSocket APIs with\nbackend HTTP service. A developer needs to build a feature in the application to\nidentify a client that keeps connecting to and disconnecting from the WebSocket\nconnection. The developer also needs the ability to remove the client.\n\nWhich combination of changes should the developer make to the application to\nmeet these requirements? (Choose two.)""]",Switch to HTTP APIs in the backend service.,Switch to REST APIs in the backend service.,Use the callback URL to disconnect the client from the backend service.,Add code to track the client status in Amazon ElastiCache in the backend service.,"# Answer
- **Correct options:** D, C

- **Reason:** 
  - To track client status and identify clients connecting and disconnecting from a WebSocket API, the developer needs an efficient way to store and manage that state information. Option D suggests using Amazon ElastiCache, which is an excellent choice for maintaining high-performance states for multiple connections, allowing for quick retrieval of client status. 
  - Option C provides a mechanism to disconnect the client from the backend service through a callback URL, which allows the application to communicate effectively back to the client that it should disconnect. This combination aligns with the need to manage client connections dynamically.

# Example / Analogy
- Think of a real-time messaging application like a party where guests (clients) continuously come and go. You need a guest list (ElastiCache) to keep track of who is currently at the party and their status. If someone needs to leave (disconnection), you'd call them (callback URL) and politely ask them to exit the venue.

# Common Mistakes / Traps
- A common trap is to choose options that suggest switching APIs without addressing the core need for tracking client connections. Options A and B imply changing the API type without providing a solution to managing client states, which is crucial for this requirement.

# Memory Tip
- Track client status effectively with ElastiCache and use callback URLs for disconnections."
80,"[""A developer has written code for an application and wants to share it with other\ndevelopers on the team to receive feedback. The shared application code needs to\nbe stored long-term with multiple versions and batch change tracking.\n\nWhich AWS service should the developer use?""]",AWS CodeBuild,Amazon S3,AWS CodeCommit,AWS Cloud9,"# Answer
- **Correct option:** C: AWS CodeCommit
- **Reason:** AWS CodeCommit is a source control service that allows you to host secure and scalable Git repositories. It fits the requirements since it supports versioning and change tracking, enabling the developer to share their code with the team effectively.

# Example / Analogy
- Think of AWS CodeCommit as a shared digital library where multiple developers can not only deposit their books (code) but also check out past editions (versions) whenever needed. Just like a library keeps track of all versions of its books, CodeCommit enables tracking of changes in application code.

# Common Mistakes / Traps
- A common mistake is confusing CodeCommit with other AWS services like Amazon S3, which is primarily for object storage and does not offer version control and collaborative features suitable for code development.

# Memory Tip
- Remember: CodeCommit is for ""Code Collaboration and Versioning."""
81,"[""A company's developer is building a static website to be deployed in Amazon S3\nfor a production environment. The website integrates with an Amazon Aurora\nPostgreSQL database by using an AWS Lambda function. The website that is\ndeployed to production will use a Lambda alias that points to a specific version\nof the Lambda function.\n\nThe company must rotate the database credentials every 2 weeks. Lambda functions\nthat the company deployed previously must be able to use the most recent\ncredentials.\n\nWhich solution will meet these requirements?""]",Store the database credentials in AWS Secrets Manager. Turn on rotation. Write code in the Lambda function to retrieve the credentials from Secrets Manager.,Include the database credentials as part of the Lambda function code. Update the credentials periodically and deploy the new Lambda function.,Use Lambda environment variables. Update the environment variables when new credentials are available.,Store the database credentials in AWS Systems Manager Parameter Store. Turn on rotation. Write code in the Lambda function to retrieve the credentials from Systems Manager Parameter Store.,"# Answer
- **Correct option:** A

- **Reason:** Storing the database credentials in AWS Secrets Manager allows for secure secret management, automatic rotation of credentials, and easy retrieval within Lambda functions. Turning on rotation ensures that each Lambda function can always access the most recent credentials without needing manual updates or redeployments.

# Example / Analogy
- Imagine using a secure vault for keeping your house keys. Instead of storing spare keys in various places (which could get lost or stolen), you use a trusty vault that securely manages your keys and can create new ones at regular intervals. You simply access the vault whenever you need the most current key, just as the Lambda function accesses Secrets Manager for the most up-to-date database credentials.

# Common Mistakes / Traps
- A common mistake is opting for environment variables or hardcoding credentials within Lambda functions, which poses security risks and does not support automated rotation. Additionally, many may overlook the implications of managing secrets securely, leading to vulnerabilities.

# Memory Tip
- Use ""Secrets Manager for Secrets"" to remember that AWS Secrets Manager is your go-to for securely storing and rotating sensitive information."
82,"[""A developer is developing an application that uses signed requests (Signature\nVersion 4) to call other AWS services. The developer has created a canonical\nrequest, has created the string to sign, and has calculated signing information.\n\nWhich methods could the developer use to complete a signed request? (Choose\ntwo.)""]",Add the signature to an HTTP header that is named Authorization.,Add the signature to a session cookie.,Add the signature to an HTTP header that is named Authentication.,Add the signature to a query string parameter that is named X-Amz-Signature.,"# Answer
- **Correct option:** A and D
- **Reason:** To complete a signed request using Signature Version 4, the developer can add the signature to an HTTP header named Authorization (option A) or to a query string parameter named X-Amz-Signature (option D). The Authorization header is the standard method for sending credentials in AWS, while using a query string parameter is also acceptable for certain use cases, such as when the request is being made via a web browser.

# Example / Analogy
- Imagine sending a letter through the mail. You can either write your signature directly on the letter (like adding it to the Authorization header) or include it in a special envelope that highlights your signature (similar to adding it to a query string parameter). Both methods verify your identity and intent of sending the letter.

# Common Mistakes / Traps
- A common trap is selecting the incorrect option regarding how the signature should be passed. Some might confuse ""Authentication"" (which is not a valid header for AWS requests) or think cookies can be used like Microsoft signatures, but that’s not applicable in AWS’s signed request methodology.

# Memory Tip
- Remember: Signatures go in the Authorization header or as a query parameter X-Amz-Signature!"
83,"[""A company must deploy all its Amazon RDS DB instances by using AWS\nCloudFormation templates as part of AWS CodePipeline continuous integration and\ncontinuous delivery (CI/CD) automation. The primary password for the DB instance\nmust be automatically generated as part of the deployment process.\n\nWhich solution will meet these requirements with the LEAST development effort?""]",Create an AWS Lambda-backed CloudFormation custom resource. Write Lambda code that generates a secure string. Return the value of the secure string as a data field of the custom resource response object. Use the CloudFormation Fn::GetAtt intrinsic function to get the value of the secure string. Use the value to create the DB instance.,Use the AWS CodeBuild action of CodePipeline to generate a secure string by using the following AWS CLI command: aws secretsmanager get-random-password. Pass the generated secure string as a CloudFormation parameter with the NoEcho attribute set to true. Use the parameter reference to create the DB instance.,Create an AWS Lambda-backed CloudFormation custom resource. Write Lambda code that generates a secure string. Return the value of the secure string as a data field of the custom resource response object. Use the CloudFormation Fn::GetAtt intrinsic function to get a value of the secure string. Create secrets in AWS Secrets Manager. Use the secretsmanager dynamic reference to use the value stored in the secret to create the DB instance.,Use the AWS::SecretsManager::Secret resource to generate a secure string. Store the secure string as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to use the value stored in the secret to create the DB instance.,"# Answer
- **Correct option:** D 
- **Reason:** This option uses the AWS::SecretsManager::Secret resource to generate and store a secure string/password in AWS Secrets Manager directly as part of the CloudFormation template. This method is straightforward and incorporates best practices by leveraging Secrets Manager for secure password storage. It simplifies the process while adhering to AWS's security and resource management best practices, meeting the requirements with minimal additional effort.

# Example / Analogy
- Think of AWS Secrets Manager like a vault where you can keep your passwords safe and easily access them whenever needed. By using AWS CloudFormation to store the password directly into this vault, it’s like ensuring that instead of carrying a key in your pocket, it’s securely locked in a safe, accessed only when necessary.

# Common Mistakes / Traps
- A common mistake might be to think that using additional Lambda functions or complex custom resources is always necessary. This can lead to over-engineering solutions. Often, native solutions like AWS Secrets Manager can accomplish the task with less complexity and better maintainability.

# Memory Tip
- Remember: **“Use Secrets Manager for passwords, it's the simplest and safest path!”**"
84,"[""An organization is storing large files in Amazon S3, and is writing a web\napplication to display meta-data about the files to end-users. Based on the\nmetadata a user selects an object to download. The organization needs a\nmechanism to index the files and provide single-digit millisecond latency\nretrieval for the metadata.\n\nWhat AWS service should be used to accomplish this?""]",Amazon DynamoDB,Amazon EC2,AWS Lambda,Amazon RDS,"# Answer
- **Correct option:** A : Amazon DynamoDB
- **Reason:** Amazon DynamoDB is a fully managed NoSQL database service that provides single-digit millisecond response times at any scale. This makes it ideal for storing and retrieving metadata about files quickly, as required by the web application.

# Example / Analogy
- Think of DynamoDB like a high-speed library where you can quickly look up book information (metadata) based on various attributes (like author, genre, etc.) rather than scanning each book (file) physically. Just as you can find a book by searching its title or author in a well-organized library, DynamoDB allows quick indexing and retrieval of metadata.

# Common Mistakes / Traps
- A common mistake is to confuse the need for fast metadata retrieval (DynamoDB) with the necessity to store files or larger data sets. Candidates might mistakenly think they could use Amazon RDS (a relational database) which, while it can store more complex queries, typically involves higher latency compared to DynamoDB.

# Memory Tip
- Remember: **DynamoDB is your go-to for high-speed metadata access – it's like your metadata librarian!**"
85,"[""A developer is creating an AWS Serverless Application Model (AWS SAM) template.\nThe AWS SAM template contains the definition of multiple AWS Lambda functions,\nan Amazon S3 bucket, and an Amazon CloudFront distribution. One of the Lambda\nfunctions runs on Lambda@Edge in the CloudFront distribution. The S3 bucket is\nconfigured as an origin for the CloudFront distribution.\n\nWhen the developer deploys the AWS SAM template in the eu-west-1 Region, the\ncreation of the stack fails.\n\nWhich of the following could be the reason for this issue?""]",CloudFront distributions can be created only in the us-east-1 Region.,Lambda@Edge functions can be created only in the us-east-1 Region.,A single AWS SAM template cannot contain multiple Lambda functions.,The CloudFront distribution and the S3 bucket cannot be created in the same Region.,"# Answer
- **Correct option:** B
- **Reason:** Lambda@Edge functions are a specific type of AWS Lambda function that can only be created and associated with CloudFront distributions in the us-east-1 (N. Virginia) Region. Although the CloudFront distribution itself can be used globally, the function must be created in us-east-1 for it to be associated with the CloudFront distribution before it is deployed globally.

# Example / Analogy
- Think of a movie premiere event (the CloudFront distribution) that can take place in various theaters worldwide. However, the movie (Lambda@Edge function) must be finalized and produced in a specific studio (us-east-1) before it can be shown anywhere else. Essentially, even though the premiere is global, the actual movie has to be made in that one location.

# Common Mistakes / Traps
- A common trap is misunderstanding the relationship between Lambda functions and CloudFront distributions. Developers may think they can deploy Lambda@Edge functions in any region where other elements of the CloudFront setup can be managed, not realizing the specific requirement for us-east-1.

# Memory Tip
- Remember: Lambda@Edge is like a movie that must be made in us-east-1 before it goes global."
86,"[""A developer is integrating Amazon ElastiCache in an application. The cache will\nstore data from a database. The cached data must populate real-time dashboards.\n\nWhich caching strategy will meet these requirements?""]",A read-through cache,A write-behind cache,A lazy-loading cache,A write-through cache,"# Answer
- **Correct option:** A : A read-through cache
- **Reason:** A read-through cache allows the application to automatically fetch data from the database and populate the cache when there is a cache miss (i.e., when the requested data is not found in the cache). This strategy is ideal for real-time applications since it ensures that the most current data is fetched and stored in the cache for quick access, which aligns perfectly with the requirement of populating real-time dashboards.

# Example / Analogy
- Think of a read-through cache like a library with a smart librarian. When you request a book (data), if it's not on the shelf (cache), the librarian goes to find it in the storage room (database) and brings it back for you. In future requests, the book is available on the shelf, allowing for quick access without delays.

# Common Mistakes / Traps
- A common mistake is confusing a read-through cache with a write-through cache. In a write-through cache, updates are made to both the cache and the database immediately, which may not always ensure the real-time aspect needed for dashboards. 

# Memory Tip
- ""Read-through is like a smart librarian fetching fresh books for you."""
87,"[""A developer is creating an AWS Lambda function. The Lambda function needs an\nexternal library to connect to a third-party solution. The external library is a\ncollection of files with a total size of 100 MB. The developer needs to make the\nexternal library available to the Lambda execution environment and reduce the\nLambda package space.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Create a Lambda layer to store the external library. Configure the Lambda function to use the layer.,Create an Amazon S3 bucket. Upload the external library into the S3 bucket. Mount the S3 bucket folder in the Lambda function. Import the library by using the proper folder in the mount point.,Load the external library to the Lambda function's /tmp directory during deployment of the Lambda package. Import the library from the /tmp directory.,Create an Amazon Elastic File System (Amazon EFS) volume. Upload the external library to the EFS volume. Mount the EFS volume in the Lambda function. Import the library by using the proper folder in the mount point.,"# Answer
- **Correct option:** A
- **Reason:** Creating a Lambda layer to store the external library allows you to package the library separately from your Lambda function code. This method helps to keep the Lambda package size within the limits and reduces operational overhead by making the libraries available across multiple functions without needing to include them in every deployment package.

# Example / Analogy
- Think of Lambda layers like a toolbox that you can share among various projects. Instead of duplicating the same set of tools (libraries) in different toolboxes (Lambda functions), you just keep one shared toolbox (the layer) and refer to it whenever you need those tools.

# Common Mistakes / Traps
- A common mistake would be choosing option B (using S3) or D (using EFS) thinking they may seem like effective solutions. However, both involve more complexity and administrative effort compared to simply using Lambda layers, which are designed specifically for this purpose.

# Memory Tip
- **Layers are the shared toolbox for Lambda, reducing clutter and making deployment simpler.**"
88,"[""A company has a front-end application that runs on four Amazon EC2 instances\nbehind an Elastic Load Balancer (ELB) in a production environment that is\nprovisioned by AWS Elastic Beanstalk. A developer needs to deploy and test new\napplication code while updating the Elastic Beanstalk platform from the current\nversion to a newer version of Node.js. The solution must result in zero downtime\nfor the application.\n\nWhich solution meets these requirements?""]","Clone the production environment to a different platform version. Deploy the new application code, and test it. Swap the environment URLs upon verification.",Deploy the new application code in an all-at-once deployment to the existing EC2 instances. Test the code. Redeploy the previous code if verification fails.,Perform an immutable update to deploy the new application code to new EC2 instances. Serve traffic to the new instances after they pass health checks.,Use a rolling deployment for the new application code. Apply the code to a subset of EC2 instances until the tests pass. Redeploy the previous code if the tests fail.,"# Answer
- **Correct option:** A
- **Reason:** Cloning the production environment allows the developer to deploy and test the new application code and platform version without affecting the live application. Once the new environment is verified, swapping the environment URLs ensures zero downtime as users are directed to the fully tested environment.

# Example / Analogy
- Think of it like a restaurant that builds a new kitchen (the cloned environment) while still serving customers from the old one. The restaurant can test new recipes in the new kitchen, and once everything is perfect, they switch over completely, so diners don’t notice any disruption.

# Common Mistakes / Traps
- A common mistake is thinking that rolling or immutable updates are sufficient for zero downtime, which they can be, but they require careful implementation. Additionally, ""all-at-once"" deployments (like option B) can lead to downtime if something goes wrong.

# Memory Tip
- **Tip:** Use clones for safe changes—test first, switch URLs later!"
89,"[""A company has a critical application on AWS. The application exposes an HTTP API\nby using Amazon API Gateway. The API is integrated with an AWS Lambda function.\nThe application stores data in an Amazon RDS for MySQL DB instance with 2\nvirtual CPUs (vCPUs) and 64 GB of RAM.\n\nCustomers have reported that some of the API calls return HTTP 500 Internal\nServer Error responses. Amazon CloudWatch Logs shows errors for “too many\nconnections.” The errors occur during peak usage times that are unpredictable.\n\nThe company needs to make the application resilient. The database cannot be down\noutside of scheduled maintenance hours.\n\nWhich solution will meet these requirements?""]",Decrease the number of vCPUs for the DB instance. Increase the max_connections setting.,Use Amazon RDS Proxy to create a proxy that connects to the DB instance. Update the Lambda function to connect to the proxy.,"Add a CloudWatch alarm that changes the DB instance class when the number of connections increases to more than 1,000.",Add an Amazon EventBridge rule that increases the max_connections setting of the DB instance when CPU utilization is above 75%.,"# Answer
- **Correct option:** B : Use Amazon RDS Proxy to create a proxy that connects to the DB instance. Update the Lambda function to connect to the proxy.

- **Reason:** Amazon RDS Proxy helps manage database connections more efficiently, especially during peak usage times when you're encountering ""too many connections"" errors. It acts as a connection pool that reduces the number of connections made to your RDS instance by reusing connections, which optimizes resource utilization and can improve application resilience.

# Example / Analogy
- Think of RDS Proxy as a traffic police officer at a busy intersection. Just as the officer helps manage the flow of cars, preventing congestion and maintaining smooth traffic, RDS Proxy helps manage database connections, ensuring your application doesn't overwhelm the database with too many requests at once and avoids errors like “too many connections.”

# Common Mistakes / Traps
- A common trap is to assume that simply increasing instance resources (like vCPUs or max_connections) will solve the problem. While this might help to some extent, it does not address the fundamental issue of connection management during peak loads.

# Memory Tip
- Remember: ""RDS Proxy is the traffic controller for my database connections."""
90,"[""A company has installed smart meters in all its customer locations. The smart\nmeters measure power usage at 1-minute intervals and send the usage readings to\na remote endpoint for collection. The company needs to create an endpoint that\nwill receive the smart meter readings and store the readings in a database. The\ncompany wants to store the location ID and timestamp information.\n\nThe company wants to give its customers low-latency access to their current\nusage and historical usage on demand. The company expects demand to increase\nsignificantly. The solution must not impact performance or include downtime\nwhile scaling.\n\nWhich solution will meet these requirements MOST cost-effectively?""]",Store the smart meter readings in an Amazon RDS database. Create an index on the location ID and timestamp columns. Use the columns to filter on the customers' data.,Store the smart meter readings in an Amazon DynamoDB table. Create a composite key by using the location ID and timestamp columns. Use the columns to filter on the customers' data.,Store the smart meter readings in Amazon ElastiCache for Redis. Create a SortedSet key by using the location ID and timestamp columns. Use the columns to filter on the customers' data.,Store the smart meter readings in Amazon S3. Partition the data by using the location ID and timestamp columns. Use Amazon Athena to filter on the customers' data.,"# Answer
- **Correct option:** B
- **Reason:** Amazon DynamoDB is a fully managed NoSQL database that offers low-latency access and scales seamlessly to handle increased demand without downtime. By using a composite key of location ID and timestamp, you can effectively query and filter the data for customers' smart meter readings.

# Example / Analogy
- Think of DynamoDB like a library with well-organized sections (keys) based on genres (location ID) and publication dates (timestamp). This allows you to quickly find the books (data) you want without sifting through an entire shelf, mirroring how DynamoDB allows fast access to specific readings without impacting performance.

# Common Mistakes / Traps
- A common trap is assuming that traditional SQL databases like Amazon RDS (Option A) can scale without performance issues. While RDS can handle queries, it may require more management and can face scaling challenges when demand spikes. Option C (ElastiCache) focuses on caching rather than persistent storage, which doesn't suit the requirement of storing historical usage effectively. Option D (S3 with Athena) can introduce latency when querying the data for low-latency access.

# Memory Tip
- Remember: For real-time performance and scalability with high concurrency, choose DynamoDB as your cloud database solution."
91,"[""A developer is configuring an application's deployment environment in AWS\nCodePipeline. The application code is stored in a GitHub repository. The\ndeveloper wants to ensure that the repository package's unit tests run in the\nnew deployment environment. The developer has already set the pipeline's source\nprovider to GitHub and has specified the repository and branch to use in the\ndeployment.\n\nWhich combination of steps should the developer take next to meet these\nrequirements with the LEAST overhead? (Choose two.)""]",Create an AWS CodeCommit project. Add the repository package's build and test commands to the project's buildspec.,Create an AWS CodeBuild project. Add the repository package's build and test commands to the project's buildspec.,Create an AWS CodeDeploy project. Add the repository package's build and test commands to the project's buildspec.,Add an action to the source stage. Specify the newly created project as the action provider. Specify the build artifact as the action's input artifact.,"# Answer
- **Correct options:** B and D

- **Reason:** 
  To run unit tests as part of a deployment process in AWS CodePipeline, the developer should create an AWS CodeBuild project (option B) that will execute the build commands specified in the buildspec file, which includes the unit tests. Then, the developer must add an action to the source stage of the pipeline (option D), specifying the CodeBuild project as the provider. This configuration allows CodePipeline to trigger the test and build processes as soon as there's a change in the GitHub repository, thus ensuring that tests are run automatically with the least overhead.

# Example / Analogy
- Think of it like a cooking competition. You have the ingredients (your source code in GitHub) and a cooking station (CodeBuild). You set up a recipe card (buildspec) with specific instructions to prepare your dish (build and run tests). Once your ingredients are ready, the competition (CodePipeline) calls you to start cooking (running tests and building), monitoring your progress throughout the stages.

# Common Mistakes / Traps
- A common mistake would be to overlook the necessity of linking the CodeBuild project with the CodePipeline. Some might mistakenly assume that merely creating a CodeDeploy project is sufficient, but CodeDeploy is primarily for deployment rather than handling builds and tests.

# Memory Tip
- Remember: ""Build with CodeBuild, Test with CodePipeline."""
92,"[""An application under development is required to store hundreds of video files.\nThe data must be encrypted within the application prior to storage, with a\nunique key for each video file.\n\nHow should the developer code the application?""]",Use the KMS Encrypt API to encrypt the data. Store the encrypted data key and data.,Use a cryptography library to generate an encryption key for the application. Use the encryption key to encrypt the data. Store the encrypted data.,Use the KMS GenerateDataKey API to get a data key. Encrypt the data with the data key. Store the encrypted data key and data.,Upload the data to an S3 bucket using server side-encryption with an AWS KMS key.,"# Answer
- **Correct option:** C : Use the KMS GenerateDataKey API to get a data key. Encrypt the data with the data key. Store the encrypted data key and data.

- **Reason:** This option correctly addresses the requirement of using a unique encryption key for each video file while leveraging AWS Key Management Service (KMS). The process of using the GenerateDataKey API allows the application to obtain a data encryption key that is encrypted by KMS and can be safely stored alongside the encrypted video data. This method combines security (by using KMS for key management) with the flexibility of managing unique keys for each file.

# Example / Analogy
- Think of KMS as a secure vault where you can store gold keys (data keys). Each time you need gold to lock your treasure (video files), you ask KMS for a new gold key. You use it to lock your treasure and then put the key in the vault, secured by KMS, ensuring that each treasure is uniquely secured.

# Common Mistakes / Traps
- A common mistake is misunderstanding when to use KMS versus application-level encryption. Option A suggests directly using KMS to encrypt data, but it does not mention managing unique keys for each video file. Option D assumes server-side encryption with KMS, which does not meet the requirement of pre-encrypting the data with unique keys before storage. 

# Memory Tip
- ""Use KMS for gold keys, keep them safe and unique for each video!"""
93,"[""A company is planning to deploy an application on AWS behind an Elastic Load\nBalancer. The application uses an HTTP/HTTPS listener and must access the client\nIP addresses.\n\nWhich load-balancing solution meets these requirements?""]",Use an Application Load Balancer and the X-Forwarded-For headers.,Use a Network Load Balancer (NLB). Enable proxy protocol support on the NLB and the target application.,Use an Application Load Balancer. Register the targets by the instance ID.,Use a Network Load Balancer and the X-Forwarded-For headers.,"# Answer
- **Correct option:** A : Use an Application Load Balancer and the X-Forwarded-For headers.
- **Reason:** The Application Load Balancer (ALB) supports both HTTP and HTTPS listeners and can include the original client IP address in the requests forwarded to the backend instances through the `X-Forwarded-For` header. This allows the application to identify the true client IP, which is essential for certain functionalities such as logging and security.

# Example / Analogy
- Think of a restaurant where customers submit their orders. If an order ticket only has the name of the waiter who took the order, the kitchen staff may not know who actually placed the order. The `X-Forwarded-For` header acts like a note that includes the actual customer’s name, allowing the kitchen to know who made the request.

# Common Mistakes / Traps
- A common trap is to assume that using a Network Load Balancer or relying solely on instance IP addresses will allow access to client IPs. NLBs don’t natively forward client IPs unless you enable proxy protocol, which can add unnecessary complexity. 

# Memory Tip
- Remember: **ALB + X-Forwarded-For = Client IP Awareness!**"
94,"[""A developer is building a highly secure healthcare application using serverless\ncomponents. This application requires writing temporary data to /tmp storage on\nan AWS Lambda function.\n\nHow should the developer encrypt this data?""]",Enable Amazon EBS volume encryption with an AWS KMS key in the Lambda function configuration so that all storage attached to the Lambda function is encrypted.,Set up the Lambda function with a role and key policy to access an AWS KMS key. Use the key to generate a data key used to encrypt all data prior to writing to /tmp storage.,Use OpenSSL to generate a symmetric encryption key on Lambda startup. Use this key to encrypt the data prior to writing to /tmp.,"Use an on-premises hardware security module (HSM) to generate keys, where the Lambda function requests a data key from the HSM and uses that to encrypt data on all requests to the function.","# Answer
- **Correct option:** B
- **Reason:** Option B describes the proper approach to encrypting temporary data in an AWS Lambda function. By setting up the Lambda function with a role and key policy to access an AWS KMS key, the developer can safely generate a data key that is used to encrypt the data before writing it to the /tmp storage. This ensures that sensitive healthcare data is protected while in transit and at rest.

# Example / Analogy
- Think of your data as a valuable package that you want to store temporarily. Instead of leaving it unguarded, you use a special key (the KMS key) to lock it in a secure container (the encryption method). When you need the package, you unlock it with your key, ensuring that only authorized parties can access it, similar to securing important documents in a safe.

# Common Mistakes / Traps
- A common mistake may be opting for option A, which mentions Amazon EBS volume encryption. However, Lambda functions do not have attached EBS volumes, meaning this option is irrelevant. Furthermore, relying solely on OpenSSL in option C might introduce risks associated with key management and does not leverage AWS's built-in security features effectively.

# Memory Tip
- Use AWS KMS for secure key management to encrypt sensitive data in Lambda!"
95,"[""A developer deployed an application to an Amazon EC2 instance. The application\nneeds to know the public IPv4 address of the instance.\n\nHow can the application find this information?""]",Query the instance metadata from http://169.254.169.254/latest/meta-data/.,Query the instance user data from http://169.254.169.254/latest/user-data/.,Query the Amazon Machine Image (AMI) information from http://169.254.169.254/latest/meta-data/ami/.,Check the hosts file of the operating system.,"# Answer
- **Correct option:** A : Query the instance metadata from http://169.254.169.254/latest/meta-data/.

- **Reason:** The instance metadata is a special feature provided by AWS that allows an EC2 instance to access information about itself, including its public IPv4 address. This information can be retrieved by making a simple HTTP request to the instance metadata service at a predefined link, which for IPv4 addressed information is specifically located at `http://169.254.169.254/latest/meta-data/`. 

# Example / Analogy
- Imagine your EC2 instance is like a person at a conference. They have a name tag (instance metadata) which includes their contact information. To find out their phone number (public IPv4 address), you simply need to ask them directly in a specific way, rather than looking at a directory (like the hosts file) or asking about other irrelevant information (like AMI or user data), which wouldn’t help you get the phone number you actually need.

# Common Mistakes / Traps
- A common mistake is to confuse instance metadata with user data or AMI information. User data is used to pass initialization scripts and configurations to the instance at launch but does not contain dynamic metadata such as the public IPv4 address. Additionally, some might incorrectly assume to search through system files (like the hosts file), which would not provide this specific information.

# Memory Tip
- Remember: **""Instance metadata is the key to knowing your EC2 instance's address!""**"
96,"[""A developer wants to debug an application by searching and filtering log data.\nThe application logs are stored in Amazon CloudWatch Logs. The developer creates\na new metric filter to count exceptions in the application logs. However, no\nresults are returned from the logs.\n\nWhat is the reason that no filtered results are being returned?""]",A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC.,CloudWatch Logs only publishes metric data for events that happen after the filter is created.,The log group for CloudWatch Logs should be first streamed to Amazon OpenSearch Service before metric filtering returns the results.,Metric data points for logs groups can be filtered only after they are exported to an Amazon S3 bucket.,"# Answer
- **Correct option:** B
- **Reason:** CloudWatch Logs only publishes metric data for events that happen after the filter is created. This means that if the metric filter was set up after the relevant log data was generated (such as the exceptions), then it won't count those prior events because they were logged before the filter was in place.

# Example / Analogy
- Think of it like setting up a new subscription to a newspaper. If you subscribe today, you'll only receive the articles published from today onward. Any articles that were published before your subscription won't be delivered to you. Similarly, the metric filter only tracks logs generated after it has been created.

# Common Mistakes / Traps
- A common mistake is assuming that metric filters can retroactively analyze logs that have already been written. It's important to remember that filters only operate on new events recorded after their creation.

# Memory Tip
- Remember: ""Filters catch only new data, like a net only catching fish that swim by after it’s cast."""
97,"[""A company is planning to use AWS CodeDeploy to deploy an application to Amazon\nElastic Container Service (Amazon ECS). During the deployment of a new version\nof the application, the company initially must expose only 10% of live traffic\nto the new version of the deployed application. Then, after 15 minutes elapse,\nthe company must route all the remaining live traffic to the new version of the\ndeployed application.\n\nWhich CodeDeploy predefined configuration will meet these requirements?""]",CodeDeployDefault.ECSCanary10Percent15Minutes,CodeDeployDefault.LambdaCanary10Percent5Minutes,CodeDeployDefault.LambdaCanary10Percentl15Minutes,CodeDeployDefault.ECSLinear10PercentEvery1Minutes,"# Answer
- **Correct option:** A : CodeDeployDefault.ECSCanary10Percent15Minutes
- **Reason:** This predefined configuration in AWS CodeDeploy is specifically designed for Canary deployments. It allows you to shift only 10% of the traffic to the new version for a duration of 15 minutes before promoting the remaining traffic to the new version, which aligns perfectly with the company's requirements to gradually expose the new version.

# Example / Analogy
- Think of it like a restaurant testing a new dish. The restaurant serves the new dish to 10% of its customers for 15 minutes to see if they like it. If the feedback is positive, they then decide to serve the dish to all customers. This gradual testing ensures that they mitigate risks before a full rollout.

# Common Mistakes / Traps
- A common mistake would be confusing the configurations for Lambda and ECS. Remember that the ECS deployment types will have ""ECS"" in their names, while Lambda deployment types will have ""Lambda"" in them. Always look for the specific service you are deploying to in the options.

# Memory Tip
- **Remember: Canary is gradual, ECS specifies service!**"
98,"[""A developer is creating a Ruby application and needs to automate the deployment,\nscaling, and management of an environment without requiring knowledge of the\nunderlying infrastructure.\n\nWhich service would best accomplish this task?""]",AWS CodeDeploy,AWS CloudFormation,AWS OpsWorks,AWS Elastic Beanstalk,"# Answer
- **Correct option:** D : AWS Elastic Beanstalk  
- **Reason:** AWS Elastic Beanstalk is a Platform as a Service (PaaS) that allows developers to deploy applications without worrying about the underlying infrastructure. It automates the deployment, scaling, and management of applications, providing a straightforward way for Ruby developers (and others) to focus on writing code rather than managing servers.

# Example / Analogy
- Consider Elastic Beanstalk like a restaurant that prepares and serves meals for you without you needing to learn how to cook or manage the kitchen. You simply provide the recipe (your application code), and the restaurant handles everything else (servers, scaling, and management).

# Common Mistakes / Traps
- A common mistake is confusing AWS Elastic Beanstalk with AWS CloudFormation or AWS OpsWorks. CloudFormation focuses on infrastructure as code (IAC), while OpsWorks is based on Chef/Puppet for configuration management, neither of which automates deployment as seamlessly as Elastic Beanstalk does for application management.

# Memory Tip
- Remember: ""Elastic Beanstalk = Easy deployment with no infrastructure worries!"""
99,"[""A company has a web application that is deployed on AWS. The application uses an\nAmazon API Gateway API and an AWS Lambda function as its backend.\n\nThe application recently demonstrated unexpected behavior. A developer examines\nthe Lambda function code, finds an error, and modifies the code to resolve the\nproblem. Before deploying the change to production, the developer needs to run\ntests to validate that the application operates properly.\n\nThe application has only a production environment available. The developer must\ncreate a new development environment to test the code changes. The developer\nmust also prevent other developers from overwriting these changes during the\ntest cycle.\n\nWhich combination of steps will meet these requirements with the LEAST\ndevelopment effort? (Choose two.)""]",Create a new resource in the current stage. Create a new method with Lambda proxy integration. Select the Lambda function. Add the hotfix alias. Redeploy the current stage. Test the backend.,Update the Lambda function in the API Gateway API integration request to use the hotfix alias. Deploy the API Gateway API to a new stage named hotfix. Test the backend.,Modify the Lambda function by fixing the code. Test the Lambda function. Create the alias hotfix. Point the alias to the $LATEST version.,"Modify the Lambda function by fixing the code. Test the Lambda function. When the Lambda function is working as expected, publish the Lambda function as a new version. Create the alias hotfix. Point the alias to the new version.","# Answer
- **Correct options:** B and D
- **Reason:** 
  - Option **B** involves updating the Lambda function integration in the API Gateway to point to a new stage, which allows the developer to test the changes without affecting the production environment. This approach ensures that other developers cannot overwrite the hotfix changes because it operates in an isolated stage. 
  - Option **D** allows the developer to test the changes and ensure they're working as expected before publishing a new version of the Lambda function and creating an alias named ""hotfix."" This allows ongoing development and testing while maintaining stability in the production environment. The alias points to a specific version, preventing changes from disrupting this version. 

# Example / Analogy
- Imagine a restaurant that wants to add a new dish to its menu (the new code). Instead of immediately replacing a popular dish (the production environment), the restaurant tries the new dish in a special weekend trial (the separate stage in API Gateway) while keeping the old dish on the menu. Once they know the new dish is fantastic (successful testing), they officially add it to the menu (publish the new version).

# Common Mistakes / Traps
- One common mistake is rushing to modify the production environment directly, which can lead to unexpected downtime or issues. Also, utilizing aliases without appropriately versioning can create confusion over what version is in use.

# Memory Tip
- Always test changes in a dedicated environment and use aliases for version control in AWS Lambda!"
100,"[""A company's new mobile app uses Amazon API Gateway. As the development team\ncompletes a new release of its APIs, a developer must safely and transparently\nroll out the API change.\n\nWhat is the SIMPLEST solution for the developer to use for rolling out the new\nAPI version to a limited number of users through API Gateway?""]",Create a new API in API Gateway. Direct a portion of the traffic to the new API using an Amazon Route 53 weighted routing policy.,Validate the new API version and promote it to production during the window of lowest expected utilization.,Implement an Amazon CloudWatch alarm to trigger a rollback if the observed HTTP 500 status code rate exceeds a predetermined threshold.,Use the canary release deployment option in API Gateway. Direct a percentage of the API traffic using the canarySettings setting.,"# Answer
- **Correct option:** D
- **Reason:** Using the canary release deployment option in API Gateway allows the developer to route a percentage of traffic to the new API version while keeping the majority on the stable version. This approach provides a safe way to test the new version with real users, enabling the team to monitor performance and issues without fully committing to the new release.

# Example / Analogy
- Think of it like testing a new flavor of ice cream in a small bowl rather than switching all customers to the new flavor immediately. If customers love it, you can roll it out to everyone; if they don't, you still have your original flavor available.

# Common Mistakes / Traps
- A common mistake is to choose options that involve creating a completely new API (Choice A) or merely rolling out updates without testing them (Choice B). Canary releases specifically require managing traffic, which can’t be achieved with those strategies.

# Memory Tip
- **Canary releases let you ""test the waters"" before a full rollout.**"
101,"[""A company caches session information for a web application in an Amazon DynamoDB\ntable. The company wants an automated way to delete old items from the table.\n\nWhat is the simplest way to do this?""]",Write a script that deletes old records; schedule the script as a cron job on an Amazon EC2 instance.,Add an attribute with the expiration time; enable the Time To Live feature based on that attribute.,"Each day, create a new table to hold session data; delete the previous day's table.",Add an attribute with the expiration time; name the attribute ItemExpiration.,"# Answer
- **Correct option:** B : Add an attribute with the expiration time; enable the Time To Live feature based on that attribute.

- **Reason:** The Time To Live (TTL) feature in DynamoDB allows you to specify an expiration time for each item in your table. When the specified time is reached, DynamoDB automatically deletes the item. This eliminates the need for manual scripts or external processes to manage old records, simplifying your architecture and reducing costs.

# Example / Analogy
- Imagine the TTL feature as a ""best before"" date on packaged food. Once the date passes, you can safely dispose of the item without risk, just as DynamoDB automatically clears expired items, keeping your data fresh without manual intervention.

# Common Mistakes / Traps
- A common trap is thinking that a script-based solution is necessary for cleanup, overlooking the built-in TTL functionality that simplifies management. Some may also get confused by suggesting naming conventions for attributes (like ItemExpiration) instead of focusing on enabling TTL itself.

# Memory Tip
- **Think of TTL as a ""self-cleaning"" mechanism for your DynamoDB items.**"
102,"[""An e-commerce web application that shares session state on-premises is being\nmigrated to AWS. The application must be fault tolerant, natively highly\nscalable, and any service interruption should not affect the user experience.\n\nWhat is the best option to store the session state?""]",Store the session state in Amazon ElastiCache.,Store the session state in Amazon CloudFront.,Store the session state in Amazon S3.,Enable session stickiness using elastic load balancers.,"# Answer
- **Correct option:** A: Store the session state in Amazon ElastiCache.
- **Reason:** Amazon ElastiCache is designed for in-memory data storage and can provide fast access to session data, which is crucial for maintaining a smooth user experience. It supports both Redis and Memcached, enabling fault tolerance and high scalability. This approach ensures that even if one instance goes down, another can pick up the session state without any service interruption, which aligns with the application's requirements.

# Example / Analogy
- Imagine a bus station where multiple buses (application servers) need access to the same schedule (session state). If the schedule is kept in a central location (like ElastiCache), any bus can access or update it quickly, keeping all buses synchronized and running on time, even if one runs late or has to be replaced.

# Common Mistakes / Traps
- A common trap is to confuse session stickiness (option D) with session state storage. While stickiness helps route a user to the same server, it doesn't store state information. Additionally, options B (CloudFront) and C (S3) are not suitable for session state storage as they are more suited for caching and file storage respectively.

# Memory Tip
- Think of ElastiCache as the ""fast memory"" for your app, keeping your session data quick to access and fail-proof."
103,"[""A developer is leveraging a Border Gateway Protocol (BGP)-based AWS VPN\nconnection to connect from on-premises to Amazon EC2 instances in the\ndeveloper's account. The developer is able to access an EC2 instance in subnet\nA, but is unable to access an EC2 instance in subnet B in the same VPC.\n\nWhich logs can the developer use to verify whether the traffic is reaching\nsubnet B?""]",VPN logs,BGP logs,VPC Flow Logs,AWS CloudTrail logs,"# Answer
- **Correct option:** C : VPC Flow Logs
- **Reason:** VPC Flow Logs allow the developer to capture information about the IP traffic going to and from the network interfaces in the VPC. By analyzing these logs, the developer can determine if traffic is reaching subnet B from the on-premises network, helping diagnose any potential issues with connectivity.

# Example / Analogy
- Think of VPC Flow Logs like a traffic camera at a busy intersection. Just as the camera records all the cars passing through, VPC Flow Logs track all the data packets moving in and out of your VPC subnets, helping you understand where traffic is flowing and where blockages may occur.

# Common Mistakes / Traps
- A common mistake is to assume that VPN logs or BGP logs would directly indicate the status of traffic reaching specific subnets, but they primarily focus on the status and health of the VPN connection itself rather than on the traffic flow within the VPC.

# Memory Tip
- **Remember:** VPC Flow Logs = Traffic Reports for your VPC zones."
104,"[""A developer has created an AWS Lambda function to provide notification through\nAmazon Simple Notification Service (Amazon SNS) whenever a file is uploaded to\nAmazon S3 that is larger than 50 MB. The developer has deployed and tested the\nLambda function by using the CLI. However, when the event notification is added\nto the S3 bucket and a 3,000 MB file is uploaded, the Lambda function does not\nlaunch.\n\nWhich of the following is a possible reason for the Lambda function's inability\nto launch?""]","The S3 event notification does not activate for files that are larger than 1,000 MB.",The resource-based policy for the Lambda function does not have the required permissions to be invoked by Amazon S3.,Lambda functions cannot be invoked directly from an S3 event.,The S3 bucket needs to be made public.,"# Answer
- **Correct option:** B
- **Reason:** The resource-based policy for the Lambda function does not have the required permissions to be invoked by Amazon S3. When setting up event notifications in S3, the Lambda function must include permissions to allow S3 to invoke it, which is managed through a resource-based policy.

# Example / Analogy
- Think of it like having a special door (the Lambda function) that only opens if you have the right key (permissions). If the door doesn’t recognize the key presented by S3 to open it, the door stays shut. In this case, if the Lambda function's policy isn't set up to allow S3 to invoke it, even if the event occurs (the file upload), the door won't open.

# Common Mistakes / Traps
- A common mistake is to assume that simply uploading a file will automatically trigger the Lambda, without considering permissions. Developers might overlook the need to explicitly allow S3 to invoke the Lambda function.

# Memory Tip
- Always check function permissions when integrating AWS services—it's like making sure your keys fit the locks!"
105,"[""A company moved some of its secure files to a private Amazon S3 bucket that has\nno public access. The company wants to develop a serverless application that\ngives its employees the ability to log in and securely share the files with\nother users.\n\nWhich AWS feature should the company use to share and access the files securely?""]",Amazon Cognito user pool,S3 presigned URLs,S3 bucket policy,Amazon Cognito identity pool,"# Answer
- **Correct option:** B: S3 presigned URLs
- **Reason:** S3 presigned URLs allow users to generate temporary URLs that grant time-limited access to private S3 objects. This feature is ideal for securely sharing files stored in a private S3 bucket, as it does not require changing the bucket's permissions or making it public.

# Example / Analogy
- Think of S3 presigned URLs like giving a friend a temporary key to your house. You can specify how long they can use that key before it stops working, allowing them to access a specific room (the file) without granting them permanent access to your entire home (the bucket).

# Common Mistakes / Traps
- A common mistake might be choosing options like Amazon Cognito user pool or identity pool, which are more focused on user authentication and identity management rather than specifically sharing secure files. Remember that user management and file access are different tasks.

# Memory Tip
- Remember: ""Presigned URLs = Temporary keys for secure file access!"""
106,"[""A company needs to develop a proof of concept for a web service application. The\napplication will show the weather forecast for one of the company's office\nlocations. The application will provide a REST endpoint that clients can call.\nWhere possible, the application should use caching features provided by AWS to\nlimit the number of requests to the backend service. The application backend\nwill receive a small amount of traffic only during testing.\n\nWhich approach should the developer take to provide the REST endpoint MOST\ncost-effectively?""]",Create a container image. Deploy the container image by using Amazon Elastic Kubernetes Service (Amazon EKS). Expose the functionality by using Amazon API Gateway.,Create an AWS Lambda function by using the AWS Serverless Application Model (AWS SAM). Expose the Lambda functionality by using Amazon API Gateway.,Create a container image. Deploy the container image by using Amazon Elastic Container Service (Amazon ECS). Expose the functionality by using Amazon API Gateway.,Create a microservices application. Deploy the application to AWS Elastic Beanstalk. Expose the AWS Lambda functionality by using an Application Load Balancer.,"# Answer
- **Correct option:** B
- **Reason:** Creating an AWS Lambda function with the AWS Serverless Application Model (AWS SAM) and exposing it via Amazon API Gateway is the most cost-effective approach for a low-traffic proof of concept. It leverages serverless architecture, which means you only pay for actual usage without provisioning dedicated servers. Lambda automatically scales to handle incoming requests while caching capabilities in API Gateway can further reduce backend requests efficiently.

# Example / Analogy
- Think of AWS Lambda like a food truck that only serves meals when customers request them. When the truck is closed, you don't incur any costs, and when it's open, you only pay for the meals made. API Gateway is like the ordering system that handles customer requests and manages the flow to the truck efficiently, ensuring minimal wait times and optimal service.

# Common Mistakes / Traps
- Many might assume that deploying containers is a better choice for a proof of concept due to familiarity or perceived performance benefits. However, deploying containers incurs higher operational overhead and costs in this context, especially for small, intermittent traffic, making serverless options more appealing.

# Memory Tip
- Remember: ""For low traffic and cost savings, go serverless with Lambda!"""
107,"[""A company has an AWS Lambda function that processes incoming requests from an\nAmazon API Gateway API. The API calls the Lambda function by using a Lambda\nalias. A developer updated the Lambda function code to handle more details\nrelated to the incoming requests. The developer wants to deploy the new Lambda\nfunction for more testing by other developers with no impact to customers that\nuse the API.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Create a new version of the Lambda function. Create a new stage on API Gateway with integration to the new Lambda version. Use the new API Gateway stage to test the Lambda function.,Update the existing Lambda alias used by API Gateway to a weighted alias. Add the new Lambda version as an additional Lambda function with a weight of 10%. Use the existing API Gateway stage for testing.,"Create a new version of the Lambda function. Create and deploy a second Lambda function to filter incoming requests from API Gateway. If the filtering Lambda function detects a test request, the filtering Lambda function will invoke the new Lambda version of the code. For other requests, the filtering Lambda function will invoke the old Lambda version. Update the API Gateway API to use the filtering Lambda function.",Create a new version of the Lambda function. Create a new API Gateway API for testing purposes. Update the integration of the new API with the new Lambda version. Use the new API for testing.,"# Answer
- **Correct option:** A

- **Reason:** Option A entails creating a new version of the Lambda function and deploying it through a new stage in API Gateway. This approach allows for isolated testing of the new function without affecting the existing production setup since the existing API Gateway stage remains unchanged. It provides a clear segregation between production and testing environments with minimal operational overhead. The use of stages in API Gateway makes it straightforward to manage different versions of the API, which is essential for ensuring no impact on current customers.

# Example / Analogy
- Imagine a restaurant that wants to test a new dish but doesn't want to affect customers who are enjoying the existing menu. Instead of modifying the current menu, the restaurant creates a special ""test menu"" that only selected customers can order from. This way, the restaurant can gather feedback without disrupting the regular dining experience.

# Common Mistakes / Traps
- A common mistake is to think that updating the existing Lambda alias or using a weighted alias (as in option B) is the best strategy. While these methods can create a transition between versions, they might inadvertently let some customers access the new Lambda code prematurely, which is not desirable in a testing scenario.

# Memory Tip
- Remember, isolating environments for testing ensures a stable production experience—like having a test menu for new dishes!"
108,"[""A developer has written an application that runs on Amazon EC2 instances. The\ndeveloper is adding functionality for the application to write objects to an\nAmazon S3 bucket.\n\nWhich policy must the developer modify to allow the instances to write these\nobjects?""]",The IAM policy that is attached to the EC2 instance profile role,The session policy that is applied to the EC2 instance role session,The AWS Key Management Service (AWS KMS) key policy that is attached to the EC2 instance profile role,The Amazon VPC endpoint policy,"# Answer
- **Correct option:** A: The IAM policy that is attached to the EC2 instance profile role
- **Reason:** In AWS, when an EC2 instance needs to perform actions like writing to an S3 bucket, it relies on an IAM role that is associated with the instance. This role must have the appropriate permissions. The IAM policy attached to the EC2 instance profile role is what defines the permissions for actions taken by the instance, such as writing objects to S3.

# Example / Analogy
- Consider the EC2 instance as a person who needs permission to enter a building (S3 bucket) and leave items (write objects). The IAM policy is like an ID badge that grants access to certain areas and tasks. If the ID badge does not allow entry or defines what tasks can be performed inside, the person (EC2 instance) will be restricted from completing the desired activities (writing to S3).

# Common Mistakes / Traps
- A common mistake is to assume that policies related to KMS key management, VPC endpoints, or session benefits apply to granting permissions to S3 resources, when in fact they manage different aspects of the AWS environment and do not directly impact permissions for S3 object operations.

# Memory Tip
- **Remember:** ""IAM Role = Access Badge; Ensure it grants the right permissions!"""
109,"[""A company uses AWS Lambda functions and an Amazon S3 trigger to process images\ninto an S3 bucket. A development team set up multiple environments in a single\nAWS account.\n\nAfter a recent production deployment, the development team observed that the\ndevelopment S3 buckets invoked the production environment Lambda functions.\nThese invocations caused unwanted execution of development S3 files by using\nproduction Lambda functions. The development team must prevent these\ninvocations. The team must follow security best practices.\n\nWhich solution will meet these requirements?""]",Update the Lambda execution role for the production Lambda function to add a policy that allows the execution role to read from only the production environment S3 bucket.,Move the development and production environments into separate AWS accounts. Add a resource policy to each Lambda function to allow only S3 buckets that are within the same account to invoke the function.,Add a resource policy to the production Lambda function to allow only the production environment S3 bucket to invoke the function.,Move the development and production environments into separate AWS accounts. Update the Lambda execution role for each function to add a policy that allows the execution role to read from the S3 bucket that is within the same account.,"# Answer
- **Correct option:** C
- **Reason:** Adding a resource policy to the production Lambda function limits its invocation to only the production environment S3 bucket. This effectively prevents development S3 buckets from triggering production Lambda functions, maintaining a clear separation between environments and adhering to security best practices.

# Example / Analogy
- Think of a restaurant with separate dining areas for different groups (like families and business meetings). If you want to ensure that families don’t accidentally enter the business area, you can install a door that only allows certain groups to enter specific spaces. Similarly, a resource policy on the Lambda function acts as that restrictive door, ensuring that only the right S3 bucket can invoke the associated Lambda function.

# Common Mistakes / Traps
- A common mistake is to think that changing the Lambda execution role alone (as in option A) will solve the invocation issue. In reality, the trigger permission settings (resource policies) on the Lambda function itself are what control which S3 buckets can invoke it.

# Memory Tip
- ""Use resource policies to lock down Lambda invocations to the correct S3 buckets."""
110,"[""A developer wants to use AWS Elastic Beanstalk to test a new version of an\napplication in a test environment.\n\nWhich deployment method offers the FASTEST deployment?""]",Immutable,Rolling,Rolling with additional batch,All at once,"# Answer
- **Correct option:** D : All at once
- **Reason:** The ""All at once"" deployment method in AWS Elastic Beanstalk updates all instances of the application at the same time, providing the fastest deployment option. While this method can lead to downtime, it is the quickest way to roll out a new version when speed is the top priority.

# Example / Analogy
- Think of it like flipping a switch to turn off all the lights in a room at once versus turning them off one by one. If your goal is to darken the room as quickly as possible, flipping the switch gives immediate results.

# Common Mistakes / Traps
- A common mistake is to confuse ""All at once"" with methods that provide gradual updates, such as ""Rolling"" or ""Immutable,"" which are safer but not as fast. Remember that, while faster, ""All at once"" can also increase the risk of making the application unavailable.

# Memory Tip
- For the fastest deployment, remember: ""All at once goes fastest!"""
111,"[""A developer is creating an AWS Lambda function in VPC mode. An Amazon S3 event\nwill invoke the Lambda function when an object is uploaded into an S3 bucket.\nThe Lambda function will process the object and produce some analytic results\nthat will be recorded into a file. Each processed object will also generate a\nlog entry that will be recorded into a file.\n\nOther Lambda functions, AWS services, and on-premises resources must have access\nto the result files and log file. Each log entry must also be appended to the\nsame shared log file. The developer needs a solution that can share files and\nappend results into an existing file.\n\nWhich solution should the developer use to meet these requirements?""]",Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in Lambda. Store the result files and log file in the mount point. Append the log entries to the log file.,"Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach enabled volume. Attach the EBS volume to all Lambda functions. Update the Lambda function code to download the log file, append the log entries, and upload the modified log file to Amazon EBS.",Create a reference to the /tmp local directory. Store the result files and log file by using the directory reference. Append the log entry to the log file.,Create a reference to the /opt storage directory. Store the result files and log file by using the directory reference. Append the log entry to the log file.,"# Answer
- **Correct option:** A
- **Reason:** Creating an Amazon Elastic File System (EFS) file system allows multiple AWS Lambda functions, as well as other AWS services and on-premises resources, to easily share files. EFS is a fully-managed NFS (Network File System) that can be mounted in AWS Lambda. This enables the Lambda function to store result files and log files in EFS, and append new log entries to the existing log file. The ability to share the same file system across different resources makes it ideal for this use case.

# Example / Analogy
- Think of Amazon EFS like a shared drive in a corporate office where multiple team members (Lambda functions, services, etc.) can access and edit documents (result and log files) concurrently, adding notes and updates as needed. Just as everyone can see and modify the same file in a shared drive, various AWS resources can seamlessly read from and write to EFS.

# Common Mistakes / Traps
- A common mistake is to choose Amazon EBS, thinking it can be attached to multiple Lambda functions. However, EBS volumes cannot be shared across multiple instances simultaneously unless they use a specific arrangement like Multi-Attach with additional limitations, making it unsuitable for this scenario. Also, referencing local directories like `/tmp` only provides temporary storage (limited to 512 MB) per invocation, and nothing persists beyond the execution lifetime.

# Memory Tip
- **EFS = Easy File Sharing**: Remember that Amazon EFS is designed for shared storage across multiple resources."
112,"[""A company is using an Amazon API Gateway REST API endpoint as a webhook to\npublish events from an on-premises source control management (SCM) system to\nAmazon EventBridge. The company has configured an EventBridge rule to listen for\nthe events and to control application deployment in a central AWS account. The\ncompany needs to receive the same events across multiple receiver AWS accounts.\n\nHow can a developer meet these requirements without changing the configuration\nof the SCM system?""]",Deploy the API Gateway REST API to all the required AWS accounts. Use the same custom domain name for all the gateway endpoints so that a single SCM webhook can be used for all events from all accounts.,Deploy the API Gateway REST API to all the receiver AWS accounts. Create as many SCM webhooks as the number of AWS accounts.,Grant permission to the central AWS account for EventBridge to access the receiver AWS accounts. Add an EventBridge event bus on the receiver AWS accounts as the targets to the existing EventBridge rule.,Convert the API Gateway type from REST API to HTTP API.,"# Answer
- **Correct option:** C
- **Reason:** Granting permission to the central AWS account for EventBridge to access the receiver AWS accounts allows you to route the events from the central EventBridge rule directly to the event buses in the receiver accounts. This method enables all designated accounts to receive the same events without needing to change the existing SCM webhook configuration or requiring separate webhooks for each AWS account.

# Example / Analogy
- Think of it like a central post office (the central AWS account) that can send packages (events) to various homes (receiver AWS accounts) under a shared agreement. The post office has permission to deliver to these homes, so you only need to send one package instead of multiple copies to each house.

# Common Mistakes / Traps
- A common mistake is to consider options that suggest deploying multiple versions of resources (like deploying distinct API Gateway instances or creating multiple webhooks), which would complicate the architecture unnecessarily and increase maintenance overhead.

# Memory Tip
- ""One central post office delivers to many homes."""
113,"[""A developer is working on a web application that uses Amazon DynamoDB as its\ndata store. The application has two DynamoDB tables: one table that is named\nartists and one table that is named songs. The artists table has artistName as\nthe partition key. The songs table has songName as the partition key and\nartistName as the sort key.\n\nThe table usage patterns include the retrieval of multiple songs and artists in\na single database operation from the webpage. The developer needs a way to\nretrieve this information with minimal network traffic and optimal application\nperformance.\n\nWhich solution will meet these requirements?""]",Perform a BatchGetltem operation that returns items from the two tables. Use the list of songName/artistName keys for the songs table and the list of artistName key for the artists table.,Create a local secondary index (LSI) on the songs table that uses artistName as the partition key. Perform a query operation for each artistName on the songs table that filters by the list of songName. Perform a query operation for each artistName on the artists table.,Perform a BatchGetitem operation on the songs table that uses the songName/artistName keys. Perform a BatchGetltem operation on the artists table that uses artistName as the key.,Perform a Scan operation on each table that filters by the list of songName/artistName for the songs table and the list of artistName in the artists table.,"# Answer
- **Correct option:** A
- **Reason:** A BatchGetItem operation allows you to retrieve multiple items from different tables in a single request, reducing network traffic and improving performance by minimizing the number of round trips to the database. You can specify the keys for both tables, thus efficiently obtaining the required information.

# Example / Analogy
- Imagine you are ordering groceries online. Instead of making separate trips to each aisle in a store (which is akin to performing multiple queries or scans), you provide a list of items you need at once (like the BatchGetItem operation) and the store gathers everything for you in one go. This saves time and effort, just like reducing network calls in a web application.

# Common Mistakes / Traps
- A common trap is choosing options that suggest performing separate queries or scans on each table. While these methods will eventually yield results, they are less efficient compared to BatchGetItem, especially in applications that require optimal performance and minimal latency.

# Memory Tip
- **BatchGetItem: Get many, fetch once!**"
114,"[""A company is developing an ecommerce application that uses Amazon API Gateway\nAPIs. The application uses AWS Lambda as a backend. The company needs to test\nthe code in a dedicated, monitored test environment before the company releases\nthe code to the production environment.\n\nWhich solution will meet these requirements?""]",Use a single stage in API Gateway. Create a Lambda function for each environment. Configure API clients to send a query parameter that indicates the environment and the specific Lambda function.,Use multiple stages in API Gateway. Create a single Lambda function for all environments. Add different code blocks for different environments in the Lambda function based on Lambda environment variables.,Use multiple stages in API Gateway. Create a Lambda function for each environment. Configure API Gateway stage variables to route traffic to the Lambda function in different environments.,Use a single stage in API Gateway. Configure API clients to send a query parameter that indicates the environment. Add different code blocks for different environments in the Lambda function to match the value of the query parameter.,"# Answer
- **Correct option:** C
- **Reason:** This option uses multiple stages in API Gateway to differentiate between environments. By creating a separate Lambda function for each environment and deploying them using stage variables in API Gateway, you ensure that each environment can be monitored and tested independently. This method also aligns with AWS best practices, allowing for separate configurations while minimizing the risk of accidentally mixing production and test environments.

# Example / Analogy
- Think of stages in API Gateway like different lanes in a racing track. Each lane can have its own car (Lambda function) that competes (runs) independently based on the rules (stage variables). Just like each car can be tuned specifically for its track, each environment can be configured without affecting the others.

# Common Mistakes / Traps
- A common mistake is assuming that a single Lambda function can efficiently handle different environments by just using query parameters or environment variables. This approach can lead to complex and error-prone code, making it harder to manage deploys and tests effectively.

# Memory Tip
- ""Separate lanes avoid collisions"": Use different stages and functions for clear separation of environments."
115,"[""A developer is creating an AWS Lambda function. The Lambda function will consume\nmessages from an Amazon Simple Queue Service (Amazon SQS) queue. The developer\nwants to integrate unit testing as part of the function's continuous integration\nand continuous delivery (CI/CD) process.\n\nHow can the developer unit test the function?""]",Create an AWS CloudFormation template that creates an SQS queue and deploys the Lambda function. Create a stack from the template during the CI/CD process. Invoke the deployed function. Verify the output.,Create an SQS event for tests. Use a test that consumes messages from the SQS queue during the function's Cl/CD process.,Create an SQS queue for tests. Use this SQS queue in the application's unit test. Run the unit tests during the CI/CD process.,Use the aws lambda invoke command with a test event during the CIICD process.,"# Answer
- **Correct option:** D
- **Reason:** Using the `aws lambda invoke` command with a test event allows the developer to run unit tests on the Lambda function directly in the CI/CD process without needing to set up external resources like an SQS queue. This approach facilitates focused testing of the function's logic using predefined test events that simulate various scenarios.

# Example / Analogy
- Think of it like using a simulation tool to test a car's performance instead of actually driving it on the road. The test events simulate real inputs, enabling you to validate the car's features (the Lambda function’s logic) in a safe and controlled environment.

# Common Mistakes / Traps
- A common mistake is to assume that integration with external services like SQS is necessary for unit testing. While integration tests may require the SQS queue, unit testing can and should focus on validating the function's logic in isolation.

# Memory Tip
- ""Test your Lambda functions in isolation using the `aws lambda invoke` command."""
116,"[""A company is developing a serverless multi-tier application on AWS. The company\nwill build the serverless logic tier by using Amazon API Gateway and AWS Lambda.\nWhile the company builds the logic tier, a developer who works on the frontend\nof the application must develop integration tests. The tests must cover both\npositive and negative scenarios, depending on success and error HTTP status\ncodes.\n\nWhich solution will meet these requirements with the LEAST effort?""]","Set up a mock integration for API methods in API Gateway. In the integration request from Method Execution, add simple logic to return either a success or error based on HTTP status code. In the integration response, add messages that correspond to the HTTP status codes.","Create two mock integration resources for API methods in API Gateway. In the integration request, return a success HTTP status code for one resource and an error HTTP status code for the other resource. In the integration response, add messages that correspond to the HTTP status codes.","Create Lambda functions to perform tests. Add simple logic to return either success or error, based on the HTTP status codes. Build an API Gateway Lambda integration. Select appropriate Lambda functions that correspond to the HTTP status codes.",Create a Lambda function to perform tests. Add simple logic to return either success or error-based HTTP status codes. Create a mock integration in API Gateway. Select the Lambda function that corresponds to the HTTP status codes.,"# Answer
- **Correct option:** A  
- **Reason:** Option A allows for the simplest and quickest setup by using a mock integration within API Gateway to simulate both success and error HTTP status responses. It provides the developer with greater flexibility and control over the response details without needing to deploy additional resources or implement Lambda functions, making it the least effort solution.

# Example / Analogy
- Think of a restaurant menu that displays various food items along with their prices. Before actually cooking the meals, the restaurant can simulate the experience by providing sample descriptions and prices without having to serve real food. Similarly, using API Gateway mock integrations allows you to simulate responses without needing a fully deployed backend.

# Common Mistakes / Traps
- A common mistake could be overcomplicating the setup by choosing options that involve creating multiple resources (like in option B) or using Lambda functions (options C and D) when a simpler mock can suffice. Developers may think they need a full backend logic for testing when these mock scenarios are adequate.

# Memory Tip
- ""Mock it to test it: Keep it simple with mock integrations in API Gateway."""
117,"[""A developer is setting up a deployment pipeline. The pipeline includes an AWS\nCodeBuild build stage that requires access to a database to run integration\ntests. The developer is using a buildspec.yml file to configure the database\nconnection. Company policy requires automatic rotation of all database\ncredentials.\n\nWhich solution will handle the database credentials MOST securely?""]",Retrieve the credentials from variables that are hardcoded in the buildspec.yml file. Configure an AWS Lambda function to rotate the credentials.,Retrieve the credentials from an environment variable that is linked to a SecureString parameter in AWS Systems Manager Parameter Store. Configure Parameter Store for automatic rotation.,Retrieve the credentials from an environment variable that is linked to an AWS Secrets Manager secret. Configure Secrets Manager for automatic rotation.,Retrieve the credentials from an environment variable that contains the connection string in plaintext. Configure an Amazon EventBridge event to rotate the credentials.,"# Answer
- **Correct option:** C
- **Reason:** AWS Secrets Manager is designed specifically for managing sensitive information like database credentials. It allows for automatic rotation of secrets, which aligns with the company's policy on credential management. Using a secure variable that links to a Secrets Manager secret ensures the credentials are handled securely.

# Example / Analogy
- Think of AWS Secrets Manager as a safe where you store your valuable items (credentials). You have a key (secure variable) that lets you access the safe without exposing its contents. Just like a locksmith who can automatically change the locks on a safe at regular intervals, Secrets Manager can automatically rotate your credentials, ensuring that your valuables are always protected.

# Common Mistakes / Traps
- Mistake: Choosing options that involve hardcoding or using plaintext credentials, which are not secure. Many might think that using environment variables without proper encryption is sufficient, but this poses a security risk.

# Memory Tip
- Remember: ""Secrets are best kept in Secrets Manager!"""
118,"[""A company created four AWS Lambda functions that connect to a relational\ndatabase server that runs on an Amazon RDS instance. A security team requires\nthe company to automatically change the database password every 30 days.\n\nWhich solution will meet these requirements MOST securely?""]",Store the database credentials in the environment variables of the Lambda function. Deploy the Lambda function with the new credentials every 30 days.,Store the database credentials in AWS Secrets Manager. Configure a 30-day rotation schedule for the credentials.,Store the database credentials in AWS Systems Manager Parameter Store secure strings. Configure a 30-day schedule for the secure strings.,Store the database credentials in an Amazon S3 bucket that uses server-side encryption with customer-provided encryption keys (SSE-C). Configure a 30-day key rotation schedule for the customer key.,"# Answer
- **Correct option:** B
- **Reason:** Storing the database credentials in AWS Secrets Manager allows for automated credential rotation, which meets the requirement to change the password every 30 days securely. Secrets Manager handles all aspects of credential management, including encryption, versioning, and automatic rotation without requiring redeployment of the Lambda functions.

# Example / Analogy
- Think of AWS Secrets Manager like a personal safe that automatically changes the combination every month. You trust it to keep your cash (credentials) secure, and you don’t have to remember the new combination every time. Instead, you just turn the dial and access your cash without worrying about theft or needing to change things manually.

# Common Mistakes / Traps
- A common mistake might be to choose option A, which suggests storing credentials in environment variables, thinking it's convenient. However, this approach lacks automated rotation and requires redeployment, which adds risk and complexity. Option C, while valid, doesn't have the same level of built-in rotation and management features as Secrets Manager. Option D is also insecure and impractical since S3 is not designed to manage sensitive credentials effectively.

# Memory Tip
- Remember: **AWS Secrets Manager is your go-to for secure, automated credential management.**"
119,"[""A company is developing a serverless application that consists of various AWS\nLambda functions behind Amazon API Gateway APIs. A developer needs to automate\nthe deployment of Lambda function code. The developer will deploy updated Lambda\nfunctions with AWS CodeDeploy. The deployment must minimize the exposure of\npotential errors to end users. When the application is in production, the\napplication cannot experience downtime outside the specified maintenance window.\n\nWhich deployment configuration will meet these requirements with the LEAST\ndeployment time?""]",Use the AWS CodeDeploy in-place deployment configuration for the Lambda functions. Shift all traffic immediately after deployment.,Use the AWS CodeDeploy linear deployment configuration to shift 10% of the traffic every minute.,Use the AWS CodeDeploy all-at-once deployment configuration to shift all traffic to the updated versions immediately.,Use the AWS CodeDeploy predefined canary deployment configuration to shift 10% of the traffic immediately and shift the remaining traffic after 5 minutes.,"# Answer
- **Correct option:** D : Use the AWS CodeDeploy predefined canary deployment configuration to shift 10% of the traffic immediately and shift the remaining traffic after 5 minutes.

- **Reason:** This deployment configuration allows for an initial 10% of the traffic to be routed to the new version of the Lambda function, enabling the team to monitor the performance and detect any potential issues with the updated version. If any problems arise, they can be addressed without affecting the majority of users. After a brief 5-minute waiting period, if everything looks good, the remaining traffic can be shifted to the new version. This minimizes user exposure to potential errors and avoids downtime, aligning with the requirements stated in the question.

# Example / Analogy
- Think of it like a restaurant trying a new dish. Instead of serving the new dish to all customers at once, they first offer it to a small group (10% of the traffic). If that group enjoys the dish without complaints, the restaurant can confidently serve it to everyone else after checking for any issues over a short period of time. This way, they ensure customer satisfaction while minimizing risk.

# Common Mistakes / Traps
- A common trap is choosing the ""all-at-once"" configuration (Option C) because it allows for the fastest deployment. However, it carries a significantly higher risk of exposing all users to potential problems if the new version has issues. Similarly, the in-place deployment (Option A) also poses risks as it shifts all traffic immediately, which does not align with the requirement to minimize user impact during production.

# Memory Tip
- **Canary deployment means testing with 10% first** to reduce risks while ensuring a smooth roll-out."
120,"[""A developer needs to store configuration variables for an application. The\ndeveloper needs to set an expiration date and time for the configuration. The\ndeveloper wants to receive notifications before the configuration expires.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Create a standard parameter in AWS Systems Manager Parameter Store. Set Expiration and ExpirationNotification policy types.,Create a standard parameter in AWS Systems Manager Parameter Store. Create an AWS Lambda function to expire the configuration and to send Amazon Simple Notification Service (Amazon SNS) notifications.,Create an advanced parameter in AWS Systems Manager Parameter Store. Set Expiration and ExpirationNotification policy types.,Create an advanced parameter in AWS Systems Manager Parameter Store. Create an Amazon EC2 instance with a cron job to expire the configuration and to send notifications.,"# Answer
- **Correct option:** C : Create an advanced parameter in AWS Systems Manager Parameter Store. Set Expiration and ExpirationNotification policy types.

- **Reason:** The advanced parameters in AWS Systems Manager Parameter Store provide built-in support for setting expiration dates and sending notifications before they expire without requiring additional resources or custom code. This solution minimizes operational overhead by utilizing the features available directly within the service.

# Example / Analogy
- Think of it like a time-sensitive coupon that has an automatic reminder feature. Just as the coupon provider informs you before the expiration date approaches, the advanced parameter can automatically notify you, ensuring you don't miss important updates in your application configuration.

# Common Mistakes / Traps
- A common mistake is to underestimate the capabilities of advanced parameters and assume that additional manual setups (like Lambda or EC2 instances) are necessary for expiration and notification, leading to more complex solutions than needed for the task.

# Memory Tip
- Remember: “Advanced equals automated!”"
121,"[""A developer creates an AWS Lambda function that retrieves and groups data from\nseveral public API endpoints. The Lambda function has been updated and\nconfigured to connect to the private subnet of a VPC. An internet gateway is\nattached to the VPC. The VPC uses the default network ACL and security group\nconfigurations.\n\nThe developer finds that the Lambda function can no longer access the public\nAPI. The developer has ensured that the public API is accessible, but the Lambda\nfunction cannot connect to the API\n\nHow should the developer fix the connection issue?""]",Ensure that the network ACL allows outbound traffic to the public internet.,Ensure that the security group allows outbound traffic to the public internet.,Ensure that outbound traffic from the private subnet is routed to a public NAT gateway.,Ensure that outbound traffic from the private subnet is routed to a new internet gateway.,"# Answer
- **Correct option:** C
- **Reason:** A Lambda function in a private subnet cannot directly access the internet. To connect to external public APIs, it needs to route outbound traffic through a NAT Gateway. A NAT Gateway provides the necessary network address translation, allowing instances in a private subnet to initiate outbound traffic to the internet while still preventing unsolicited inbound traffic.

# Example / Analogy
- Think of the private subnet as a hotel (private space) where guests (the Lambda function) can make phone calls (send requests) to the outside world (public APIs). However, the hotel has no direct phone line to the outside. Instead, there is a receptionist (NAT Gateway) who can place calls on behalf of guests, allowing them to communicate without directly being exposed to the outside.

# Common Mistakes / Traps
- A common mistake is to assume that having an internet gateway is sufficient for outbound access from a private subnet. An execution in a VPC will not have internet access unless routed through a NAT Gateway. Security groups and NACLs need to be configured appropriately, but they don’t replace the need for a NAT setup.

# Memory Tip
- ""Private subnets need a NAT to talk to the outside (APIs)!"""
122,"[""A developer is creating an application for a company. The application needs to\nread the file doc.txt that is placed in the root folder of an Amazon S3 bucket\nthat is named DOC-EXAMPLE-BUCKET. The company’s security team requires the\nprinciple of least privilege to be applied to the application’s IAM policy.\n\nWhich IAM policy statement will meet these security requirements?""]",[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image8.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image9.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image10.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image11.png],"# Answer
- **Correct option:** A

- **Reason:** Option A specifies an IAM policy that allows the required action ""s3:GetObject"" on the specific resource (the `doc.txt` file in the `DOC-EXAMPLE-BUCKET` bucket). This adheres to the principle of least privilege by ensuring the application can only access the necessary resource without granting broader permissions than required.

# Example / Analogy
- Think of it like giving a friend the key to a specific room in your house where they can retrieve a book, rather than giving them a key to your entire house. You want to limit their access to just what they need.

# Common Mistakes / Traps
- A common mistake is to choose an option that grants broader permissions, such as allowing full access to the bucket or multiple objects, which violates the principle of least privilege.

# Memory Tip
- Remember: ""Access Only What is Necessary"" to keep your permissions tight and secure."
123,"[""A company has an application that uses AWS CodePipeline to automate its\ncontinuous integration and continuous delivery (CI/CD) workflow. The application\nuses AWS CodeCommit for version control. A developer who was working on one of\nthe tasks did not pull the most recent changes from the main branch. A week\nlater, the developer noticed merge conflicts.\n\nHow can the developer resolve the merge conflicts in the developer's branch with\nthe LEAST development effort?""]",Clone the repository. Create a new branch. Update the branch with the changes.,Create a new branch. Apply the changes from the previous branch.,Use the Commit Visualizer view to compare the commits when a feature was added. Fix the merge conflicts.,Stop the pull from the main branch to the feature branch. Rebase the feature branch from the main branch.,"# Answer
- **Correct option:** D

- **Reason:** The least development effort to resolve merge conflicts is to perform a rebase (which is typically done with `git rebase`). By rebasing the feature branch onto the latest version of the main branch, the developer can incorporate the latest changes from the main branch seamlessly. This approach allows the developer to fix any merge conflicts during the rebase process instead of creating a new branch or manually applying changes.

# Example / Analogy
- Think of it like updating your playlist on your music app. If you downloaded songs a week ago but new singles have been released, instead of starting a completely new playlist from scratch, you simply update your current playlist, resolving any conflicts if two songs share similar titles. This way, you keep your collection up-to-date without losing your previous work.

# Common Mistakes / Traps
- A common mistake is to choose option A, which suggests cloning the repository and creating a new branch. This is unnecessary and adds more steps than needed, as it doesn't leverage Git's powerful rebase function which simplifies conflict resolution.

# Memory Tip
- **Rebase to merge: keep it simple, fix conflicts directly!**"
124,"[""Users are reporting errors in an application. The application consists of\nseveral microservices that are deployed on Amazon Elastic Container Service\n(Amazon ECS) with AWS Fargate.\n\nWhich combination of steps should a developer take to fix the errors? (Choose\ntwo.)""]",Deploy AWS X-Ray as a sidecar container to the microservices. Update the task role policy to allow access to the X-Ray API.,Deploy AWS X-Ray as a daemonset to the Fargate cluster. Update the service role policy to allow access to the X-Ray API.,Instrument the application by using the AWS X-Ray SDK. Update the application to use the PutXrayTrace API call to communicate with the X-Ray API.,Instrument the application by using the AWS X-Ray SDK. Update the application to communicate with the X-Ray daemon.,"# Answer
- **Correct option:** A and C
- **Reason:** Deploying AWS X-Ray as a sidecar container (Option A) allows the microservices to send trace data to X-Ray, facilitating easy monitoring and debugging. Instrumenting the application using the AWS X-Ray SDK (Option C) enables the application to actively send trace information and call the PutXrayTrace API, which is essential for capturing detailed tracing information.

# Example / Analogy
- Think of AWS X-Ray as a GPS system for your applications. Just as a GPS tracks your route and identifies possible delays, X-Ray tracks the requests that flow through your microservices, helping you find bottlenecks and errors.

# Common Mistakes / Traps
- A common mistake is to believe that deploying X-Ray as a daemonset (Option B) applies to Fargate, but Fargate doesn’t support daemonsets as it is a managed compute engine for containers. Option D suggests communicating with a daemon, which isn't applicable in the Fargate context.

# Memory Tip
- **Remember:** To debug effectively, think ""sidecar + SDK"" for AWS X-Ray."
125,"[""A developer wants to add request validation to a production environment Amazon\nAPI Gateway API. The developer needs to test the changes before the API is\ndeployed to the production environment. For the test, the developer will send\ntest requests to the API through a testing tool.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Export the existing API to an OpenAPI file. Create a new API. Import the OpenAPI file. Modify the new API to add request validation. Perform the tests. Modify the existing API to add request validation. Deploy the existing API to production.,Modify the existing API to add request validation. Deploy the updated API to a new API Gateway stage. Perform the tests. Deploy the updated API to the API Gateway production stage.,"Create a new API. Add the necessary resources and methods, including new request validation. Perform the tests. Modify the existing API to add request validation. Deploy the existing API to production",Clone the existing API. Modify the new API to add request validation. Perform the tests. Modify the existing API to add request validation. Deploy the existing API to production.,"# Answer
- **Correct option:** B  
- **Reason:** This option has the least operational overhead because it allows the developer to modify the existing API directly and deploy it to a new stage for testing. This way, no additional APIs need to be created or imported, keeping the process streamlined and efficient. Testing in a separate stage is a best practice in API development, ensuring that changes can be validated without affecting the production environment.

# Example / Analogy
- Think of this like a restaurant's menu. If the chef wants to introduce a new dish, they can add it temporarily as a special (new stage) instead of reprinting the entire menu (new API). This way, customers can try it out without the entire menu being affected while they gather feedback and make adjustments.

# Common Mistakes / Traps
- A common mistake is to think that creating a new API or cloning is a safer route; however, this increases complexity and can lead to more operational overhead. Developers may also overlook the testing stages available in API Gateway, which allow for adequate validation without impacting production.

# Memory Tip
- Use stages like test kitchens to safely evaluate changes before serving to customers (production)."
126,"[""An online food company provides an Amazon API Gateway HTTP API to receive orders\nfor partners. The API is integrated with an AWS Lambda function. The Lambda\nfunction stores the orders in an Amazon DynamoDB table.\n\nThe company expects to onboard additional partners. Some of the partners require\nadditional Lambda functions to receive orders. The company has created an Amazon\nS3 bucket. The company needs to store all orders and updates in the S3 bucket\nfor future analysis.\n\nHow can the developer ensure that all orders and updates are stored to Amazon S3\nwith the LEAST development effort?""]",Create a new Lambda function and a new API Gateway API endpoint. Configure the new Lambda function to write to the S3 bucket. Modify the original Lambda function to post updates to the new API endpoint.,Use Amazon Kinesis Data Streams to create a new data stream. Modify the Lambda function to publish orders to the data stream. Configure the data stream to write to the S3 bucket.,Enable DynamoDB Streams on the DynamoDB table. Create a new Lambda function. Associate the stream’s Amazon Resource Name (ARN) with the Lambda function. Configure the Lambda function to write to the S3 bucket as records appear in the table's stream.,Modify the Lambda function to publish to a new Amazon Simple Notification Service (Amazon SNS) topic as the Lambda function receives orders. Subscribe a new Lambda function to the topic. Configure the new Lambda function to write to the S3 bucket as updates come through the topic.,"# Answer
- **Correct option:** C
- **Reason:** Enabling DynamoDB Streams allows you to automatically capture changes (inserts, updates) to the DynamoDB table in real-time. By creating a Lambda function that is triggered by the DynamoDB stream, you can write records directly to the S3 bucket without modifying the original order processing functionality. This approach minimizes changes to the existing system and handles future updates efficiently.

# Example / Analogy
- Think of DynamoDB Streams as a security camera that watches a busy intersection (the DynamoDB table). When there are any changes (traffic), the camera records those events. By attaching a responsive team (the Lambda function) that immediately takes notes (writes to S3), you ensure that all traffic (orders) is documented without needing to overhaul the entire traffic control system (existing Lambda function and API).

# Common Mistakes / Traps
- A common mistake is to assume that modifying the existing Lambda function would be easier. However, this could introduce more complexity and potential bugs, as it requires changing the existing flow of order processing. Also, assuming that direct API integrations (as in A) would provide a better solution can lead to unnecessary overhead.

# Memory Tip
- **Use DynamoDB Streams to watch and react instead of reworking!**"
127,"[""A company has an Amazon S3 bucket containing premier content that it intends to\nmake available to only paid subscribers of its website. The S3 bucket currently\nhas default permissions of all objects being private to prevent inadvertent\nexposure of the premier content to non-paying website visitors.\n\nHow can the company limit the ability to download a premier content file in the\nS3 bucket to paid subscribers only?""]",Apply a bucket policy that allows anonymous users to download the content from the S3 bucket.,Generate a pre-signed object URL for the premier content file when a paid subscriber requests a download.,Add a bucket policy that requires multi-factor authentication for requests to access the S3 bucket objects.,Enable server-side encryption on the S3 bucket for data protection against the non-paying website visitors.,"# Answer
- **Correct option:** B : Generate a pre-signed object URL for the premier content file when a paid subscriber requests a download.
- **Reason:** Pre-signed URLs allow you to create a time-limited URL that provides temporary access to specific S3 objects. This enables the company to securely grant download access to paid subscribers without exposing the content to everyone, as the URL must be generated by someone with permission to access the content.

# Example / Analogy
- Think of a pre-signed URL like an event ticket that grants access to specific seating for a limited period. Just as you wouldn't let everyone enter the theater with a ticket meant for just one person, a pre-signed URL ensures that only the intended subscriber can access the content for a short time.

# Common Mistakes / Traps
- A common trap is to assume that bucket policies can fully manage access on their own. Options that provide open access or require additional authentication (like multifactor) don't effectively control access to specific paid content since they either expose data or complicate the access unnecessarily.

# Memory Tip
- ""Pre-sign for timing: give access, keep it secure!"""
128,"[""A developer is creating an AWS Lambda function that searches for items from an\nAmazon DynamoDB table that contains customer contact information. The DynamoDB\ntable items have the customer’s email_address as the partition key and\nadditional properties such as customer_type, name and job_title.\n\nThe Lambda function runs whenever a user types a new character into the\ncustomer_type text input. The developer wants the search to return partial\nmatches of all the email_address property of a particular customer_type. The\ndeveloper does not want to recreate the DynamoDB table.\n\nWhat should the developer do to meet these requirements?""]",Add a global secondary index (GSI) to the DynamoDB table with customer_type as the partition key and email_address as the sort key. Perform a query operation on the GSI by using the begins_with key condition expression with the email_address property.,Add a global secondary index (GSI) to the DynamoDB table with email_address as the partition key and customer_type as the sort key. Perform a query operation on the GSI by using the begins_with key condition expression with the email_address property.,Add a local secondary index (LSI) to the DynamoDB table with customer_type as the partition key and email_address as the sort key. Perform a query operation on the LSI by using the begins_with key condition expression with the email_address property.,Add a local secondary index (LSI) to the DynamoDB table with job_title as the partition key and email_address as the sort key. Perform a query operation on the LSI by using the begins_with key condition expression with the email_address property.,"# Answer
- **Correct option:** A

- **Reason:** The developer wants to perform searches based on customer type and return partial matches of the email addresses. By adding a global secondary index (GSI) with `customer_type` as the partition key, the Lambda function can query the index for all customers with a specific type, and then use the `begins_with` key condition on the `email_address` (which is the sort key) to find email addresses that match the beginning characters input by the user.

# Example / Analogy
- Think of the GSI as a specialized phone directory. If you have a directory where you can look up people not just by their name (like a phone book's traditional method), but by their profession (customer type), and once you find the right profession, you can further filter by the first few letters of their emails, which represents a more specific search.

# Common Mistakes / Traps
- A common mistake is confusing the use of global and local secondary indexes. A GSI allows you to query on a different partition key than the main table, which is essential for this scenario. Local secondary indexes (LSIs) can only use the same partition key as the base table, which won't support the required search pattern based on `customer_type` in this scenario.

# Memory Tip
- **Remember:** Use a GSI when you need flexibility in how you access data—like searching in multiple ways, not just one!"
129,"[""A developer is building an application that uses AWS API Gateway APIs, AWS\nLambda functions, and AWS DynamoDB tables. The developer uses the AWS Serverless\nApplication Model (AWS SAM) to build and run serverless applications on AWS.\nEach time the developer pushes changes for only to the Lambda functions, all the\nartifacts in the application are rebuilt.\n\nThe developer wants to implement AWS SAM Accelerate by running a command to only\nredeploy the Lambda functions that have changed.\n\nWhich command will meet these requirements?""]",sam deploy --force-upload,sam deploy --no-execute-changeset,sam package,sam sync --watch,"# Answer
- **Correct option:** D : sam sync --watch
- **Reason:** The `sam sync` command allows you to synchronize your local code changes with the AWS Lambda functions in your AWS account. With the `--watch` option, it automatically watches for changes in your code and sends only the modified Lambda function code to AWS, enabling faster deployments and avoiding the need to rebuild all artifacts in the application.

# Example / Analogy
- Think of it like a painter who is working on a mural. Instead of repainting the entire mural every time they want to make a change, they only touch up the areas that need improvement. `sam sync --watch` is like having a helper remind the painter to only focus on the sections that require modification while the rest of the mural remains untouched.

# Common Mistakes / Traps
- A common mistake is confusing `sam deploy` options like `--force-upload` and `--no-execute-changeset` with `sam sync`. While `sam deploy` is used for full deployments, it recompiles everything rather than focusing on incremental updates of just the Lambda functions, which is not efficient for smaller changes.

# Memory Tip
- Use `sam sync` for quick updates: it’s like a smart painter only fixing what needs fixing!"
130,"[""A developer is building an application that gives users the ability to view bank\naccounts from multiple sources in a single dashboard. The developer has\nautomated the process to retrieve API credentials for these sources. The process\ninvokes an AWS Lambda function that is associated with an AWS CloudFormation\ncustom resource.\n\nThe developer wants a solution that will store the API credentials with minimal\noperational overhead.\n\nWhich solution will meet these requirements in the MOST secure way?""]",Add an AWS Secrets Manager GenerateSecretString resource to the CloudFormation template. Set the value to reference new credentials for the CloudFormation resource.,Use the AWS SDK ssm:PutParameter operation in the Lambda function from the existing custom resource to store the credentials as a parameter. Set the parameter value to reference the new credentials. Set the parameter type to SecureString.,Add an AWS Systems Manager Parameter Store resource to the CloudFormation template. Set the CloudFormation resource value to reference the new credentials. Set the resource NoEcho attribute to true.,Use the AWS SDK ssm:PutParameter operation in the Lambda function from the existing custom resource to store the credentials as a parameter. Set the parameter value to reference the new credentials. Set the parameter NoEcho attribute to true.,"# Answer
- **Correct option:** A

- **Reason:** AWS Secrets Manager is designed specifically for securely storing and managing sensitive information, such as API credentials. By incorporating the `GenerateSecretString` resource into the CloudFormation template, credentials can be automatically generated and stored with built-in encryption and secrets management capabilities. This minimizes operational overhead for secure storage and retrieval of sensitive data.

# Example / Analogy
- Think of AWS Secrets Manager like a secure vault specifically built for holding valuable items like gold coins (API credentials). Just as a vault protects your gold and allows only authorized access, Secrets Manager protects your credentials and provides controlled access through IAM policies.

# Common Mistakes / Traps
- A common trap is assuming that AWS Systems Manager Parameter Store (with SecureString) provides the same level of built-in functionality and ease of management for secrets as AWS Secrets Manager. Parameter Store can be used for sensitive parameters, but Secrets Manager is tailored specifically for secrets management, including automatic rotation, which can lead to better security practices.

# Memory Tip
- Use AWS Secrets Manager for secrets needing secure storage and automatic management."
131,"[""A developer is trying to get data from an Amazon DynamoDB table called\ndemoman-table. The developer configured the AWS CLI to use a specific IAM user’s\ncredentials and ran the following command:\n\naws dynamodb get-item --table-name demoman-table --key '{\""id\"": {\""N\"":\""1993\""}}'\n\nThe command returned errors and no rows were returned.\n\nWhat is the MOST likely cause of these issues?""]",The command is incorrect; it should be rewritten to use put-item with a string argument.,The developer needs to log a ticket with AWS Support to enable access to the demoman-table.,Amazon DynamoDB cannot be accessed from the AWS CLI and needs to be called via the REST API.,The IAM user needs an associated policy with read access to demoman-table.,"# Answer
- **Correct option:** D : The IAM user needs an associated policy with read access to demoman-table.
- **Reason:** In order to access resources in AWS, the IAM user must have the appropriate permissions configured in their IAM policy. If the user doesn't have the necessary read access to the specified DynamoDB table, the command will fail, resulting in no rows being returned. 

# Example / Analogy
- Think of IAM permissions like keys to a locked door. If you have the correct key (permissions), you can open the door (access the table). If you don't have a key, you can't even see what's behind the door—just like the IAM user won’t be able to access the DynamoDB table without permissions.

# Common Mistakes / Traps
- A common mistake is assuming that simply configuring the AWS CLI with IAM user credentials provides access without verifying the permissions. Users often overlook the necessity of an appropriate IAM policy when access is denied.

# Memory Tip
- Always check IAM permissions before troubleshooting access issues in AWS resources."
132,"[""An organization is using Amazon CloudFront to ensure that its users experience\nlow-latency access to its web application. The organization has identified a\nneed to encrypt all traffic between users and CloudFront, and all traffic\nbetween CloudFront and the web application.\n\nHow can these requirements be met? (Choose two.)""]",Use AWS KMS to encrypt traffic between CloudFront and the web application.,Set the Origin Protocol Policy to “HTTPS Only”.,Set the Origin’s HTTP Port to 443.,Set the Viewer Protocol Policy to “HTTPS Only” or “Redirect HTTP to HTTPS”.,"# Answer
- **Correct options:** B, D

- **Reason:** 
  - **Option B (Set the Origin Protocol Policy to “HTTPS Only”)** ensures that all communication between CloudFront and the origin server (web application) occurs over HTTPS, which encrypts the traffic and protects it from eavesdropping.
  - **Option D (Set the Viewer Protocol Policy to “HTTPS Only” or “Redirect HTTP to HTTPS”)** secures the connection between the users and CloudFront by ensuring that users can only interact with CloudFront through HTTPS, thereby encrypting their data in-transit.

# Example / Analogy
- Think of CloudFront as a secure postal service. If you want to make sure that the packages (data) sent to and from your residence (web application) are secured, you would only accept packages that are placed in locked boxes (HTTPS). Thus, enforcing HTTPS at both ends protects the contents from being opened by anyone else.

# Common Mistakes / Traps
- A common mistake is to overlook the necessity of both policies. Students may think that only one policy (like just setting Origin Protocol Policy) is sufficient when both viewer and origin protocols need to be secured to meet encryption requirements.

# Memory Tip
- ""Secure both ends with HTTPS to keep data safe!"""
133,"[""A team of developers is using an AWS CodePipeline pipeline as a continuous\nintegration and continuous delivery (CI/CD) mechanism for a web application. A\ndeveloper has written unit tests to programmatically test the functionality of\nthe application code. The unit tests produce a test report that shows the\nresults of each individual check. The developer now wants to run these tests\nautomatically during the CI/CD process.\n\nWhich solution will meet this requirement with the LEAST operational effort?""]",Write a Git pre-commit hook that runs the tests before every commit. Ensure that each developer who is working on the project has the pre-commit hook installed locally. Review the test report and resolve any issues before pushing changes to AWS CodeCommit.,Add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage after the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues.,Add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage before the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues.,Add a new stage to the pipeline. Use Jenkins as the provider. Configure CodePipeline to use Jenkins to run the unit tests. Write a Jenkinsfile that fails the stage if any test does not pass. Use the test report plugin for Jenkins to integrate the report with the Jenkins dashboard. View the test results in Jenkins. Resolve any issues.,"# Answer
- **Correct option:** C 
- **Reason:** This option effectively integrates the unit testing phase into the CI/CD pipeline before the deployment stage, ensuring that any changes to the application code are verified and validated before they are moved to the test environment. It minimizes operational effort by using AWS CodeBuild, which is a managed service that provides a straightforward way to run builds and tests without requiring manual setup or management.

# Example / Analogy
- Think of this process like a quality check in a bakery. Before you serve a batch of cookies (deploying to test), you taste-test the dough (run unit tests) to ensure quality. If the dough is not up to standard (tests fail), you fix the recipe (the code) before going further. This ensures only the best cookies are served.

# Common Mistakes / Traps
- A common mistake is choosing option A, which relies on each developer to manually run tests locally. This adds variability and human error into the process, defeating the purpose of automation in CI/CD. It’s crucial to automate testing and integrate it into the pipeline to ensure consistency and reliability.

# Memory Tip
- ""Always test before you deploy!"" - Keep testing integrated in your CI/CD workflow."
134,"[""A company uses a custom root certificate authority certificate chain (Root CA\nCert) that is 10 KB in size to generate SSL certificates for its on-premises\nHTTPS endpoints. One of the company’s cloud-based applications has hundreds of\nAWS Lambda functions that pull data from these endpoints. A developer updated\nthe trust store of the Lambda execution environment to use the Root CA Cert when\nthe Lambda execution environment is initialized. The developer bundled the Root\nCA Cert as a text file in the Lambda deployment bundle.\n\nAfter 3 months of development, the Root CA Cert is no longer valid and must be\nupdated. The developer needs a more efficient solution to update the Root CA\nCert for all deployed Lambda functions. The solution must not include rebuilding\nor updating all Lambda functions that use the Root CA Cert. The solution must\nalso work for all development, testing, and production environments. Each\nenvironment is managed in a separate AWS account.\n\nWhich combination of steps should the developer take to meet these requirements\nMOST cost-effectively? (Choose two.)""]",Store the Root CA Cert as a secret in AWS Secrets Manager. Create a resource-based policy. Add IAM users to allow access to the secret.,Store the Root CA Cert as a SecureString parameter in AWS Systems Manager Parameter Store. Create a resource-based policy. Add IAM users to allow access to the policy.,Store the Root CA Cert in an Amazon S3 bucket. Create a resource-based policy to allow access to the bucket.,Refactor the Lambda code to load the Root CA Cert from the Root CA Cert’s location. Modify the runtime trust store inside the Lambda function handler.,"# Answer
- **Correct option:** B and D

- **Reason:** 
  - Option B is correct because using AWS Systems Manager Parameter Store to store the Root CA Cert as a SecureString allows for easy updates without redeploying Lambda functions. The functions can dynamically retrieve the updated certificate at runtime, thus providing flexibility and efficiency across multiple AWS accounts and environments.
  - Option D is also correct as it suggests refactoring the Lambda code to load the Root CA Cert from a specified location at runtime. This step avoids the need to bundle the certificate with each deployment and facilitates updating it without code changes.

# Example / Analogy
- Imagine you are storing a key to a safe in a library. If the library needs to replace the key, they can just change the lock mechanism instead of replacing the entire safe. Similarly, by storing the Root CA Cert in Parameter Store or another accessible location (like S3) and loading it at runtime, you streamline updates without needing to redeploy every Lambda function.

# Common Mistakes / Traps
- A common mistake is thinking that secrets or parameters must be directly baked into Lambda functions, which can lead to cumbersome redeployments for updates. Another trap is overlooking the need to grant necessary permissions, which could cause access issues at runtime.

# Memory Tip
- **Retrieve and refactor:** Store certificates in a managed location and refactor code to retrieve them dynamically, minimizing deployments."
135,"[""A developer is planning to migrate on-premises company data to Amazon S3. The\ndata must be encrypted, and the encryption keys must support automatic annual\nrotation. The company must use AWS Key Management Service (AWS KMS) to encrypt\nthe data.\n\nWhich type of keys should the developer use to meet these requirements?""]",Amazon S3 managed keys,Symmetric customer managed keys with key material that is generated by AWS,Asymmetric customer managed keys with key material that is generated by AWS,Symmetric customer managed keys with imported key material,"# Answer
- **Correct option:** B : Symmetric customer managed keys with key material that is generated by AWS
- **Reason:** Symmetric customer managed keys allow for automatic rotation, which aligns with the requirement for annual key rotation. These keys use key material generated by AWS and can be managed via AWS KMS, which provides the necessary encryption features and compliance.

# Example / Analogy
- Imagine you have a safe where you can automatically change the combination every year. Using symmetric customer managed keys with key material generated by AWS is like having a smart safe that not only keeps your valuables secure but also refreshes its combination automatically without requiring you to manually change it.

# Common Mistakes / Traps
- Many might confuse Amazon S3 managed keys as suitable for this requirement. However, S3 managed keys do not provide the option for key rotation, which is essential in this case. Another common misconception is that asymmetric keys would be usable in typical data encryption scenarios, which they are not in this context.

# Memory Tip
- Think ""Symmetric = Simple rotation"" for easy recall of the key rotation feature."
136,"[""A company has multiple Amazon VPC endpoints in the same VPC. A developer needs\nto configure an Amazon S3 bucket policy so users can access an S3 bucket only by\nusing these VPC endpoints.\n\nWhich solution will meet these requirements?""]",Create multiple S3 bucket polices by using each VPC endpoint ID that have the aws:SourceVpce value in the StringNotEquals condition.,Create a single S3 bucket policy that has the aws:SourceVpc value and in the StringNotEquals condition to use VPC ID.,Create a single S3 bucket policy that has the aws:SourceVpce value and in the StringNotEquals condition to use vpce*.,Create a single S3 bucket policy that has multiple aws:sourceVpce value in the StringNotEquals condition. Repeat for all the VPC endpoint IDs.,"# Answer
- **Correct option:** D
- **Reason:** The correct way to restrict access to an S3 bucket via specific VPC endpoints is to create a single S3 bucket policy that includes each VPC endpoint ID in the `StringEquals` condition for the `aws:SourceVpce` key. This ensures that only requests made through specified VPC endpoints can access the bucket, while denying access through any other means.

# Example / Analogy
- Think of it like an exclusive club that only allows entry through specific gates (VPC endpoints). Each gate has its own ID, and to gain access, you must enter through one of those designated gates. The club (S3 bucket) checks if the gate ID you use (sourceVpce) matches one of the approved gates before letting you in.

# Common Mistakes / Traps
- A common mistake is to use a wildcard or `StringNotEquals` condition when specifying VPC endpoint IDs. This can lead to access being granted through unintended endpoints or configurations that don't specifically enumerate the allowed endpoints.

# Memory Tip
- ""Use specific IDs in `aws:SourceVpce` for controlled bucket access."""
137,"[""A developer maintains applications that store several secrets in AWS Secrets\nManager. The applications use secrets that have changed over time. The developer\nneeds to identify required secrets that are still in use. The developer does not\nwant to cause any application downtime.\n\nWhat should the developer do to meet these requirements?""]",Configure an AWS CloudTrail log file delivery to an Amazon S3 bucket. Create an Amazon CloudWatch alarm for the GetSecretValue Secrets Manager API operation requests.,Create a secretsmanager-secret-unused AWS Config managed rule. Create an Amazon EventBridge rule to initiate notifications when the AWS Config managed rule is met.,Deactivate the applications secrets and monitor the applications error logs temporarily.,Configure AWS X-Ray for the applications. Create a sampling rule to match the GetSecretValue Secrets Manager API operation requests.,"# Answer
- **Correct option:** A

- **Reason:** Configuring an AWS CloudTrail log file delivery to an Amazon S3 bucket allows the developer to record all API operations, including those related to AWS Secrets Manager. By creating a CloudWatch alarm for the GetSecretValue Secrets Manager API operation, the developer can monitor which secrets are being accessed without impacting application performance or causing downtime. This method provides insight into which secrets are actively used by the applications.

# Example / Analogy
- Think of it like keeping a log of books borrowed from a library. By recording which books are checked out (using CloudTrail logs), the library can easily see which books are popular and which are gathering dust on the shelves, without having to remove any from the shelves or interrupt the borrowing process.

# Common Mistakes / Traps
- A common mistake might be to choose options that disrupt application operations or could lead to application downtime, such as deactivating secrets (Option C), which could cause immediate failures in applications reliant on those secrets. Additionally, relying on AWS Config rules (Option B) might not provide immediate insight into secret usage patterns.

# Memory Tip
- **Log secrets usage to track importance without impacting functionality!**"
138,"[""A developer is writing a serverless application that requires an AWS Lambda\nfunction to be invoked every 10 minutes.\n\nWhat is an automated and serverless way to invoke the function?""]","Deploy an Amazon EC2 instance based on Linux, and edit its /etc/crontab file by adding a command to periodically invoke the Lambda function.",Configure an environment variable named PERIOD for the Lambda function. Set the value to 600.,Create an Amazon EventBridge rule that runs on a regular schedule to invoke the Lambda function.,Create an Amazon Simple Notification Service (Amazon SNS) topic that has a subscription to the Lambda function with a 600-second timer.,"# Answer
- **Correct option:** C : Create an Amazon EventBridge rule that runs on a regular schedule to invoke the Lambda function.

- **Reason:** Amazon EventBridge (formerly called CloudWatch Events) is designed for event-driven computing and can be used to trigger AWS Lambda functions on a schedule. By defining a rule that triggers every 10 minutes, you can automate the invocation of your Lambda function without the need for managing any servers or instances.

# Example / Analogy
- Think of EventBridge like a digital alarm clock that you set to wake you up at a specific time. Just as the alarm triggers an action (waking you up) at predefined intervals, EventBridge triggers your Lambda function according to the schedule you set, allowing you to run tasks regularly without any manual effort.

# Common Mistakes / Traps
- A common mistake is to choose methods that require server management or constant runtime, such as using an EC2 instance (Option A) which is not serverless. Another trap is misunderstanding how timings work and selecting options that imply direct control over intervals, like environment variables (Option B) or SNS subscriptions (Option D), which do not inherently provide scheduling capabilities.

# Memory Tip
- Remember: **""EventBridge is my serverless scheduler!""**"
139,"[""A company is using Amazon OpenSearch Service to implement an audit monitoring\nsystem. A developer needs to create an AWS CloudFormation custom resource that\nis associated with an AWS Lambda function to configure the OpenSearch Service\ndomain. The Lambda function must access the OpenSearch Service domain by using\nOpenSearch Service internal master user credentials.\n\nWhat is the MOST secure way to pass these credentials to the Lambda function?""]",Use a CloudFormation parameter to pass the master user credentials at deployment to the OpenSearch Service domain’s MasterUserOptions and the Lambda function’s environment variable. Set the NoEcho attribute to true.,Use a CloudFormation parameter to pass the master user credentials at deployment to the OpenSearch Service domain’s MasterUserOptions and to create a parameter in AWS Systems Manager Parameter Store. Set the NoEcho attribute to true. Create an IAM role that has the ssm:GetParameter permission. Assign the role to the Lambda function. Store the parameter name as the Lambda function’s environment variable. Resolve the parameter’s value at runtime.,Use a CloudFormation parameter to pass the master user credentials at deployment to the OpenSearch Service domain’s MasterUserOptions and the Lambda function’s environment variable. Encrypt the parameter’s value by using the AWS Key Management Service (AWS KMS) encrypt command.,Use CloudFormation to create an AWS Secrets Manager secret. Use a CloudFormation dynamic reference to retrieve the secret’s value for the OpenSearch Service domain’s MasterUserOptions. Create an IAM role that has the secretsmanager:GetSecretValue permission. Assign the role to the Lambda function. Store the secret’s name as the Lambda function’s environment variable. Resolve the secret’s value at runtime.,"# Answer
- **Correct option:** D

- **Reason:** This option leverages AWS Secrets Manager, which is specifically designed for managing sensitive information such as credentials. By using a secret, you can easily rotate credentials, and the Lambda function can access them securely through an IAM role, keeping sensitive data hidden and adding a layer of security compared to other methods.

# Example / Analogy
- Think of AWS Secrets Manager like a locked safe where you can store your valuable items (credentials). Just like you wouldn't share the key to your safe with everyone, you only grant access to those who absolutely need it (the Lambda function in this case) through permissions. This ensures that even if someone gets access to the Lambda function, they still can't see what’s in the safe unless they have the key.

# Common Mistakes / Traps
- A common mistake is assuming that simply encryption (like in option C) is sufficient for securing credentials. While encryption adds a layer of security, managing secrets and roles properly, as done in option D, is a much more comprehensive and secure approach. Using CloudFormation parameters (option A and B) for passing credentials could expose sensitive information if not handled correctly.

# Memory Tip
- Always use AWS Secrets Manager for sensitive credentials to ensure they are managed securely."
140,"[""An application runs on multiple EC2 instances behind an ELB.\n\nWhere is the session data best written so that it can be served reliably across\nmultiple requests?""]",Write data to Amazon ElastiCache.,Write data to Amazon Elastic Block Store.,Write data to Amazon EC2 Instance Store.,Write data to the root filesystem.,"# Answer
- **Correct option:** A: Write data to Amazon ElastiCache.
- **Reason:** Amazon ElastiCache is a fully managed, in-memory data store that supports session state persistence across multiple application servers. By using ElastiCache, session data can be accessed quickly and consistently by all EC2 instances behind the Elastic Load Balancer (ELB), ensuring that users have a seamless experience regardless of which instance handles their request.

# Example / Analogy
- Imagine ElastiCache as a shared whiteboard in a group project, where all team members can write and refer to notes in real-time. No matter who is looking at the board (which EC2 instance is handling the request), everyone has access to the same information, making it easy to keep track of their discussions (session data).

# Common Mistakes / Traps
- A common mistake is selecting options like Amazon Elastic Block Store (EBS), Amazon EC2 Instance Store, or the root filesystem, thinking they provide persistent or shared storage for session data. However, these options either do not offer the necessary quick access across instances or are not designed for scalable and distributed session data storage.

# Memory Tip
- Remember: ""For scalable sessions, think ElastiCache!"""
141,"[""An ecommerce application is running behind an Application Load Balancer. A\ndeveloper observes some unexpected load on the application during non-peak\nhours. The developer wants to analyze patterns for the client IP addresses that\nuse the application.\n\nWhich HTTP header should the developer use for this analysis?""]",The X-Forwarded-Proto header,The X-Forwarded-Host header,The X-Forwarded-For header,The X-Forwarded-Port header,"# Answer
- **Correct option:** C : The X-Forwarded-For header
- **Reason:** The X-Forwarded-For header is used to identify the originating IP address of a client connecting to a web server through an intermediary (like an Application Load Balancer). This header is essential for the developer to analyze which clients are making requests to the application, especially when observing unexpected loads.

# Example / Analogy
- Think of the X-Forwarded-For header as a postmark on a letter that shows where it originally came from, despite passing through several hands (intermediaries). Just like a postmark helps you know the true origin of the letter, the X-Forwarded-For header reveals the real IP address of the client, even when the client connects through a load balancer.

# Common Mistakes / Traps
- A common mistake is to confuse the X-Forwarded-For header with other headers, such as X-Forwarded-Proto or X-Forwarded-Port, which do not provide client IP address information. Developers may overlook the importance of the correct header when troubleshooting issues related to traffic sources.

# Memory Tip
- Remember: The ""For"" in X-Forwarded-For indicates it’s about the ""originating"" client IP address."
142,"[""A developer migrated a legacy application to an AWS Lambda function. The\nfunction uses a third-party service to pull data with a series of API calls at\nthe end of each month. The function then processes the data to generate the\nmonthly reports. The function has been working with no issues so far.\n\nThe third-party service recently issued a restriction to allow a fixed number of\nAPI calls each minute and each day. If the API calls exceed the limit for each\nminute or each day, then the service will produce errors. The API also provides\nthe minute limit and daily limit in the response header. This restriction might\nextend the overall process to multiple days because the process is consuming\nmore API calls than the available limit.\n\nWhat is the MOST operationally efficient way to refactor the serverless\napplication to accommodate this change?""]",Use an AWS Step Functions state machine to monitor API failures. Use the Wait state to delay calling the Lambda function.,Use an Amazon Simple Queue Service (Amazon SQS) queue to hold the API calls. Configure the Lambda function to poll the queue within the API threshold limits.,Use an Amazon CloudWatch Logs metric to count the number of API calls. Configure an Amazon CloudWatch alarm that stops the currently running instance of the Lambda function when the metric exceeds the API threshold limits.,Use Amazon Kinesis Data Firehose to batch the API calls and deliver them to an Amazon S3 bucket with an event notification to invoke the Lambda function.,"# Answer
- **Correct option:** B : Use an Amazon Simple Queue Service (Amazon SQS) queue to hold the API calls. Configure the Lambda function to poll the queue within the API threshold limits.

- **Reason:** Using Amazon SQS allows the application to effectively manage API calls by queuing them up and ensuring that the Lambda function only processes a number of calls that adhere to the limits set by the third-party service. The queuing mechanism creates elasticity and reliability, allowing the processing of calls over time without exceeding rate limits. SQS can handle high throughput, and the Lambda function can be set to poll messages from the queue while respecting the API's rate limits.

# Example / Analogy
- Think of it like a bakery that can only bake a limited number of cakes every hour due to an oven restriction (the API's limit). Instead of trying to bake all the cakes at once, the bakery can let customers place orders (SQS) and bake them one by one as per the oven's capacity. This ensures that they comply with the limitation without losing orders and also makes the process smoother.

# Common Mistakes / Traps
- A common trap is to assume that monitoring and stopping the function (like in option C) is a viable long-term solution. This is less efficient, as it may disrupt processing and create operational challenges. Option A, while relevant, does not inherently solve the problem of exceeding the API limits directly.

# Memory Tip
- Think of SQS as a ""waiting line"" that respects capacity limits while ensuring that all tasks are processed efficiently."
143,"[""A developer must analyze performance issues with production-distributed\napplications written as AWS Lambda functions. These distributed Lambda\napplications invoke other components that make up the applications.\n\nHow should the developer identify and troubleshoot the root cause of the\nperformance issues in production?""]","Add logging statements to the Lambda functions, then use Amazon CloudWatch to view the logs.",Use AWS CloudTrail and then examine the logs.,"Use AWS X-Ray, then examine the segments and errors.",Run Amazon Inspector agents and then analyze performance.,"# Answer
- **Correct option:** C : Use AWS X-Ray, then examine the segments and errors.
- **Reason:** AWS X-Ray is specifically designed for analyzing and debugging distributed applications, including Lambda functions. It provides a detailed view of the request flow through the different components, allowing the developer to pinpoint performance bottlenecks, see execution times, and analyze errors. This level of insight is essential for performance troubleshooting in distributed architectures.

# Example / Analogy
- Imagine trying to fix a leaky faucet in your house that has multiple sinks, all connected by pipes. Instead of just listening for drips (logging), you need to trace the water flow and see where the leak is actually occurring (X-Ray). AWS X-Ray helps visualize that flow through your application's components, making it easier to see where the problem lies.

# Common Mistakes / Traps
- A common mistake is to solely rely on logging (Option A). While logging is useful, it doesn't provide the same depth of insight into the interactions and latencies between microservices that X-Ray does. Additionally, some may confuse CloudTrail (Option B) as being suitable for performance troubleshooting, whereas it is mainly used for auditing API calls and changes in AWS services.

# Memory Tip
- **""Use X-Ray to draw the line on your application’s performance.""**"
144,"[""A developer wants to deploy a new version of an AWS Elastic Beanstalk\napplication. During deployment, the application must maintain full capacity and\navoid service interruption. Additionally, the developer must minimize the cost\nof additional resources that support the deployment.\n\nWhich deployment method should the developer use to meet these requirements?""]",All at once,Rolling with additional batch,Blue/green,Immutable,"# Answer
- **Correct option:** C : Blue/green

- **Reason:** The blue/green deployment method involves creating a new environment (the ""green"" environment) alongside the current environment (the ""blue"" environment). This allows for full capacity to be maintained during the deployment, as the application can switch traffic from blue to green without any service interruption. Additionally, while this method may temporarily increase costs due to running two environments, it ensures minimal risk and a seamless transition.

# Example / Analogy
- Think of a blue/green deployment like a restaurant opening a new location (green) while keeping the existing location (blue) open. Customers can continue to dine at the original location while the new one gets ready. Once everything is perfect at the new location, you can direct customers to it without any downtime, ensuring service remains uninterrupted.

# Common Mistakes / Traps
- A common mistake is choosing the ""All at once"" method, which introduces significant risk of downtime since all instances are updated simultaneously, leading to potential service interruption.

# Memory Tip
- ""Blue/green means smooth transitions, maintaining green while blue serves."""
145,"[""A developer is troubleshooting an application that uses Amazon DynamoDB in the\nus-west-2 Region. The application is deployed to an Amazon EC2 instance. The\napplication requires read-only permissions to a table that is named Cars. The\nEC2 instance has an attached IAM role that contains the following IAM policy:\n\n[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image12.png]\n\nWhen the application tries to read from the Cars table, an Access Denied error\noccurs.\n\nHow can the developer resolve this error?""]",Modify the IAM policy resource to be “arn:aws:dynamodb:us-west-2:account-id:table/*”.,Modify the IAM policy to include the dynamodb:* action.,Create a trust policy that specifies the EC2 service principal. Associate the role with the policy.,Create a trust relationship between the role and dynamodb.amazonaws.com.,"# Answer
- **Correct option:** A : Modify the IAM policy resource to be “arn:aws:dynamodb:us-west-2:account-id:table/*”.

- **Reason:** The Access Denied error indicates that the IAM role attached to the EC2 instance does not have sufficient permissions to access the ""Cars"" table. In this scenario, the IAM policy likely restricts access to a specific table (which may not be ""Cars"") or may not match the resource accurately. By modifying the policy resource to include ""arn:aws:dynamodb:us-west-2:account-id:table/*"", the role will have access to all tables within that account in the specified region, including ""Cars.""

# Example / Analogy
- Think of IAM policies as keys to a house. If you give someone a key that only opens a certain room (a specific table) but they need to access the entire house (all tables), they won't be able to enter. By giving them a key to the whole house (using a wildcard), they can access the necessary rooms (tables) as needed.

# Common Mistakes / Traps
- A common mistake is to assume that permission errors are solely due to the actions specified (like using `dynamodb:*`). However, it may also stem from incorrect resource ARNs, which need to match the actual resources the policy is intended to allow access to.

# Memory Tip
- Always check the resource ARN in IAM policies to ensure the correct access scope is defined."
146,"[""A developer has observed an increase in bugs in the AWS Lambda functions that a\ndevelopment team has deployed in its Node.js application. To minimize these\nbugs, the developer wants to implement automated testing of Lambda functions in\nan environment that closely simulates the Lambda environment.\n\nThe developer needs to give other developers the ability to run the tests\nlocally. The developer also needs to integrate the tests into the team’s\ncontinuous integration and continuous delivery (CI/CD) pipeline before the AWS\nCloud Development Kit (AWS CDK) deployment.\n\nWhich solution will meet these requirements?""]",Create sample events based on the Lambda documentation. Create automated test scripts that use the cdk local invoke command to invoke the Lambda functions. Check the response. Document the test scripts for the other developers on the team. Update the CI/CD pipeline to run the test scripts.,Install a unit testing framework that reproduces the Lambda execution environment. Create sample events based on the Lambda documentation. Invoke the handler function by using a unit testing framework. Check the response. Document how to run the unit testing framework for the other developers on the team. Update the CI/CD pipeline to run the unit testing framework.,Install the AWS Serverless Application Model (AWS SAM) CLI tool. Use the sam local generate-event command to generate sample events for the automated tests. Create automated test scripts that use the sam local invoke command to invoke the Lambda functions. Check the response. Document the test scripts for the other developers on the team. Update the CI/CD pipeline to run the test scripts.,Create sample events based on the Lambda documentation. Create a Docker container from the Node.js base image to invoke the Lambda functions. Check the response. Document how to run the Docker container for the other developers on the team. Update the CI/CD pipeline to run the Docker container.,"# Answer
- **Correct option:** C

- **Reason:** The AWS Serverless Application Model (AWS SAM) CLI provides tools that closely simulate the AWS Lambda execution environment on your local machine. Using `sam local generate-event` allows developers to create sample events that can be used for testing Lambda functions. The command `sam local invoke` enables the invocation of Lambda functions as they would be executed in the AWS environment. This makes it an ideal solution for automated testing in a local environment that mirrors the actual Lambda environment. Additionally, integrating this process into a CI/CD pipeline ensures consistent testing before deployment.

# Example / Analogy
- Imagine you are rehearsing a play. You need a stage that closely resembles the actual theater, with the same lighting, sounds, and props. Using AWS SAM is like having a practice stage that mimics the real one, allowing you and your cast (the development team) to prepare thoroughly before the real performance (the deployment), ensuring fewer mistakes during the actual show.

# Common Mistakes / Traps
- A common mistake is to assume that just using sample events without a proper local execution environment (like a mock or unit testing framework) will provide adequate testing coverage. This may lead to unexpected behavior when the Lambda functions are deployed in AWS.

# Memory Tip
- Use AWS SAM for local Lambda testing—it's like having a rehearsal space that feels just like the real stage!"
147,"[""A developer has code that is stored in an Amazon S3 bucket. The code must be\ndeployed as an AWS Lambda function across multiple accounts in the same AWS\nRegion as the S3 bucket. An AWS CloudFormation template that runs for each\naccount will deploy the Lambda function.\n\nWhat is the MOST secure way to allow CloudFormation to access the Lambda code in\nthe S3 bucket?""]",Grant the CloudFormation service role the S3 ListBucket and GetObject permissions. Add a bucket policy to Amazon S3 with the principal of “AWS”: [account numbers].,Grant the CloudFormation service role the S3 GetObject permission. Add a bucket policy to Amazon S3 with the principal of “*”.,Use a service-based link to grant the Lambda function the S3 ListBucket and GetObject permissions by explicitly adding the S3 bucket’s account number in the resource.,Use a service-based link to grant the Lambda function the S3 GetObject permission. Add a resource of “*” to allow access to the S3 bucket.,"# Answer
- **Correct option:** A: Grant the CloudFormation service role the S3 ListBucket and GetObject permissions. Add a bucket policy to Amazon S3 with the principal of “AWS”: [account numbers].

- **Reason:** This option is the most secure because it restricts access to only the specific AWS accounts that need it, ensuring that only authorized CloudFormation stacks from those accounts can access the Lambda code in the S3 bucket. By granting both ListBucket and GetObject permissions, you ensure that CloudFormation can not only retrieve the Lambda code but also list the objects in the bucket, which is often necessary in a deployment scenario.

# Example / Analogy
- Think of an S3 bucket as a library. If you want to allow a specific group of people (the developers in a few trusted accounts) to borrow books (Lambda code), you would create a policy to let them not only check out books but also look at what books are available. Allowing access only to certain people (specific account numbers) is like giving out library cards only to your friends and family, while a general “everyone” approach would be like leaving the library unlocked for anyone to take books whenever they want.

# Common Mistakes / Traps
- A common mistake is to select options that provide overly broad access, such as using the principal of “*” (anyone) in a bucket policy, which can expose sensitive code to unauthorized users. It can also be tempting to ignore the principle of least privilege, leading to potential vulnerabilities in security.

# Memory Tip
- Secure access is like giving keys only to specific trusted individuals; restrict who can access what!"
148,"[""An application that runs on AWS Lambda requires access to specific highly\nconfidential objects in an Amazon S3 bucket. In accordance with the principle of\nleast privilege, a company grants access to the S3 bucket by using only\ntemporary credentials.\n\nHow can a developer configure access to the S3 bucket in the MOST secure way?""]",Hardcode the credentials that are required to access the S3 objects in the application code. Use the credentials to access the required S3 objects.,Create a secret access key and access key ID with permission to access the S3 bucket. Store the key and key ID in AWS Secrets Manager. Configure the application to retrieve the Secrets Manager secret and use the credentials to access the S3 objects.,Create a Lambda function execution role. Attach a policy to the role that grants access to specific objects in the S3 bucket.,Create a secret access key and access key ID with permission to access the S3 bucket. Store the key and key ID as environment variables in Lambda. Use the environment variables to access the required S3 objects.,"# Answer
- **Correct option:** C
- **Reason:** Using a Lambda function execution role to grant access to specific objects in the S3 bucket ensures that the application operates under the principle of least privilege. The execution role is automatically managed by AWS, and temporary security credentials are provided, which enhances security by eliminating the need for hardcoded or stored static credentials.

# Example / Analogy
- Think of the execution role like a guest badge at a corporate event. You get a badge that grants you access only to certain areas based on your role and needs for that event. Once the event is over, the badge is no longer valid, ensuring that only authorized personnel can access sensitive areas.

# Common Mistakes / Traps
- A common mistake is choosing options that involve hardcoding credentials or using static keys (Option A and D), which poses a significant security risk. Additionally, while using AWS Secrets Manager (as in Option B) is secure, it still requires managing static credentials, which goes against the principle of least privilege when possible.

# Memory Tip
- ""Use Lambda's execution role for secure, temporary access!"""
149,"[""When using the AWS Encryption SDK, how does the developer keep track of the data\nencryption keys used to encrypt data?""]",The developer must manually keep track of the data encryption keys used for each data object.,The SDK encrypts the data encryption key and stores it (encrypted) as part of the returned ciphertext.,The SDK stores the data encryption keys automatically in Amazon S3.,The data encryption key is stored in the Userdata for the EC2 instance.,"# Answer
- **Correct option:** B : The SDK encrypts the data encryption key and stores it (encrypted) as part of the returned ciphertext.
- **Reason:** The AWS Encryption SDK utilizes a technique called key wrapping, where the data encryption key (DEK) is itself encrypted with a key encryption key (KEK) and stored as part of the encrypted data. This process ensures that the DEK is securely managed without requiring the developer to track or store it manually. By including the wrapped DEK within the ciphertext, it also simplifies decryption, as the SDK can automatically extract and decrypt the DEK when needed.

# Example / Analogy
- Think of the scenario where you have a valuable item (like a diamond) you want to keep safe. Instead of carrying the diamond around with you and risking losing it, you put it into a very secure vault (this represents the encrypted form of the DEK). You keep the vault key (the KEK) safe and, when you need the diamond back, you can use the vault key to access your diamond effortlessly. The encrypted data is your vault, and the key to the vault is secured separately.

# Common Mistakes / Traps
- A common mistake is choosing option A, thinking that manual tracking is needed for security, not realizing that the SDK provides a robust solution for managing encryption keys. Another mistake could be assuming that AWS services automatically store encryption keys for you (as suggested in option C), which is not the case here.

# Memory Tip
- Remember: ""The AWS Encryption SDK wraps the key, so you don’t have to track it!"""
150,"[""A developer at a company needs to create a small application that makes the same\nAPI call once each day at a designated time. The company does not have\ninfrastructure in the AWS Cloud yet, but the company wants to implement this\nfunctionality on AWS.\n\nWhich solution meets these requirements in the MOST operationally efficient\nmanner?""]",Use a Kubernetes cron job that runs on Amazon Elastic Kubernetes Service (Amazon EKS).,Use an Amazon Linux crontab scheduled job that runs on Amazon EC2.,Use an AWS Lambda function that is invoked by an Amazon EventBridge scheduled event.,Use an AWS Batch job that is submitted to an AWS Batch job queue.,"# Answer
- **Correct option:** C : Use an AWS Lambda function that is invoked by an Amazon EventBridge scheduled event.
- **Reason:** This solution is the most operationally efficient because AWS Lambda is a serverless compute service that automatically scales and manages resources, meaning there is no infrastructure to maintain or pay for when it's not in use. Amazon EventBridge can easily schedule events to trigger the Lambda function at the specified time each day, making it a perfect fit for running a task on a schedule without managing servers.

# Example / Analogy
- Think of AWS Lambda and EventBridge like a self-watering garden. You set up an automated system (EventBridge) that waters your plants (invokes Lambda) at a designated time each day. This system requires no additional resources or upkeep once set, just like you don’t need to physically be there to water your plants every day.

# Common Mistakes / Traps
- A common mistake is to overthink and choose solutions that involve maintaining infrastructure (like EC2 or EKS) when a simpler serverless option exists. Developers might also underestimate the ease of setup and operational efficiency of using EventBridge with Lambda.

# Memory Tip
- ""For scheduled tasks, go serverless with Lambda and EventBridge!"""
151,"[""A developer is building a serverless application that is based on AWS Lambda.\nThe developer initializes the AWS software development kit (SDK) outside of the\nLambda handler function.\n\nWhat is the PRIMARY benefit of this action?""]",Improves legibility and stylistic convention,Takes advantage of runtime environment reuse,Provides better error handling,Creates a new SDK instance for each invocation,"# Answer
- **Correct option:** B : Takes advantage of runtime environment reuse
- **Reason:** Initializing the AWS SDK outside of the Lambda handler function allows the SDK to be reused across multiple invocations of the Lambda function. This means that the SDK does not need to be reloaded and reconfigured each time the function runs, which can lead to reduced cold start times and improved performance.

# Example / Analogy
- Real-life example or memorable comparison: Think of it like preparing a cooking station. If you set up your tools (like knives and pots) before you start cooking, you can cook multiple meals faster without wasting time gathering your tools each time. Similarly, by initializing the SDK once, the Lambda function can handle more requests efficiently without the overhead of repeated setups.

# Common Mistakes / Traps
- Common trap points, if any: Developers might incorrectly think that creating a new instance of the SDK in each invocation is beneficial for isolation or error handling, but this actually adds unnecessary overhead and slows down execution.

# Memory Tip
- Initialize once, reuse forever – keep your SDK warm for faster Lambda performance!"
152,"[""A company is using Amazon RDS as the backend database for its application. After\na recent marketing campaign, a surge of read requests to the database increased\nthe latency of data retrieval from the database. The company has decided to\nimplement a caching layer in front of the database. The cached content must be\nencrypted and must be highly available.\n\nWhich solution will meet these requirements?""]",Amazon CloudFront,Amazon ElastiCache for Memcached,Amazon ElastiCache for Redis in cluster mode,Amazon DynamoDB Accelerator (DAX),"# Answer
- **Correct option:** C : Amazon ElastiCache for Redis in cluster mode
- **Reason:** Amazon ElastiCache for Redis in cluster mode provides a caching layer that supports encryption at rest and in transit, ensuring that cached content is secure. Additionally, Redis supports high availability through automatic failover and replication features, which are essential for maintaining performance and reliability during surges in read requests.

# Example / Analogy
- Think of ElastiCache for Redis like a premium vault for frequently accessed documents (data) that not only keeps them secure (encrypted) but also ensures there's always access without delays (high availability) even if the main storage is temporarily overloaded.

# Common Mistakes / Traps
- A common mistake is to select Amazon CloudFront, which is primarily a content delivery network (CDN) for web assets rather than a caching layer for database queries. Also, while ElastiCache for Memcached provides caching functionality, it does not support persistence and encryption features as robustly as Redis.

# Memory Tip
- Remember: ""Redis is secure and reliable for cache, making your app faster in busy times!"""
153,"[""A developer at a company recently created a serverless application to process\nand show data from business reports. The application’s user interface (UI)\nallows users to select and start processing the files. The UI displays a message\nwhen the result is available to view. The application uses AWS Step Functions\nwith AWS Lambda functions to process the files. The developer used Amazon API\nGateway and Lambda functions to create an API to support the UI.\n\nThe company’s UI team reports that the request to process a file is often\nreturning timeout errors because of the size or complexity of the files. The UI\nteam wants the API to provide an immediate response so that the UI can display a\nmessage while the files are being processed. The backend process that is invoked\nby the API needs to send an email message when the report processing is\ncomplete.\n\nWhat should the developer do to configure the API to meet these requirements?""]",Change the API Gateway route to add an X-Amz-Invocation-Type header with a static value of ‘Event’ in the integration request. Deploy the API Gateway stage to apply the changes.,Change the configuration of the Lambda function that implements the request to process a file. Configure the maximum age of the event so that the Lambda function will run asynchronously.,Change the API Gateway timeout value to match the Lambda function timeout value. Deploy the API Gateway stage to apply the changes.,Change the API Gateway route to add an X-Amz-Target header with a static value of ‘Async’ in the integration request. Deploy the API Gateway stage to apply the changes.,"# Answer
- **Correct option:** A
- **Reason:** Option A suggests changing the API Gateway route to add an `X-Amz-Invocation-Type` header with a static value of `'Event'`. This tells the API Gateway to invoke the Lambda function asynchronously. By doing this, the API can return an immediate response to the UI while the file processing occurs in the background, meeting the UI team’s requirement of not timing out.

# Example / Analogy
- Think of a restaurant where you place an order (API call) and get a notification (immediate response) that your meal will be ready soon. The kitchen (Lambda function) prepares the meal asynchronously, allowing you to carry on with your tasks until your meal is served. The restaurant doesn't keep you waiting at the counter, while you get a message confirming your order was received.

# Common Mistakes / Traps
- A common mistake might be to choose option C, thinking increasing the API Gateway timeout would resolve the issue, but this doesn't address the root problem of requiring an immediate response while processing happens in the background.

# Memory Tip
- Remember: Asynchronous invocation allows you to say ""I’ll handle it later!"" Instead of waiting, which is crucial for user experience in serverless applications."
154,"[""A developer has an application that is composed of many different AWS Lambda\nfunctions. The Lambda functions all use some of the same dependencies. To avoid\nsecurity issues, the developer is constantly updating the dependencies of all of\nthe Lambda functions. The result is duplicated effort for each function.\n\nHow can the developer keep the dependencies of the Lambda functions up to date\nwith the LEAST additional complexity?""]",Define a maintenance window for the Lambda functions to ensure that the functions get updated copies of the dependencies.,Upgrade the Lambda functions to the most recent runtime version.,Define a Lambda layer that contains all of the shared dependencies.,Use an AWS CodeCommit repository to host the dependencies in a centralized location.,"# Answer
- **Correct option:** C : Define a Lambda layer that contains all of the shared dependencies.
  
- **Reason:** Using a Lambda Layer allows the developer to package and manage shared dependencies in a single place. This approach reduces duplication of effort, as the Lambda functions can reference the same layer instead of including the dependencies within each function's deployment package. This not only simplifies the update process but also minimizes the size of each Lambda function, leading to faster deployments and lower potential for errors in dependency management.

# Example / Analogy
- Think of Lambda layers like a library where multiple students can borrow the same books instead of each student individually buying their own. This library approach allows for easy updates and shared access without the need for everyone to manage their own collection.

# Common Mistakes / Traps
- A common mistake could be choosing to update all functions individually (as in choice A), which leads to increased complexity and risk of missing updates. Another trap is assuming that upgrading the runtime (choice B) will inherently resolve dependency issues, which it may not, as dependencies are often unrelated to the language runtime itself. Option D may centralize the code but not efficiently update functions since it doesn't utilize Lambda's built-in layer feature.

# Memory Tip
- **Layers for shared dependencies:** Use Lambda layers to streamline and simplify dependency management for your functions."
155,"[""A developer is modifying an existing AWS Lambda function. While checking the\ncode, the developer notices hardcoded parameter values for an Amazon RDS for SQL\nServer user name, password, database, host, and port. There are also hardcoded\nparameter values for an Amazon DynamoDB table, an Amazon S3 bucket, and an\nAmazon Simple Notification Service (Amazon SNS) topic.\n\nThe developer wants to securely store the parameter values outside the code in\nan encrypted format and wants to turn on rotation for the credentials. The\ndeveloper also wants to be able to reuse the parameter values from other\napplications and to update the parameter values without modifying code.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]","Create an RDS database secret in AWS Secrets Manager. Set the user name, password, database, host, and port. Turn on secret rotation. Create encrypted Lambda environment variables for the DynamoDB table, S3 bucket, and SNS topic.","Create an RDS database secret in AWS Secrets Manager. Set the user name, password, database, host, and port. Turn on secret rotation. Create SecureString parameters in AWS Systems Manager Parameter Store for the DynamoDB table, S3 bucket, and SNS topic.","Create RDS database parameters in AWS Systems Manager Parameter Store for the user name, password, database, host, and port. Create encrypted Lambda environment variables for the DynamoDB table, S3 bucket, and SNS topic. Create a Lambda function and set the logic for the credentials rotation task. Schedule the credentials rotation task in Amazon EventBridge.","Create RDS database parameters in AWS Systems Manager Parameter Store for the user name, password, database, host, and port. Store the DynamoDB table, S3 bucket, and SNS topic in Amazon S3. Create a Lambda function and set the logic for the credentials rotation. Invoke the Lambda function on a schedule.","# Answer
- **Correct option:** B

- **Reason:** Option B meets all the developer's requirements by using AWS Secrets Manager to create a secure, rotatable secret for the RDS database parameters and AWS Systems Manager Parameter Store to securely store the configurations for the DynamoDB table, S3 bucket, and SNS topic as SecureString parameters. This setup allows for centralized management of sensitive information, easy updates without changing code, and reuse among different applications with minimal operational overhead.

# Example / Analogy
- Think of it like storing your important documents in a secure cloud vault (Secrets Manager for RDS) while also having labeled envelopes for each category (Parameter Store for DynamoDB, S3, and SNS). You can easily access and update the contents of the envelopes, while the important documents are kept safe and can be automatically replaced every few months without you having to break into the vault.

# Common Mistakes / Traps
- A common mistake might be to think that using only Parameter Store or hardcoding values in Lambda is sufficient. Additionally, some might overlook that not all options turned on automatic rotation, which is a key requirement.

# Memory Tip
- ""Secrets Manage safely, Parameters Store securely for flexibility."""
156,"[""A mobile app stores blog posts in an Amazon DynamoDB table. Millions of posts\nare added every day, and each post represents a single item in the table. The\nmobile app requires only recent posts. Any post that is older than 48 hours can\nbe removed.\n\nWhat is the MOST cost-effective way to delete posts that are older than 48\nhours?""]","For each item, add a new attribute of type String that has a timestamp that is set to the blog post creation time. Create a script to find old posts with a table scan and remove posts that are older than 48 hours by using the BatchWriteItem API operation. Schedule a cron job on an Amazon EC2 instance once an hour to start the script.","For each item, add a new attribute of type String that has a timestamp that is set to the blog post creation time. Create a script to find old posts with a table scan and remove posts that are older than 48 hours by using the BatchWriteItem API operation. Place the script in a container image. Schedule an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate that invokes the container every 5 minutes.","For each item, add a new attribute of type Date that has a timestamp that is set to 48 hours after the blog post creation time. Create a global secondary index (GSI) that uses the new attribute as a sort key. Create an AWS Lambda function that references the GSI and removes expired items by using the BatchWriteItem API operation. Schedule the function with an Amazon CloudWatch event every minute.","For each item, add a new attribute of type Number that has a timestamp that is set to 48 hours after the blog post creation time. Configure the DynamoDB table with a TTL that references the new attribute.","# Answer
- **Correct option:** D
- **Reason:** By configuring the DynamoDB table with a Time to Live (TTL) attribute, which automatically deletes items based on their timestamps set as a Number type, the process becomes fully automated. This means that DynamoDB will handle the deletion of old posts without requiring any additional infrastructure, scripts, or scheduled jobs, making it the most cost-effective solution.

# Example / Analogy
- Think of the TTL feature as a “self-cleaning” fridge. Just as a fridge can be set to automatically discard expired food items, TTL allows the DynamoDB table to automatically remove items (old blog posts) that are no longer relevant after a set period (48 hours).

# Common Mistakes / Traps
- A common trap here is to assume that frequent scanning of the table and manually deleting items with scripts (like in options A and B) would be efficient. However, these methods can lead to higher costs and complexity, especially at scale.

# Memory Tip
- Remember: ""TTL is like a self-cleaning service for your DynamoDB table!"""
157,"[""A developer accesses AWS CodeCommit over SSH. The SSH keys configured to access\nAWS CodeCommit are tied to a user with the following permissions:\n\n[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image13.png]\n\nThe developer needs to create/delete branches.\n\nWhich specific IAM permissions need to be added, based on the principle of least\nprivilege?""]","""codecommit:CreateBranch"" ""codecommit:DeleteBranch""","""codecommit:Put*""","""codecommit:Update*""","""codecommit:*""","# Answer
- **Correct option:** A : ""codecommit:CreateBranch"" ""codecommit:DeleteBranch""

- **Reason:** The principle of least privilege dictates that users should only be given the permissions necessary to perform their required tasks. In this case, the developer needs the ability to create and delete branches in AWS CodeCommit, which specifically requires the permissions `codecommit:CreateBranch` and `codecommit:DeleteBranch`. Choosing these specific permissions ensures that the developer has just the access they need without granting excessive permissions.

# Example / Analogy
- Imagine a library where you can borrow books. If you only need access to the fiction section to borrow a couple of specific novels, you don't need keys to access the entire library, including the restricted areas like the archives or maintenance rooms. Similarly, the developer only requires permission to create and delete branches, not all possible actions in CodeCommit.

# Common Mistakes / Traps
- A common mistake is to choose options that provide broader permissions (like ""codecommit:*"") or permissions that are not relevant to the specific task (like ""codecommit:Put*"" or ""codecommit:Update*""). Such choices would violate the principle of least privilege by allowing more access than necessary.

# Memory Tip
- Remember: Grant only the permissions you need—like only taking the keys for the doors you need to unlock."
158,"[""An application that is deployed to Amazon EC2 is using Amazon DynamoDB. The\napplication calls the DynamoDB REST API. Periodically, the application receives\na ProvisionedThroughputExceededException error when the application writes to a\nDynamoDB table.\n\nWhich solutions will mitigate this error MOST cost-effectively? (Choose two.)""]",Modify the application code to perform exponential backoff when the error is received.,Modify the application to use the AWS SDKs for DynamoDB.,Increase the read and write throughput of the DynamoDB table.,Create a DynamoDB Accelerator (DAX) cluster for the DynamoDB table.,"# Answer
- **Correct options:** A, B

- **Reason:** 
  - **Option A** (Modify the application code to perform exponential backoff when the error is received) is a valid solution because exponential backoff helps manage retry attempts effectively by spacing out retries when encountering transient errors like ProvisionedThroughputExceededException. This avoids overwhelming the DynamoDB service with immediate retries, allowing it time to recover. 
  - **Option B** (Modify the application to use the AWS SDKs for DynamoDB) is also correct as these SDKs handle retries and exponential backoff natively, thereby simplifying the application code and reducing the likelihood of encountering the exception. Meanwhile, they provide better optimization and features for integrating with DynamoDB compared to directly using the REST API.

# Example / Analogy
- Think of a restaurant during a busy evening. If the kitchen is overwhelmed and can’t take more orders, instead of continually placing orders immediately (that would lead to chaos), you may wait a moment (exponential backoff) before trying again. Also, if you have a friend (using SDKs) who knows how to navigate the restaurant's system better, they can help place your order without you constantly reordering every minute.

# Common Mistakes / Traps
- A common trap is assuming that simply increasing the write capacity (Option C) is the best fix without considering cost-effectiveness. While it may solve the immediate issue, it can lead to unnecessary costs. Additionally, creating a DAX cluster (Option D) adds complexity and may not directly address write capacity issues.

# Memory Tip
- Use exponential backoff and AWS SDKs to gracefully handle temporary DynamoDB throttling."
159,"[""When a developer tries to run an AWS CodeBuild project, it raises an error\nbecause the length of all environment variables exceeds the limit for the\ncombined maximum of characters.\n\nWhat is the recommended solution?""]","Add the export LC_ALL=""en_US.utf8"" command to the pre_build section to ensure POSIX localization.",Use Amazon Cognito to store key-value pairs for large numbers of environment variables.,Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables.,Use AWS Systems Manager Parameter Store to store large numbers of environment variables.,"# Answer
- **Correct option:** D : Use AWS Systems Manager Parameter Store to store large numbers of environment variables.
- **Reason:** AWS Systems Manager Parameter Store is designed to manage application configuration data and secure information such as passwords and secrets. It allows you to store key-value pairs efficiently, even for large sets of configuration data. By using Parameter Store, you can overcome the character limit imposed by CodeBuild for environment variables.

# Example / Analogy
- Imagine you're a chef in a restaurant (CodeBuild) with a tiny prep table (environment variable limit). Instead of cramming all your ingredients (environment variables) onto that small table, you can store some of them in a pantry (AWS Systems Manager Parameter Store) and take them out as needed. This way, you have more space to work with and can access your ingredients anytime without cluttering your work area.

# Common Mistakes / Traps
- A common mistake is to assume that simply reducing the number of environment variables will solve the issue when the real problem is the total character count. Developers might overlook that using dedicated storage solutions like Parameter Store can provide a more scalable and manageable approach.

# Memory Tip
- **Tip:** ""Use Parameter Store for more room; keep CodeBuild neat and in Bloom!"""
160,"[""A company is expanding the compatibility of its photo-sharing mobile app to\nhundreds of additional devices with unique screen dimensions and resolutions.\nPhotos are stored in Amazon S3 in their original format and resolution. The\ncompany uses an Amazon CloudFront distribution to serve the photos. The app\nincludes the dimension and resolution of the display as GET parameters with\nevery request.\n\nA developer needs to implement a solution that optimizes the photos that are\nserved to each device to reduce load time and increase photo quality.\n\nWhich solution will meet these requirements MOST cost-effectively?""]",Use S3 Batch Operations to invoke an AWS Lambda function to create new variants of the photos with the required dimensions and resolutions. Create a dynamic CloudFront origin that automatically maps the request of each device to the corresponding photo variant.,Use S3 Batch Operations to invoke an AWS Lambda function to create new variants of the photos with the required dimensions and resolutions. Create a Lambda@Edge function to route requests to the corresponding photo variant by using request headers.,Create a Lambda@Edge function that optimizes the photos upon request and returns the photos as a response. Change the CloudFront TTL cache policy to the maximum value possible.,"Create a Lambda@Edge function that optimizes the photos upon request and returns the photos as a response. In the same function, store a copy of the processed photos on Amazon S3 for subsequent requests.","# Answer
- **Correct option:** B

- **Reason:** Option B leverages S3 Batch Operations to efficiently create dimension and resolution variants of photos stored in S3. By using a Lambda@Edge function, it enables dynamic routing based on incoming request headers, optimizing resource utilization by avoiding unnecessary image processing for each request. This approach is cost-effective because it processes images only once and serves them efficiently via CloudFront by routing users to pre-generated optimized images.

# Example / Analogy
- Imagine a coffee shop that provides different drink sizes. Instead of making a fresh drink every time someone orders, the barista pre-makes various sizes and fetches the appropriate one based on the customer's request. This saves time and resources, similar to how using Lambda@Edge for routing requests to pre-processed images minimizes processing time and improves application performance.

# Common Mistakes / Traps
- A common mistake is choosing options that suggest processing images on-the-fly (like in options C and D) without considering the efficiency and high load that this can introduce, especially with many unique requests. This can lead to latency and potentially higher costs due to increased compute usage.

# Memory Tip
- Pre-process, don’t reprocess: Optimize images once and serve based on request details."
161,"[""A company is building an application for stock trading. The application needs\nsub-millisecond latency for processing trade requests. The company uses Amazon\nDynamoDB to store all the trading data that is used to process each trading\nrequest.\n\nA development team performs load testing on the application and finds that the\ndata retrieval time is higher than expected. The development team needs a\nsolution that reduces the data retrieval time with the least possible effort.\n\nWhich solution meets these requirements?""]",Add local secondary indexes (LSIs) for the trading data.,"Store the trading data in Amazon S3, and use S3 Transfer Acceleration.",Add retries with exponential backoff for DynamoDB queries.,Use DynamoDB Accelerator (DAX) to cache the trading data.,"# Answer
- **Correct option:** D: Use DynamoDB Accelerator (DAX) to cache the trading data.
- **Reason:** DAX is a fully managed, in-memory caching service for DynamoDB that provides microsecond response times for read-heavy and bursty workloads. By caching the data, DAX significantly reduces the data retrieval time, which is crucial for applications that require sub-millisecond latency like stock trading.

# Example / Analogy
- **Real-life example or memorable comparison:** Think of DAX like a speedboat in a busy harbor. While a regular boat may take longer to navigate through many boats (database read operations), a speedboat (DAX) easily zips through, getting you where you need to go in a fraction of the time. In this analogy, DAX reduces the time it takes to retrieve trading data, ensuring prompt processing of trade requests.

# Common Mistakes / Traps
- A common mistake is to choose local secondary indexes (option A) under the assumption that they will optimize query performance. However, they primarily help in querying data differently rather than improving retrieval speed significantly. Remember that they still rely on the base table data and do not provide caching benefits like DAX.

# Memory Tip
- **Tip:** Remember that DAX is like a turbocharger for DynamoDB, boosting retrieval speeds to microsecond levels!"
162,"[""A developer is working on a Python application that runs on Amazon EC2\ninstances. The developer wants to enable tracing of application requests to\ndebug performance issues in the code.\n\nWhich combination of actions should the developer take to achieve this goal?\n(Choose two.)""]",Install the Amazon CloudWatch agent on the EC2 instances.,Install the AWS X-Ray daemon on the EC2 instances.,Configure the application to write JSON-formatted logs to /var/log/cloudwatch.,Configure the application to write trace data to /var/log/xray.,"# Answer
- **Correct options:** B, D

- **Reason:** AWS X-Ray is specifically designed for tracing requests in your applications to analyze and debug performance issues. By installing the AWS X-Ray daemon on your EC2 instances (Option B) and configuring your application to write trace data to the correct log file (Option D), you enable detailed request tracing and insight into how requests are processed, which is essential for identifying performance bottlenecks.

# Example / Analogy
- Think of AWS X-Ray like a GPS tracking system for your application's requests. Just as GPS allows you to see the exact route you took and where delays occurred during your journey, AWS X-Ray provides visibility into the paths your requests take through your application, helping you pinpoint where the slowdowns are happening.

# Common Mistakes / Traps
- A common mistake is to confuse AWS X-Ray with CloudWatch. Some may think that installing the CloudWatch agent (Option A) is enough for tracing requests, but CloudWatch primarily handles monitoring and logging, not request tracing.

# Memory Tip
- Remember: For tracing requests, ""X-Ray tracks; CloudWatch stores."""
163,"[""A company has an application that runs as a series of AWS Lambda functions. Each\nLambda function receives data from an Amazon Simple Notification Service (Amazon\nSNS) topic and writes the data to an Amazon Aurora DB instance.\n\nTo comply with an information security policy, the company must ensure that the\nLambda functions all use a single securely encrypted database connection string\nto access Aurora.\n\nWhich solution will meet these requirements?""]",Use IAM database authentication for Aurora to enable secure database connections for all the Lambda functions.,Store the credentials and read the credentials from an encrypted Amazon RDS DB instance.,Store the credentials in AWS Systems Manager Parameter Store as a secure string parameter.,Use Lambda environment variables with a shared AWS Key Management Service (AWS KMS) key for encryption.,"# Answer
- **Correct option:** C : Store the credentials in AWS Systems Manager Parameter Store as a secure string parameter.
- **Reason:** This option allows you to securely store and manage database credentials centrally. The AWS Systems Manager Parameter Store supports encryption using AWS Key Management Service (KMS), ensuring that sensitive information is kept secure and is accessible by your Lambda functions.

# Example / Analogy
- Think of AWS Systems Manager Parameter Store as a locked filing cabinet in a secure office where important documents (like sensitive credentials) are stored safely and can be accessed by employees (Lambda functions) who have the key (appropriate IAM permissions) to get the data they need.

# Common Mistakes / Traps
- A common mistake is choosing options that imply storing credentials in less secure ways (like plain environment variables or an unencrypted database) or those that don’t consolidate credential management effectively (like IAM authentication that might not cover all use cases).

# Memory Tip
- Remember: ""Use Parameter Store for secure secrets—it's the safe place for your Lambda connects!"""
164,"[""A developer is troubleshooting an Amazon API Gateway API. Clients are receiving\nHTTP 400 response errors when the clients try to access an endpoint of the API.\n\nHow can the developer determine the cause of these errors?""]",Create an Amazon Kinesis Data Firehose delivery stream to receive API call logs from API Gateway. Configure Amazon CloudWatch Logs as the delivery stream’s destination.,Turn on AWS CloudTrail Insights and create a trail. Specify the Amazon Resource Name (ARN) of the trail for the stage of the API.,Turn on AWS X-Ray for the API stage. Create an Amazon CloudWatch Logs log group. Specify the Amazon Resource Name (ARN) of the log group for the API stage.,Turn on execution logging and access logging in Amazon CloudWatch Logs for the API stage. Create a CloudWatch Logs log group. Specify the Amazon Resource Name (ARN) of the log group for the API stage.,"# Answer
- **Correct option:** D
- **Reason:** Turning on execution logging and access logging for the API stage in Amazon API Gateway allows you to capture detailed logs of requests and responses, including the reasons for errors like HTTP 400 responses. These logs can be sent to an Amazon CloudWatch Logs log group, making it easier to monitor and troubleshoot the API.

# Example / Analogy
- Think of it like having a security camera (CloudWatch Logs) installed in a store (API Gateway). When a customer (client) tries to enter but is rejected (HTTP 400 error), the camera (logs) captures the event, showing what went wrong—be it a problem with their ID (request validation) or something else—allowing you to investigate the issue later.

# Common Mistakes / Traps
- A common mistake is to assume that simply viewing the API Gateway console will provide enough detail for troubleshooting. However, without detailed logs, understanding the specific reasons for errors can be challenging. Some might confuse logging with monitoring services like AWS CloudTrail, which does not provide the same level of detail for request logging.

# Memory Tip
- ""Logs reveal the story - enable them to know the cause!"""
165,"[""A company developed an API application on AWS by using Amazon CloudFront, Amazon\nAPI Gateway, and AWS Lambda. The API has a minimum of four requests every\nsecond. A developer notices that many API users run the same query by using the\nPOST method. The developer wants to cache the POST request to optimize the API\nresources.\n\nWhich solution will meet these requirements?""]",Configure the CloudFront cache. Update the application to return cached content based upon the default request headers.,Override the cache method in the selected stage of API Gateway. Select the POST method.,Save the latest request response in Lambda /tmp directory. Update the Lambda function to check the /tmp directory.,Save the latest request in AWS Systems Manager Parameter Store. Modify the Lambda function to take the latest request response from Parameter Store.,"# Answer
- **Correct option:** B: Override the cache method in the selected stage of API Gateway. Select the POST method.

- **Reason:** API Gateway allows for caching responses to certain HTTP methods, including POST, which is essential in this scenario. By overriding the cache settings and enabling caching for the specific POST method, repeated requests for the same query can be served from the cache, optimizing resource usage and reducing latency. This approach effectively allows the developer to cache responses based on individual requests while maintaining API integrity.

# Example / Analogy
- Imagine a sandwich shop where every customer orders the same popular sandwich. If the shop maintains a few ready-made sandwiches based on that popular order (instead of making one from scratch for each customer), it speeds up service. Similarly, caching the POST request responses allows the API to quickly serve repeated requests without having to process them each time.

# Common Mistakes / Traps
- A common mistake is to think that caching can only apply to GET requests. While GET requests are typically cached, POST requests can also be cached in API Gateway with the right configuration, which is often overlooked.

# Memory Tip
- Cache repeated requests for efficiency—just like fast food restaurants prepare popular menu items in advance!"
166,"[""A company is using AWS CloudFormation to deploy a two-tier application. The\napplication will use Amazon RDS as its backend database. The company wants a\nsolution that will randomly generate the database password during deployment.\nThe solution also must automatically rotate the database password without\nrequiring changes to the application.\n\nWhat is the MOST operationally efficient solution that meets these requirements?""]",Use an AWS Lambda function as a CloudFormation custom resource to generate and rotate the password.,Use an AWS Systems Manager Parameter Store resource with the SecureString data type to generate and rotate the password.,Use a cron daemon on the application’s host to generate and rotate the password.,Use an AWS Secrets Manager resource to generate and rotate the password.,"# Answer
- **Correct option:** D : Use an AWS Secrets Manager resource to generate and rotate the password.

- **Reason:** AWS Secrets Manager is specifically designed for managing sensitive information like database credentials. It can automatically perform password generation and rotation seamlessly, integrating directly with Amazon RDS without requiring any manual intervention or application code changes. This aligns perfectly with the company's requirements for operational efficiency and automated password management.

# Example / Analogy
- Think of AWS Secrets Manager like a safe where you can store your valuables (in this case, the database password). Not only does it securely keep your valuables, but it can also change the lock (rotate the password) automatically every month without you needing to remember to do it or needing to access it.

# Common Mistakes / Traps
- A common mistake is to consider AWS Lambda or Systems Manager Parameter Store as alternatives, but these solutions require custom scripting or manual updates that can introduce complexity and operational overhead compared to the fully managed service provided by Secrets Manager.

# Memory Tip
- ""Secrets Manager is the key to automated password management in AWS."""
167,"[""A company is building a microservices application that consists of many AWS\nLambda functions. The development team wants to use AWS Serverless Application\nModel (AWS SAM) templates to automatically test the Lambda functions. The\ndevelopment team plans to test a small percentage of traffic that is directed to\nnew updates before the team commits to a full deployment of the application.\n\nWhich combination of steps will meet these requirements in the MOST\noperationally efficient way? (Choose two.)""]",Use AWS SAM CLI commands in AWS CodeDeploy to invoke the Lambda functions to test the deployment.,Declare the EventInvokeConfig on the Lambda functions in the AWS SAM templates with OnSuccess and OnFailure configurations.,Enable gradual deployments through AWS SAM templates.,Set the deployment preference type to Canary10Percent30Minutes. Use hooks to test the deployment.,"# Answer
- **Correct options:** C and D
- **Reason:** 
  - Option C is correct because enabling gradual deployments through AWS SAM templates allows for smooth testing of new updates without affecting the entire user base. It helps in minimizing risk associated with deploying untested updates. 
  - Option D is also correct as setting the deployment preference to `Canary10Percent30Minutes` aligns with the requirement to test a small percentage (10%) of traffic directed to new updates over a defined time frame (30 minutes) before proceeding with the full deployment. This ensures operational efficiency by catching potential issues early.

# Example / Analogy
- Think of gradual deployments like a restaurant that introduces a new dish to a few customers before making it part of the full menu. If the feedback is positive, they can confidently roll it out to everyone; if not, they can adjust the recipe without impacting all diners.

# Common Mistakes / Traps
- A common mistake is confusing different deployment strategies or tools. For instance, choosing options that involve invoking functions directly or dealing with success/failure configurations can mislead one to think they are handling traffic and testing effectively, which is not the case here.

# Memory Tip
- **""Gradual testing = Safe rollout""**  – remember that gradual deployments allow for risk management in updates."
168,"[""A developer has been asked to create an AWS Lambda function that is invoked any\ntime updates are made to items in an Amazon DynamoDB table. The function has\nbeen created, and appropriate permissions have been added to the Lambda\nexecution role. Amazon DynamoDB streams have been enabled for the table, but the\nfunction is still not being invoked.\n\nWhich option would enable DynamoDB table updates to invoke the Lambda function?""]",Change the StreamViewType parameter value to NEW_AND_OLD_IMAGES for the DynamoDB table.,Configure event source mapping for the Lambda function.,Map an Amazon Simple Notification Service (Amazon SNS) topic to the DynamoDB streams.,Increase the maximum runtime (timeout) setting of the Lambda function.,"# Answer
- **Correct option:** B : Configure event source mapping for the Lambda function.
- **Reason:** In order for the Lambda function to be invoked by updates in the DynamoDB table, there must be an event source mapping that connects the DynamoDB stream to the Lambda function. This mapping tells AWS Lambda to listen to the stream and invoke the function whenever there are changes in the table.

# Example / Analogy
- Think of DynamoDB streams as a conveyor belt where updates to items are placed. The Lambda function is like a worker standing next to the conveyor belt, ready to pick up and process each item that passes. If the worker (Lambda) is not set up to be notified when new items arrive (event source mapping), they won't know to act on them, no matter how many updates are on the conveyor belt.

# Common Mistakes / Traps
- A common mistake is to assume that simply enabling DynamoDB streams is sufficient for invoking the Lambda function. However, without the proper event source mapping, the function will not be triggered.

# Memory Tip
- Remember: ""Event source mapping connects the stream to the function for triggers."""
169,"[""A developer needs to deploy an application running on AWS Fargate using Amazon\nECS. The application has environment variables that must be passed to a\ncontainer for the application to initialize.\n\nHow should the environment variables be passed to the container?""]",Define an array that includes the environment variables under the environment parameter within the service definition.,Define an array that includes the environment variables under the environment parameter within the task definition.,Define an array that includes the environment variables under the entryPoint parameter within the task definition.,Define an array that includes the environment variables under the entryPoint parameter within the service definition.,"# Answer
- **Correct option:** B: Define an array that includes the environment variables under the environment parameter within the task definition.

- **Reason:** The environment variables needed by a container can be specified in the task definition in Amazon ECS. This allows the Fargate service to retrieve these variables when the containers are started. The task definition is a central configuration for the application’s environment, ensuring that settings like environment variables are consistently applied across all instances of the service.

# Example / Analogy
- Think of the task definition as a recipe for baking a cake. Just like the recipe lists all the ingredients (flour, sugar, eggs) required to bake the cake, the task definition lists all the necessary environment variables (like API keys, database URLs) the application needs to function properly.

# Common Mistakes / Traps
- A common mistake is to confuse the service definition with the task definition. The service definition handles scaling and managing tasks, but the actual configuration of the environment variables must be done in the task definition.

# Memory Tip
- **Tip:** Remember that task definitions are where the ""recipe"" for your container configuration, including environment variables, is defined."
170,"[""A development team maintains a web application by using a single AWS RDS,\ntemplate. The template defines web servers and an Amazon RDS database. The team\nuses the CloudFormation template to deploy the CloudFormation stack to different\nenvironments.\n\nDuring a recent application deployment, a developer caused the primary\ndevelopment database to be dropped and recreated. The result of this incident\nwas a loss of data. The team needs to avoid accidental database deletion in the\nfuture.\n\nWhich solutions will meet these requirements? (Choose two.)""]",Add a CloudFormation DeletionPolicy attribute with the Retain value to the database resource.,Update the CloudFormation stack policy to prevent updates to the database.,Modify the database to use a Multi-AZ deployment.,Create a CloudFormation stack set for the web application and database deployments.,"# Answer
- **Correct options:** A and B
- **Reason:** 
  - **Option A:** Adding a CloudFormation DeletionPolicy with the Retain value instructs AWS CloudFormation to retain the database resource when the stack is deleted, protecting it from accidental deletion. This ensures that the database remains intact, preserving the data.
  - **Option B:** Updating the CloudFormation stack policy to prevent updates to the database can help enforce rules that discourage or block any potentially destructive operations on the database, which decreases the likelihood of accidental deletions or modifications.

# Example / Analogy
- Think of the CloudFormation DeletionPolicy as a safety lock on a valuable item. Just like you would use a lock to ensure that no one can accidentally take or destroy something precious, setting the DeletionPolicy to Retain locks your database from being removed unintentionally. A stack policy acts like a security guard, ensuring that only authorized changes can be made to your valuable property (the database), thus preventing potential mishaps.

# Common Mistakes / Traps
- A common trap is assuming that simply using Multi-AZ or a stack set will protect against accidental deletions; however, these options do not address the specific issue of preventing deletion or modification directly. Multi-AZ primarily provides high availability and fault tolerance rather than a safeguard against accidental data loss.

# Memory Tip
- **Lock your resources (use DeletionPolicy) and guard your changes (set stack policies)!**"
171,"[""A developer is storing sensitive data generated by an application in Amazon S3.\nThe developer wants to encrypt the data at rest. A company policy requires an\naudit trail of when the AWS Key Management Service (AWS KMS) key was used and by\nwhom.\n\nWhich encryption option will meet these requirements?""]",Server-side encryption with Amazon S3 managed keys (SSE-S3),Server-side encryption with AWS KMS managed keys (SSE-KMS),Server-side encryption with customer-provided keys (SSE-C),Server-side encryption with self-managed keys,"# Answer
- **Correct option:** B : Server-side encryption with AWS KMS managed keys (SSE-KMS)
- **Reason:** SSE-KMS provides an audit trail for key usage through AWS CloudTrail, allowing you to track when the KMS key was used and who accessed it. This is important given the requirement for an audit trail as part of the company policy. Additionally, it allows for fine-grained access control using IAM policies.

# Example / Analogy
- Imagine you have a safe with a special key that can only be accessed by specific trusted individuals, and every time someone uses that key to open the safe, there's a logbook that records who used it and when. SSE-KMS functions similarly by securing data with a key that can be monitored and controlled.

# Common Mistakes / Traps
- A common mistake is confusing SSE-KMS with SSE-S3. While SSE-S3 is easier to use, it doesn't provide the required audit trail as KMS does. Another trap is thinking that customer-provided keys (SSE-C) offer the same level of management and auditing, which they do not.

# Memory Tip
- **Remember:** ""KMS keeps the keys and the logs!"""
172,"[""A company has an ecommerce application. To track product reviews, the company’s\ndevelopment team uses an Amazon DynamoDB table.\n\nEvery record includes the following:\n\n• A Review ID, a 16-digit universally unique identifier (UUID)\n• A Product ID and User ID, 16-digit UUIDs that reference other tables\n• A Product Rating on a scale of 1-5\n• An optional comment from the user\n\nThe table partition key is the Review ID. The most performed query against the\ntable is to find the 10 reviews with the highest rating for a given product.\n\nWhich index will provide the FASTEST response for this query?""]",A global secondary index (GSI) with Product ID as the partition key and Product Rating as the sort key,A global secondary index (GSI) with Product ID as the partition key and Review ID as the sort key,A local secondary index (LSI) with Product ID as the partition key and Product Rating as the sort key,A local secondary index (LSI) with Review ID as the partition key and Product ID as the sort key,"# Answer
- **Correct option:** A : A global secondary index (GSI) with Product ID as the partition key and Product Rating as the sort key

- **Reason:** A Global Secondary Index (GSI) allows you to query the table using alternative keys, and it can have a different partition and sort key than the primary table. In this case, by using Product ID as the partition key and Product Rating as the sort key, the query can efficiently fetch the top 10 reviews for a specific product, sorted by the highest ratings. This enables very fast lookups when searching for reviews of a particular product based on their ratings.

# Example / Analogy
- Imagine you are searching for the best-reviewed restaurants in a city. If you have a list of all restaurants (the main table), and you create a sorted list specifically for 'cuisine type' (the GSI), it makes it easy to find the top-rated Italian restaurants without having to sift through every single restaurant. This is similar to how GSIs help you retrieve the top-rated product reviews without scanning the entire table.

# Common Mistakes / Traps
- A common trap is to confuse the use of Local Secondary Indexes (LSI) with GSIs. LSI shares the same partition key as the base table but allows a different sort key. Hence, it cannot provide an optimal solution for this query because it cannot partition by Product ID, limiting the query efficiency. Always remember that LSIs are used when you need the same partition key but a different sort key.

# Memory Tip
- **Key takeaway:** Use GSIs for flexible and efficient queries across different partition keys!"
173,"[""A company needs to distribute firmware updates to its customers around the\nworld.\n\nWhich service will allow easy and secure control of the access to the downloads\nat the lowest cost?""]",Use Amazon CloudFront with signed URLs for Amazon S3.,Create a dedicated Amazon CloudFront Distribution for each customer.,Use Amazon CloudFront with AWS Lambda@Edge.,Use Amazon API Gateway and AWS Lambda to control access to an S3 bucket.,"# Answer
- **Correct option:** A : Use Amazon CloudFront with signed URLs for Amazon S3.
- **Reason:** Amazon CloudFront is a content delivery network (CDN) that helps speed up the delivery of your files globally. By using signed URLs, you can control access to the firmware updates stored in Amazon S3, ensuring only authorized users can download them, all while keeping costs low.

# Example / Analogy
- Real-life example or memorable comparison: Think of Amazon CloudFront like a worldwide delivery service that only delivers packages (firmware updates) to those who have a special key (signed URLs). Just like you might give a guest a key to your house to access specific areas, signed URLs give customers access to specific downloads securely.

# Common Mistakes / Traps
- Common trap points, if any: A common mistake is choosing overly complex solutions like creating dedicated CloudFront distributions for each customer or using API Gateway and Lambda, which can increase costs and complexity unnecessarily.

# Memory Tip
- 1 quick tip: ""CloudFront + Signed URLs = Secure, low-cost delivery!"""
174,"[""A developer is testing an application that invokes an AWS Lambda function\nasynchronously. During the testing phase, the Lambda function fails to process\nafter two retries.\n\nHow can the developer troubleshoot the failure?""]",Configure AWS CloudTrail logging to investigate the invocation failures.,Configure Dead Letter Queues by sending events to Amazon SQS for investigation.,Configure Amazon Simple Workflow Service to process any direct unprocessed events.,Configure AWS Config to process any direct unprocessed events.,"# Answer
- **Correct option:** B : Configure Dead Letter Queues by sending events to Amazon SQS for investigation.
- **Reason:** Configuring Dead Letter Queues (DLQs) allows you to capture and store events that fail to process after a configured number of retries. By directing the failed events to an Amazon SQS queue or an SNS topic, developers can investigate the specific cause of the failure, review the messages, and take corrective actions accordingly.

# Example / Analogy
- Think of a mail delivery service. If the postman can't deliver a package after several attempts, rather than discarding it, they place it in a special holding area (the DLQ), where it can be examined later to find out what went wrong (like an incorrect address).

# Common Mistakes / Traps
- A common trap in this question is selecting option A (AWS CloudTrail) thinking it will provide insight into failures, but CloudTrail primarily logs API calls and not application-level failures directly. Similarly, options C and D suggest using services that do not directly address the handling of failed Lambda invocations.

# Memory Tip
- Remember: ""Dead Letter Queues hold the lost; retrieve and resolve what's been tossed!"""
175,"[""A developer is creating a mobile application that will not require users to log\nin.\n\nWhat is the MOST efficient method to grant users access to AWS resources?""]",Use an identity provider to securely authenticate with the application.,Create an AWS Lambda function to create an IAM user when a user accesses the application.,Create credentials using AWS KMS and apply these credentials to users when using the application.,Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources.,"# Answer
- **Correct option:** D
- **Reason:** Using Amazon Cognito to associate unauthenticated users with an IAM role allows you to provide limited and controlled access to AWS resources without requiring user login. This is efficient because it handles user identity and access management seamlessly, allowing the application to function without sign-ins while still maintaining security.

# Example / Analogy
- Think of Amazon Cognito like a guest access system at a club. Guests (unauthenticated users) can enter without a membership (login) but only have access to certain areas (limited AWS resources). The club ensures that while guests are inside, they can't access restricted VIP areas, maintaining a level of security.

# Common Mistakes / Traps
- A common trap is to assume that all users need to be authenticated through an identity provider (option A) to access AWS resources. However, for applications that do not require user logins, using Cognito for unauthenticated access is often the best approach.

# Memory Tip
- Remember: **""Cognito = Guest Access with Limits.""**"
176,"[""A company is migrating its PostgreSQL database into the AWS Cloud. The company\nwants to use a database that will secure and regularly rotate database\ncredentials. The company wants a solution that does not require additional\nprogramming overhead.\n\nWhich solution will meet these requirements?""]",Use Amazon Aurora PostgreSQL for the database. Store the database credentials in AWS Systems Manager Parameter Store. Turn on rotation.,Use Amazon Aurora PostgreSQL for the database. Store the database credentials in AWS Secrets Manager. Turn on rotation.,Use Amazon DynamoDB for the database. Store the database credentials in AWS Systems Manager Parameter Store. Turn on rotation.,Use Amazon DynamoDB for the database. Store the database credentials in AWS Secrets Manager. Turn on rotation.,"# Answer
- **Correct option:** B

- **Reason:** Amazon Aurora PostgreSQL is a managed database service that supports features like automatic backups and read replicas. By using AWS Secrets Manager to store database credentials, the company can benefit from secure credential storage and automatic rotation of those credentials without requiring any additional programming overhead. Secrets Manager integrates easily with other AWS services, including Aurora, making it a suitable choice for managing secrets.

# Example / Analogy
- Think of AWS Secrets Manager as a highly secure vault that automatically updates the keys (database credentials) based on a schedule. Just as you wouldn’t want to keep your house keys in an easy-to-find place, using Secrets Manager keeps your database credentials safe and ensures they are rotated regularly like changing the locks periodically to enhance security.

# Common Mistakes / Traps
- A common mistake here is to overlook the importance of using AWS Secrets Manager, as some may think that just storing credentials in AWS Systems Manager Parameter Store is sufficient. While Parameter Store can store secrets, it lacks the automatic rotation feature that Secrets Manager provides and requires additional coding to rotate secrets.

# Memory Tip
- **Remember:** ""Secrets Manager is a secure vault for credentials with auto-rotation – use it for database security!"""
177,"[""A company has developed a new serverless application using AWS Lambda functions\nthat will be deployed using the AWS Serverless Application Model (AWS SAM) CLI.\n\nWhich step should the developer complete prior to deploying the application?""]",Compress the application to a .zip file and upload it into AWS Lambda.,Test the new AWS Lambda function by first tracing it in AWS X-Ray.,Bundle the serverless application using a SAM package.,Create the application environment using the eb create my-env command.,"# Answer
- **Correct option:** C : Bundle the serverless application using a SAM package.
- **Reason:** Before deploying an application using the AWS Serverless Application Model (AWS SAM) CLI, the developer must package the application with the `sam package` command. This command bundles the source code and dependencies, and prepares it for deployment to AWS by creating an S3 bucket where the packaged files will be stored.

# Example / Analogy
- Real-life example or memorable comparison: Think of this step like preparing a suitcase for a trip (packaging) before you head to the airport (deployment). You wouldn't just jump on a plane without ensuring that all your items are packed neatly and securely!

# Common Mistakes / Traps
- Common trap points, if any: Many developers might mistakenly believe that just compressing the application into a .zip file (Option A) is sufficient or that testing the Lambda function first (Option B) should be prioritized. However, the packaging step is essential and must come before deployment. 

# Memory Tip
- Always remember: ""Package first, deploy later!"""
178,"[""A company wants to automate part of its deployment process. A developer needs to\nautomate the process of checking for and deleting unused resources that\nsupported previously deployed stacks but that are no longer used.\n\nThe company has a central application that uses the AWS Cloud Development Kit\n(AWS CDK) to manage all deployment stacks. The stacks are spread out across\nmultiple accounts. The developer’s solution must integrate as seamlessly as\npossible within the current deployment process.\n\nWhich solution will meet these requirements with the LEAST amount of\nconfiguration?""]","In the central AWS CDK application, write a handler function in the code that uses AWS SDK calls to check for and delete unused resources. Create an AWS CloudFormation template from a JSON file. Use the template to attach the function code to an AWS Lambda function and to invoke the Lambda function when the deployment stack runs.","In the central AWS CDK application, write a handler function in the code that uses AWS SDK calls to check for and delete unused resources. Create an AWS CDK custom resource. Use the custom resource to attach the function code to an AWS Lambda function and to invoke the Lambda function when the deployment stack runs.","In the central AWS CDK, write a handler function in the code that uses AWS SDK calls to check for and delete unused resources. Create an API in AWS Amplify. Use the API to attach the function code to an AWS Lambda function and to invoke the Lambda function when the deployment stack runs.","In the AWS Lambda console, write a handler function in the code that uses AWS SDK calls to check for and delete unused resources. Create an AWS CDK custom resource. Use the custom resource to import the Lambda function into the stack and to invoke the Lambda function when the deployment stack runs.","# Answer
- **Correct option:** B

- **Reason:** Option B is the best choice because it utilizes AWS CDK to create a custom resource, which allows the integration of the Lambda function into the deployment process seamlessly. This approach leverages existing CDK infrastructure and minimizes configuration overhead. The custom resource can trigger the Lambda function during the CloudFormation stack lifecycle, ensuring that unused resources are checked and deleted as part of the deployment.

# Example / Analogy
- Think of the AWS CDK custom resource as a ""housekeeper"" that runs periodically whenever a new ""guest"" (deployment stack) comes to stay (is deployed). The housekeeper checks if there are any rooms (resources) that are no longer needed and takes care of cleaning them up. This process fits neatly within the infrastructure management workflow.

# Common Mistakes / Traps
- A common mistake is to overcomplicate the solution by introducing unnecessary components, such as creating an API (as in option C), which adds complexity without benefiting the automation process. Also, one might overlook the importance of aligning with the CDK framework for more streamlined operations.

# Memory Tip
- ""Use custom resources to keep operations clean and integrated."""
179,"[""A company built a new application in the AWS Cloud. The company automated the\nbootstrapping of new resources with an Auto Scaling group by using AWS\nCloudFormation templates. The bootstrap scripts contain sensitive data.\n\nThe company needs a solution that is integrated with CloudFormation to manage\nthe sensitive data in the bootstrap scripts.\n\nWhich solution will meet these requirements in the MOST secure way?""]",Put the sensitive data into a CloudFormation parameter. Encrypt the CloudFormation templates by using an AWS Key Management Service (AWS KMS) key.,Put the sensitive data into an Amazon S3 bucket. Update the CloudFormation templates to download the object from Amazon S3 during bootstrap.,Put the sensitive data into AWS Systems Manager Parameter Store as a secure string parameter. Update the CloudFormation templates to use dynamic references to specify template values.,Put the sensitive data into Amazon Elastic File System (Amazon EFS). Enforce EFS encryption after file system creation. Update the CloudFormation templates to retrieve data from Amazon EFS.,"# Answer
- **Correct option:** C  
- **Reason:** This option uses AWS Systems Manager Parameter Store to securely manage sensitive data as a secure string parameter. It integrates seamlessly with CloudFormation by allowing the use of dynamic references, ensuring that sensitive data is not hard-coded in templates or exposed during stack operations.

# Example / Analogy
- Think of AWS Systems Manager Parameter Store like a secure vault for your sensitive information. Just as you wouldn’t leave a combination to a safe lying around in your house, placing sensitive data in Parameter Store keeps it secure and accessible only to those who need it when the application is bootstrapped.

# Common Mistakes / Traps
- A common mistake is to underestimate the importance of security when managing sensitive data. Options A and B propose methods that can expose sensitive information either directly in templates or during retrieval. Option D, while using EFS, adds unnecessary complexity and isn't as efficient as using a dedicated service for secrets management.

# Memory Tip
- ""Secure strings in Parameter Store: the vault for your app secrets!"""
180,"[""A company needs to set up secure database credentials for all its AWS Cloud\nresources. The company’s resources include Amazon RDS DB instances, Amazon\nDocumentDB clusters, and Amazon Aurora DB instances. The company’s security\npolicy mandates that database credentials be encrypted at rest and rotated at a\nregular interval.\n\nWhich solution will meet these requirements MOST securely?""]","Set up IAM database authentication for token-based access. Generate user tokens to provide centralized access to RDS DB instances, Amazon DocumentDB clusters, and Aurora DB instances.",Create parameters for the database credentials in AWS Systems Manager Parameter Store. Set the Type parameter to SecureString. Set up automatic rotation on the parameters.,Store the database access credentials as an encrypted Amazon S3 object in an S3 bucket. Block all public access on the S3 bucket. Use S3 server-side encryption to set up automatic rotation on the encryption key.,Create an AWS Lambda function by using the SecretsManagerRotationTemplate template in the AWS Secrets Manager console. Create secrets for the database credentials in Secrets Manager. Set up secrets rotation on a schedule.,"# Answer
- **Correct option:** D 
- **Reason:** AWS Secrets Manager is designed specifically for securely storing and managing sensitive information such as database credentials. It provides built-in encryption for secrets at rest, supports automatic rotation of those secrets, and can be easily integrated with various AWS services. This meets the company's requirement to encrypt credentials and rotate them regularly, ensuring both security and compliance with the security policy.

# Example / Analogy
- Think of AWS Secrets Manager as a secure vault for your most critical keys (database credentials). Just like how you might store important documents in a safety deposit box that automatically alerts you to check or change the contents every few months, Secrets Manager keeps your sensitive information safe and reminds you to rotate them regularly.

# Common Mistakes / Traps
- A common mistake is to overlook the specific need for automatic credential rotation. While options like B (Systems Manager) may allow for secure storage, they do not inherently provide the advanced features of automated rotation or fine-grained access control, which are essential for dynamic environments.

# Memory Tip
- ""Secrets Manager = Secure vault + Automatic rotation."""
181,"[""A developer has created an AWS Lambda function that makes queries to an Amazon\nAurora MySQL DB instance. When the developer performs a test, the DB instance\nshows an error for too many connections.\n\nWhich solution will meet these requirements with the LEAST operational effort?""]",Create a read replica for the DB instance. Query the replica DB instance instead of the primary DB instance.,Migrate the data to an Amazon DynamoDB database.,Configure the Amazon Aurora MySQL DB instance for Multi-AZ deployment.,Create a proxy in Amazon RDS Proxy. Query the proxy instead of the DB instance.,"# Answer
- **Correct option:** D : Create a proxy in Amazon RDS Proxy. Query the proxy instead of the DB instance.

- **Reason:** Using Amazon RDS Proxy helps to manage the number of active connections to the Aurora MySQL DB instance efficiently. It allows for connection pooling, which reduces the overhead of opening and closing connections frequently. This significantly lowers the chances of hitting the 'too many connections' error by managing the database connections more effectively and allowing multiple clients to share a smaller number of active connections.

# Example / Analogy
- Think of Amazon RDS Proxy as a traffic controller at a busy intersection. Just as a traffic controller can efficiently manage the flow of vehicles to prevent congestion, RDS Proxy manages database connections, ensuring that the database is not overwhelmed with too many simultaneous connection requests. By utilizing the proxy, multiple requests can be served without needing a distinct connection for each one, optimizing performance and resource usage.

# Common Mistakes / Traps
- A common mistake is to assume that merely creating a read replica (Option A) will solve the problem of too many connections. While read replicas can help with read traffic, they do not address the connection limits of the primary DB instance. Additionally, options like migrating to DynamoDB (Option B) or configuring Multi-AZ deployment (Option C) may not resolve the connection issue directly, as they pertain to data availability and scalability but don’t necessarily manage connection counts.

# Memory Tip
- Use RDS Proxy to streamline and manage database connections effectively, just like having a traffic controller to regulate vehicle flow."
182,"[""A developer is creating a new REST API by using Amazon API Gateway and AWS\nLambda. The development team tests the API and validates responses for the known\nuse cases before deploying the API to the production environment.\n\nThe developer wants to make the REST API available for testing by using API\nGateway locally.\n\nWhich AWS Serverless Application Model Command Line Interface (AWS SAM CLI)\nsubcommand will meet these requirements?""]",Sam local invoke,Sam local generate-event,Sam local start-lambda,Sam local start-api,"# Answer
- **Correct option:** D : Sam local start-api
- **Reason:** The `sam local start-api` command is specifically designed to run API Gateway locally on your machine, allowing you to test your REST API before deployment. This command helps you to invoke your AWS Lambda functions through the API Gateway locally, thereby simulating the actual API environment for testing purposes.

# Example / Analogy
- Real-life example or memorable comparison: Think of `sam local start-api` as a chef testing out a new recipe in their kitchen before serving it at a restaurant. The chef can adjust ingredients and methods in a safe environment, ensuring the dish is perfect for customers, much like the developer testing the API locally before production.

# Common Mistakes / Traps
- Common trap points, if any: A common mistake is confusing `sam local start-api` with `sam local invoke`, which only invokes a specific Lambda function directly and does not simulate an API endpoint. Developers might overlook that they are wanting to test an API endpoint rather than a single Lambda function execution.

# Memory Tip
- To remember `sam local start-api`, think of ""starting your API engine locally"" before hitting the live road."
183,"[""A company has a serverless application on AWS that uses a fleet of AWS Lambda\nfunctions that have aliases. The company regularly publishes new Lambda function\nby using an in-house deployment solution. The company wants to improve the\nrelease process and to use traffic shifting. A newly published function version\nshould initially make available only to a fixed percentage of production users.\n\nWhich solution will meet these requirements?""]",Configure routing on the alias of the new function by using a weighted alias.,Configure a canary deployment type for Lambda.,Configure routing on the new versions by using environment variables.,Configure a linear deployment type for Lambda.,"# Answer
- **Correct option:** A : Configure routing on the alias of the new function by using a weighted alias.

- **Reason:** Using weighted aliases allows you to route a specified percentage of traffic to different function versions. This means you can progressively expose the new version of the Lambda function to an increasing percentage of users, achieving traffic shifting without disrupting the entire user base. 

# Example / Analogy
- Think of it like a restaurant trying a new dish. Instead of replacing the existing menu item, the restaurant offers a tasting option where a small percentage of customers get to try the new dish. If it's well received, they can slowly introduce it to more customers by increasing the tasting options until it’s part of the main menu.

# Common Mistakes / Traps
- A common mistake is to consider canary or linear deployment types, which are specific to AWS deployment features but do not apply directly to AWS Lambda when using aliases for traffic shifting. These types can be misinterpreted as being suitable for handling traffic shifting but are not implemented through aliases.

# Memory Tip
- **Weighted aliases = Traffic control for combos!** Utilizes gradual exposure to new changes."
184,"[""A company has an application that stores data in Amazon RDS instances. The\napplication periodically experiences surges of high traffic that cause\nperformance problems. During periods of peak traffic, a developer notices a\nreduction in query speed in all database queries.\n\nThe team’s technical lead determines that a multi-threaded and scalable caching\nsolution should be used to offload the heavy read traffic. The solution needs to\nimprove performance.\n\nWhich solution will meet these requirements with the LEAST complexity?""]",Use Amazon ElastiCache for Memcached to offload read requests from the main database.,Replicate the data to Amazon DynamoDSet up a DynamoDB Accelerator (DAX) cluster.,Configure the Amazon RDS instances to use Multi-AZ deployment with one standby instance. Offload read requests from the main database to the standby instance.,Use Amazon ElastiCache for Redis to offload read requests from the main database.,"# Answer
- **Correct option:** A : Use Amazon ElastiCache for Memcached to offload read requests from the main database.
- **Reason:** ElastiCache for Memcached is a simplistic and fully managed caching solution well-suited for situations where high read traffic needs to be offloaded from the primary database. Memcached is designed to handle high-throughput workloads and can efficiently store and serve frequently accessed data, reducing the load on the RDS instance during peak times.

# Example / Analogy
- Real-life example or memorable comparison: Think of a busy restaurant where the main kitchen (the RDS instance) struggles to keep up with orders during peak hours. By adding a fast-service takeout window (ElastiCache for Memcached) that serves popular dishes quickly without burdening the kitchen, the restaurant can improve service and reduce wait times for diners.

# Common Mistakes / Traps
- Common trap points, if any: Many might mistakenly choose option B (DynamoDB with DAX) thinking that it is a viable caching solution, but it introduces more complexity and operational overhead as it involves maintaining a separate database setup. Similarly, option C (Multi-AZ) does not offload reads but rather provides high availability, and option D (ElastiCache for Redis) may be perceived as an over-engineered solution given the simpler use case that Memcached specifically addresses.

# Memory Tip
- **Quick tip:** For high read traffic, remember ""Cache first, DB later!"""
185,"[""A developer must provide an API key to an AWS Lambda function to authenticate\nwith a third-party system. The Lambda function will run on a schedule. The\ndeveloper needs to ensure that the API key remains encrypted at rest.\n\nWhich solution will meet these requirements?""]",Store the API key as a Lambda environment variable by using an AWS Key Management Service (AWS KMS) customer managed key.,Configure the application to prompt the user to provide the password to the Lambda function on the first run.,Store the API key as a value in the application code.,Use Lambda@Edge and only communicate over the HTTPS protocol.,"# Answer
- **Correct option:** A : Store the API key as a Lambda environment variable by using an AWS Key Management Service (AWS KMS) customer managed key.

- **Reason:** Storing the API key as a Lambda environment variable allows for easy access within the Lambda function while keeping it encrypted at rest when using AWS KMS for encryption. This meets both the requirement of authentication with a third-party system and the need to ensure that the API key remains secure.

# Example / Analogy
- Think of the API key as a house key that you want to keep in a safe. Storing the key in a Lambda environment variable with KMS is like keeping the house key in a secure locked box, ensuring that it’s both accessible when you need it and protected from unauthorized access.

# Common Mistakes / Traps
- A common mistake is to overlook the importance of encryption. Some may consider using basic environment variables without KMS, thinking that Lambda environment variables are secure, but they can be accessed if not properly encrypted, which exposes sensitive information.

# Memory Tip
- ""Always encrypt sensitive data at rest with AWS KMS."""
186,"[""An IT department uses Amazon S3 to store sensitive images. After more than 1\nyear, the company moves the images into archival storage. The company rarely\naccesses the images, but the company wants a storage solution that maximizes\nresiliency. The IT department needs access to the images that have been moved to\narchival storage within 24 hours.\n\nWhich solution will meet these requirements MOST cost-effectively?""]",Use S3 Standard-Infrequent Access (S3 Standard-IA) to store the images. Use S3 Glacier Deep Archive with standard retrieval to store and retrieve archived images.,Use S3 Standard-Infrequent Access (S3 Standard-IA) to store the images. Use S3 Glacier Deep Archive with bulk retrieval to store and retrieve archived images.,Use S3 Intelligent-Tiering to store the images. Use S3 Glacier Deep Archive with standard retrieval to store and retrieve archived images.,Use S3 One Zone-Infrequent Access (S3 One Zone-IA) to store the images. Use S3 Glacier Deep Archive with bulk retrieval to store and retrieve archived images.,"# Answer
- **Correct option:** B

- **Reason:** Option B is the most cost-effective solution that meets the requirements. Using S3 Standard-IA for initial storage is suitable for infrequent access, and S3 Glacier Deep Archive with bulk retrieval is designed for long-term storage of rarely-accessed data. Although bulk retrieval can take up to 48 hours, it is the most economical choice for archival data where access is not frequently needed but still allows for eventual retrieval within the acceptable limits set by the IT department (which can wait up to 24 hours). 

# Example / Analogy
- Think of S3 Glacier Deep Archive as a storage room in a warehouse where you place old files that you rarely need to access. Choosing bulk retrieval is like asking the warehouse staff to fetch a box of archived files for you; it takes some time, but it’s more cost-effective than express delivery, which would be unnecessary for low-priority files.

# Common Mistakes / Traps
- A common mistake is to overlook the retrieval times associated with different S3 storage classes. While faster retrieval options exist, they often come with higher costs which might not be justified for archival data that is seldom accessed.

# Memory Tip
- ""Choose Glacier Deep Archive for long-term savings on rarely accessed data."""
187,"[""A developer is building a serverless application by using the AWS Serverless\nApplication Model (AWS SAM). The developer is currently testing the application\nin a development environment. When the application is nearly finished, the\ndeveloper will need to set up additional testing and staging environments for a\nquality assurance team.\n\nThe developer wants to use a feature of the AWS SAM to set up deployments to\nmultiple environments.\n\nWhich solution will meet these requirements with the LEAST development effort?""]",Add a configuration file in TOML format to group configuration entries to every environment. Add a table for each testing and staging environment. Deploy updates to the environments by using the sam deploy command and the --config-env flag that corresponds to each environment.,Create additional AWS SAM templates for each testing and staging environment. Write a custom shell script that uses the sam deploy command and the --template-file flag to deploy updates to the environments.,Create one AWS SAM configuration file that has default parameters. Perform updates to the testing and staging environments by using the --parameter-overrides flag in the AWS SAM CLI and the parameters that the updates will override.,Use the existing AWS SAM template. Add additional parameters to configure specific attributes for the serverless function and database table resources that are in each environment. Deploy updates to the testing and staging environments by using the sam deploy command.,"# Answer
- **Correct option:** D
- **Reason:** This option uses the existing AWS SAM template with the addition of parameters to configure environment-specific settings. By deploying updates with the `sam deploy` command and parameters for each environment, the developer minimizes the complexity and effort involved, as they do not need to manage multiple templates or custom scripts.

# Example / Analogy
- Think of it like using a recipe that you can customize with different spices based on your preference. Instead of creating a new recipe for each dish variation, you simply adjust the ingredients (parameters) while keeping the base recipe (template) the same.

# Common Mistakes / Traps
- A common mistake is overcomplicating deployments by creating multiple AWS SAM templates or using custom scripts when modifying parameters in a single template can suffice. This can lead to increased maintenance and potential errors during deployments.

# Memory Tip
- ""One template, many parameters: customize without complication."""
188,"[""A developer is working on an application that processes operating data from IoT\ndevices. Each IoT device uploads a data file once every hour to an Amazon S3\nbucket. The developer wants to immediately process each data file when the data\nfile is uploaded to Amazon S3.\n\nThe developer will use an AWS Lambda function to process the data files from\nAmazon S3. The Lambda function is configured with the S3 bucket information\nwhere the files are uploaded. The developer wants to configure the Lambda\nfunction to immediately invoke after each data file is uploaded.\n\nWhich solution will meet these requirements?""]",Add an asynchronous invocation to the Lambda function. Select the S3 bucket as the source.,Add an Amazon EventBridge event to the Lambda function. Select the S3 bucket as the source.,Add a trigger to the Lambda function. Select the S3 bucket as the source.,Add a layer to the Lambda function. Select the S3 bucket as the source.,"# Answer
- **Correct option:** C : Add a trigger to the Lambda function. Select the S3 bucket as the source.

- **Reason:** To process files immediately upon their upload to an Amazon S3 bucket, you need to set up an event notification that triggers the AWS Lambda function. When you configure an S3 bucket to have a trigger for a Lambda function, the S3 service will automatically invoke the Lambda function each time a new file is uploaded, meeting the requirement for immediate processing.

# Example / Analogy
- Think of the S3 bucket as a mailbox and the Lambda function as a mailman. Each time a new letter (data file) arrives in the mailbox (S3 bucket), the mailman (Lambda function) is automatically notified to pick it up and deliver it (process it) without any extra steps.

# Common Mistakes / Traps
- A common mistake is to confuse the concept of triggers with other methods of invocation like asynchronous invocation or using services like EventBridge, which aren't required for direct S3 notifications. Sometimes candidates think layers are involved in the process, but they only pertain to adding libraries and not triggers.

# Memory Tip
- Remember: **S3 triggers automatically call Lambda for new files!**"
189,"[""A developer is setting up infrastructure by using AWS CloudFormation. If an\nerror occurs when the resources described in the Cloud Formation template are\nprovisioned, successfully provisioned resources must be preserved. The developer\nmust provision and update the CloudFormation stack by using the AWS CLI.\n\nWhich solution will meet these requirements?""]",Add an --enable-termination-protection command line option to the create-stack command and the update-stack command.,Add a --disable-rollback command line option to the create-stack command and the update-stack command.,"Add a --parameters ParameterKey=PreserveResources,ParameterValue=True command line option to the create-stack command and the update-stack command.","Add a --tags Key=PreserveResources,Value=True command line option to the create-stack command and the update-stack command.","# Answer
- **Correct option:** B : Add a --disable-rollback command line option to the create-stack command and the update-stack command.  
- **Reason:** The `--disable-rollback` option prevents AWS CloudFormation from automatically rolling back the stack to its previous state if an error occurs during resource creation or update. This ensures that any successfully provisioned resources remain in place, allowing for manual troubleshooting or partial stack functionality.

# Example / Analogy
- Think of it like cooking a meal: if you accidentally burn part of the dish, you might want to keep the edible parts instead of throwing the whole dish away. `--disable-rollback` lets you keep the successfully cooked portions (resources) even if the whole recipe (stack creation) fails.

# Common Mistakes / Traps
- One common mistake is confusing termination protection (option A) and rollback settings. Termination protection keeps an entire stack from being deleted but does not affect resource failure during creation or updates. Therefore, this does not directly address the requirement to preserve successfully provisioned resources.

# Memory Tip
- Remember: Use `--disable-rollback` to keep successful resources when failures happen!"
190,"[""A developer is building a serverless application that connects to an Amazon\nAurora PostgreSQL database. The serverless application consists of hundreds of\nAWS Lambda functions. During every Lambda function scale out, a new database\nconnection is made that increases database resource consumption.\n\nThe developer needs to decrease the number of connections made to the database.\nThe solution must not impact the scalability of the Lambda functions.\n\nWhich solution will meet these requirements?""]",Configure provisioned concurrency for each Lambda function by setting the ProvisionedConcurrentExecutions parameter to 10.,Enable cluster cache management for Aurora PostgreSQL. Change the connection string of each Lambda function to point to cluster cache management.,Use Amazon RDS Proxy to create a connection pool to manage the database connections. Change the connection string of each Lambda function to reference the proxy.,Configure reserved concurrency for each Lambda function by setting the ReservedConcurrentExecutions parameter to 10.,"# Answer
- **Correct option:** C : Use Amazon RDS Proxy to create a connection pool to manage the database connections. Change the connection string of each Lambda function to reference the proxy.
- **Reason:** Amazon RDS Proxy acts as an intermediary that pools and shares connections to your Aurora PostgreSQL database. This reduces the number of simultaneous connections made by the Lambda functions, optimizing resource consumption without affecting scalability, as the proxy can efficiently handle and manage connection requests.

# Example / Analogy
- Think of Amazon RDS Proxy like a receptionist managing appointments for a busy doctor. Instead of each patient (Lambda function) rushing into the office (database) at the same time, they check in with the receptionist (the proxy) first. The receptionist schedules visits to ensure the doctor isn’t overwhelmed, maintaining efficiency and allowing more patients to be seen without strain on the doctor's time and resources.

# Common Mistakes / Traps
- A common mistake is to select options related to concurrency (like A or D) thinking they control connection behavior. However, these options primarily focus on controlling the scale of Lambda executions and not addressing the database connection management issue at hand.

# Memory Tip
- RDS Proxy = Connection Management; it pools requests so the database isn’t flooded with new connections."
191,"[""A developer is preparing to begin development of a new version of an\napplication. The previous version of the application is deployed in a production\nenvironment. The developer needs to deploy fixes and updates to the current\nversion during the development of the new version of the application. The code\nfor the new version of the application is stored in AWS CodeCommit.\n\nWhich solution will meet these requirements?""]","From the main branch, create a feature branch for production bug fixes. Create a second feature branch from the main branch for development of the new version.",Create a Git tag of the code that is currently deployed in production. Create a Git tag for the development of the new version. Push the two tags to the CodeCommit repository.,"From the main branch, create a branch of the code that is currently deployed in production. Apply an IAM policy that ensures no other users can push or merge to the branch.",Create a new CodeCommit repository for development of the new version of the application. Create a Git tag for the development of the new version.,"# Answer
- **Correct option:** A

- **Reason:** Option A provides a clear approach to handle both ongoing production fixes and the development of a new version. By creating a feature branch from the main branch for production bug fixes, the developer can maintain the stability of the production application while simultaneously working on the new version in a separate feature branch. This allows for effective version control and isolation of changes, ensuring that the ongoing development does not interfere with production stability.

# Example / Analogy
- Think of it like a chef in a restaurant who is preparing a new recipe (the new version) while also fixing a couple of dishes that are currently being served (production bug fixes). The chef creates a new prep station (feature branches) in the kitchen for experimenting with the new recipe, while keeping the main cooking area (main branch) focused on serving customers (production). This way, if a dish gets too complicated or needs tweaks, the restaurant operations continue smoothly.

# Common Mistakes / Traps
- A common mistake is to assume that creating separate tags (as in option B) is sufficient for managing ongoing updates. Tags are used for marking specific states in a repository but do not allow for ongoing development or fixes as branches do.

# Memory Tip
- Think of branches as separate workstations in a kitchen: one for fixing current dishes (production fixes) and one for experimenting with new recipes (new version development)."
192,"[""A developer is creating an AWS CloudFormation stack. The stack contains IAM\nresources with custom names. When the developer tries to deploy the stack, they\nreceive an InsufficientCapabilities error.\n\nWhat should the developer do to resolve this issue?""]",Specify the CAPABILITY_AUTO_EXPAND capability in the CloudFormation stack.,Use an administrators role to deploy IAM resources with CloudFormation.,Specify the CAPABILITY_IAM capability in the CloudFormation stack.,Specify the CAPABILITY_NAMED_IAM capability in the CloudFormation stack.,"# Answer
- **Correct option:** C : Specify the CAPABILITY_IAM capability in the CloudFormation stack.
- **Reason:** The InsufficientCapabilities error occurs when a CloudFormation stack attempts to create or modify IAM resources such as roles, policies, or users without explicitly declaring that it requires permission to do so. To resolve this, the developer needs to specify the CAPABILITY_IAM capability, which allows CloudFormation to create or update IAM resources named in the stack template.

# Example / Analogy
- Real-life example: Think of it like a power of attorney document. Just as someone needs to explicitly grant another person permission to make decisions for them, in this case, you need to explicitly grant CloudFormation the capability to manage IAM resources by specifying the CAPABILITY_IAM capability in your stack.

# Common Mistakes / Traps
- A common mistake is to assume that using administrator roles is sufficient for deploying stacks without explicitly mentioning IAM capabilities. The error arises from not indicating the specific capabilities requested; thus, all roles must be stated in the stack deployment.

# Memory Tip
- Remember: **“You need permission to manage IAM—CAPABILITY_IAM says so!”**"
193,"[""A company uses Amazon API Gateway to expose a set of APIs to customers. The APIs\nhave caching enabled in API Gateway. Customers need a way to invalidate the\ncache for each API when they test the API.\n\nWhat should a developer do to give customers the ability to invalidate the API\ncache?""]",Ask the customers to use AWS credentials to call the InvalidateCache API operation.,Attach an InvalidateCache policy to the IAM execution role that the customers use to invoke the API. Ask the customers to send a request that contains the Cache-Control:max-age=0 HTTP header when they make an API call.,Ask the customers to use the AWS SDK API Gateway class to invoke the InvalidateCache API operation.,Attach an InvalidateCache policy to the IAM execution role that the customers use to invoke the API. Ask the customers to add the INVALIDATE_CACHE query string parameter when they make an API call.,"# Answer
- **Correct option:** B  
- **Reason:** This option effectively allows customers to invalidate the cache for the APIs by using a specific request header. By including the `Cache-Control: max-age=0` HTTP header in their requests, customers signal API Gateway to bypass the cached response, effectively forcing it to fetch fresh data instead.

# Example / Analogy
- Imagine you have a refrigerator that stores leftovers (the cached data) and you want to ensure you always get the freshest dish when you open the fridge. The `Cache-Control: max-age=0` header is like a switch on the fridge that tells it to ignore everything inside and get a fresh new dish from the kitchen instead. It’s a clear signal to retrieve the latest information available.

# Common Mistakes / Traps
- A common mistake is choosing options that incorrectly assume customers need to invoke specific AWS API calls or use AWS SDKs directly. Cache invalidation in the context of API Gateway can be done simply through HTTP request headers, which is often overlooked.

# Memory Tip
- Remember: ""Use the right header to refresh the data!"""
194,"[""A company’s website runs on an Amazon EC2 instance and uses Auto Scaling to\nscale the environment during peak times. Website users across the world are\nexperiencing high latency due to static content on the EC2 instance, even during\nnon-peak hours.\n\nWhich combination of steps will resolve the latency issue? (Choose two.)""]",Double the Auto Scaling group’s maximum number of servers.,Host the application code on AWS Lambda.,Scale vertically by resizing the EC2 instances.,Create an Amazon CloudFront distribution to cache the static content.,"# Answer
- **Correct option:** D, B

- **Reason:** 
   - **Option D**: Creating an Amazon CloudFront distribution caches the static content at edge locations around the world, reducing latency for users by serving content from a location closer to them rather than from a single EC2 instance. 
   - **Option B**: Hosting the application code on AWS Lambda can help reduce latency for dynamic content as it allows the application to scale automatically without the need for managing EC2 instances, thus potentially offloading static content serving to Amazon S3 alongside CloudFront.

# Example / Analogy
- Think of CloudFront as a local library in multiple cities. Instead of having to travel to one central library (the EC2 instance) that may be far away, people can get books (static content) from their local branches (CloudFront edge locations), making it faster and easier to access them.

# Common Mistakes / Traps
- A common mistake is to assume that simply increasing the number of EC2 instances (option A and C) will resolve latency. However, if the issue is specifically related to the delivery of static content, caching that content closer to users (like with CloudFront) is often far more effective.

# Memory Tip
- Use CloudFront for fast global access to static content!"
195,"[""A developer is building a serverless application by using AWS Serverless\nApplication Model (AWS SAM) on multiple AWS Lambda functions. When the\napplication is deployed, the developer wants to shift 10% of the traffic to the\nnew deployment of the application for the first 10 minutes after deployment. If\nthere are no issues, all traffic must switch over to the new version.\n\nWhich change to the AWS SAM template will meet these requirements?""]",Set the Deployment Preference Type to Canary10Percent10Minutes. Set the AutoPublishAlias property to the Lambda alias.,Set the Deployment Preference Type to Linear10PercentEvery10Minutes. Set AutoPublishAlias property to the Lambda alias.,Set the Deployment Preference Type to Canary10Percent10Minutes. Set the PreTraffic and PostTraffic properties to the Lambda alias.,Set the Deployment Preference Type to Linear10PercentEvery10Minutes. Set PreTraffic and PostTraffic properties to the Lambda alias.,"# Answer
- **Correct option:** A
- **Reason:** Setting the Deployment Preference Type to `Canary10Percent10Minutes` allows 10% of the traffic to be shifted to the new deployment over the first 10 minutes. The `AutoPublishAlias` property enables the Lambda alias to point to the new version, coordinating the traffic shift seamlessly.

# Example / Analogy
- Think of it like a restaurant test-driving a new dish. They allow a limited number of tables (10%) to try the new dish for the first 10 minutes. If no one complains and they get positive feedback, they offer the dish to everyone else after the trial.

# Common Mistakes / Traps
- Many candidates might confuse **canary** deployments with **linear** deployments. A linear deployment gradually increases the percentage of traffic over time, while a canary deployment allows a smaller percentage to test the changes first, which is ideal for the scenario described.

# Memory Tip
- **""Canary – 10% in 10 minutes is the key!""**"
196,"[""An online sales company is developing a serverless application that runs on AWS.\nThe application uses an AWS Lambda function that calculates order success rates\nand stores the data in an Amazon DynamoDB table. A developer wants an efficient\nway to invoke the Lambda function every 15 minutes.\n\nWhich solution will meet this requirement with the LEAST development effort?""]",Create an Amazon EventBridge rule that has a rate expression that will run the rule every 15 minutes. Add the Lambda function as the target of the EventBridge rule.,Create an AWS Systems Manager document that has a script that will invoke the Lambda function on Amazon EC2. Use a Systems Manager Run Command task to run the shell script every 15 minutes.,Create an AWS Step Functions state machine. Configure the state machine to invoke the Lambda function execution role at a specified interval by using a Wait state. Set the interval to 15 minutes.,Provision a small Amazon EC2 instance. Set up a cron job that invokes the Lambda function every 15 minutes.,"# Answer
- **Correct option:** A
- **Reason:** Creating an Amazon EventBridge rule with a rate expression is the simplest and most efficient method to invoke the Lambda function every 15 minutes. This solution requires minimal development effort, as it directly utilizes AWS services designed for event scheduling, and it does not require additional infrastructure or complex configurations.

# Example / Analogy
- Think of EventBridge like a digital alarm clock. Just as you set your alarm to sound every 15 minutes to remind you to check something, EventBridge can ""sound the alarm"" by invoking the Lambda function at those regular intervals without you needing to manage any additional components.

# Common Mistakes / Traps
- A common mistake is overengineering the solution by considering options like EC2 with cron jobs or creating AWS Step Functions. These methods require more setup, management, and potential costs compared to the straightforward EventBridge approach.

# Memory Tip
- Remember: **EventBridge is like an alarm clock for AWS services—set it, and it triggers your tasks on time!**"
197,"[""A developer is creating an AWS Lambda function that will generate and export a\nfile. The function requires 100 MB of temporary storage for temporary files\nwhile running. These files will not be needed after the function is complete.\n\nHow can the developer MOST efficiently handle the temporary files?""]",Store the files in Amazon Elastic Block Store (Amazon EBS) and delete the files at the end of the Lambda function.,Copy the files to Amazon Elastic File System (Amazon EFS) and delete the files at the end of the Lambda function.,Store the files in the /tmp directory and delete the files at the end of the Lambda function.,Copy the files to an Amazon S3 bucket with a lifecycle policy to delete the files.,"# Answer
- **Correct option:** C : Store the files in the /tmp directory and delete the files at the end of the Lambda function.

- **Reason:** AWS Lambda provides a temporary storage area in the `/tmp` directory that is available during the execution of the function. This storage can hold up to 512 MB of data, which suits the requirement of needing 100 MB of temporary storage. The files in the `/tmp` directory are automatically deleted when the Lambda function execution finishes, making it the most efficient option for temporary files that are not needed after the function completes.

# Example / Analogy
- Imagine you're in a hotel room (AWS Lambda) where you can unpack your suitcase (the temporary files) and keep the items you need during your stay. Once you check out, the hotel will clean the room and remove all your items. Similarly, the `/tmp` directory allows for temporary file storage that gets cleaned up after the function runs.

# Common Mistakes / Traps
- A common mistake is to choose options that involve more complexity or unnecessary resource management, like EBS or EFS, when simple temporary storage is sufficient. Many developers may not realize that the `/tmp` directory is ideal for temporary files, leading them to overcomplicate the solution.

# Memory Tip
- Remember: **Use `/tmp` in Lambda for quick, temporary storage – it cleans up after itself!**"
198,"[""A company uses Amazon DynamoDB as a data store for its order management system.\nThe company frontend application stores orders in a DynamoDB table. The DynamoDB\ntable is configured to send change events to a DynamoDB stream. The company uses\nan AWS Lambda function to log and process the incoming orders based on data from\nthe DynamoDB stream.\n\nAn operational review reveals that the order quantity of incoming orders is\nsometimes set to 0. A developer needs to create a dashboard that will show how\nmany unique customers this problem affects each day.\n\nWhat should the developer do to implement the dashboard?""]",Grant the Lambda function’s execution role permissions to upload logs to Amazon CloudWatch Logs. Implement a CloudWatch Logs Insights query that selects the number of unique customers for orders with order quantity equal to 0 and groups the results in 1-day periods. Add the CloudWatch Logs Insights query to a CloudWatch dashboard.,Use Amazon Athena to query AWS CloudTrail API logs for API calls. Implement an Athena query that selects the number of unique customers for orders with order quantity equal to 0 and groups the results in 1-day periods. Add the Athena query to an Amazon CloudWatch dashboard.,Configure the Lambda function to send events to Amazon EventBridge. Create an EventBridge rule that groups the number of unique customers for orders with order quantity equal to 0 in 1-day periods. Add a CloudWatch dashboard as the target of the rule.,Turn on custom Amazon CloudWatch metrics for the DynamoDB stream of the DynamoDB table. Create a CloudWatch alarm that groups the number of unique customers for orders with order quantity equal to 0 in 1-day periods. Add the CloudWatch alarm to a CloudWatch dashboard.,"# Answer
- **Correct option:** A
- **Reason:** Granting the Lambda function's execution role permissions to log to Amazon CloudWatch Logs allows the function to capture logs related to orders with quantities set to 0. Using CloudWatch Logs Insights, the developer can write a query to filter and group these logs by unique customer IDs over 1-day periods, which directly aligns with the requirement of showing how many unique customers are affected each day.

# Example / Analogy
- Think of the Lambda function as a cashier in a store that notes each transaction. If a customer tries to buy an item for $0, the cashier records this abnormality in a log book (CloudWatch Logs). Later, a manager (the developer) can analyze this log book to count how many different customers made $0 purchases each day, using a special tool that reads the log (CloudWatch Logs Insights).

# Common Mistakes / Traps
- A common mistake is to confuse where the data is stored and how it's analyzed. Using CloudTrail or EventBridge might seem appealing, but they do not directly address logging the specific issues with order quantities and the requirement for unique customer counts. 

# Memory Tip
- Remember: **""Log it, query it, visualize it""** with CloudWatch Logs Insights for specific operational issues."
199,"[""A developer is working on an AWS Lambda function that accesses Amazon DynamoDB.\nThe Lambda function must retrieve an item and update some of its attributes, or\ncreate the item if it does not exist. The Lambda function has access to the\nprimary key.\n\nWhich IAM permissions should the developer request for the Lambda function to\nachieve this functionality?""]",dynamodb:DeleleItem dynamodb:GetItem dynamodb:PutItem,dynamodb:UpdateItem dynamodb:GetItem dynamodb:DescribeTable,dynamodb:GetRecords dynamodb:PutItem dynamodb:UpdateTable,dynamodb:UpdateItem dynamodb:GetItem dynamodb:PutItem,"# Answer
- **Correct option:** D : dynamodb:UpdateItem dynamodb:GetItem dynamodb:PutItem
- **Reason:** This option includes the necessary permissions for retrieving an item (`dynamodb:GetItem`), updating an existing item or creating it if it doesn't exist (`dynamodb:UpdateItem`), and inserting a new item into the table (`dynamodb:PutItem`). This allows the Lambda function to perform all required actions to achieve the specified functionality.

# Example / Analogy
- Think of a library where you can check out a book, update the information on the book if necessary (like changing a return date), or add a new book to the catalog if it doesn't exist. The permissions represent the actions you need to perform these tasks in the library.

# Common Mistakes / Traps
- A common mistake is to select options that include permissions that are irrelevant to the task, such as `dynamodb:DeleteItem`, which is not needed here. Also, some might mistakenly think that `dynamodb:DescribeTable` is required, but it’s not needed for retrieving or updating items.

# Memory Tip
- Remember ""Get, Put, Update"" — those are the keys for fetching, creating, and modifying items in DynamoDB."
200,"[""A company requires that all applications running on Amazon EC2 use IAM roles to\ngain access to AWS services. A developer is modifying an application that\ncurrently relies on IAM user access keys stored in environment variables to\naccess Amazon DynamoDB tables using boto, the AWS SDK for Python.\n\nThe developer associated a role with the same permissions as the IAM user to the\nEC2 instance, then deleted the IAM user. When the application was restarted, the\nAWS AccessDeniedException messages started appearing in the application logs.\nThe developer was able to use their personal account on the server to run\nDynamoDB API commands using the AWS CLI.\n\nWhat is the MOST likely cause of the exception?""]",IAM policies might take a few minutes to propagate to resources.,Disabled environment variable credentials are still being used by the application.,The AWS SDK does not support credentials obtained using an instance role.,The instance’s security group does not allow access to http://169.254.169.254.,"# Answer
- **Correct option:** B : Disabled environment variable credentials are still being used by the application.
- **Reason:** The application was initially configured to use IAM user access keys stored in environment variables to authenticate with DynamoDB. Although the IAM user was deleted and a role was correctly associated with the EC2 instance, if the application is still referencing the old environment variables, it will continue to use the now-invalid access keys, leading to the `AccessDeniedException`.

# Example / Analogy
- Think of it like a car (the application) that has been configured to use a specific key (the IAM user access keys) to start. After you remove that key and replace it with a new one (the role), if the driver (the application) keeps trying to use the old key, the car won’t start. The driver must use the new key to get the vehicle moving.

# Common Mistakes / Traps
- A common mistake is to overlook how the application retrieves its credentials. Many developers may incorrectly assume that assigning a role to the instance automatically updates all components of the application to use that role's credentials without checking the code or environment configuration.

# Memory Tip
- Always check for legacy credentials in your environment before modifying IAM users or roles!"
201,"[""A company has an existing application that has hardcoded database credentials. A\ndeveloper needs to modify the existing application. The application is deployed\nin two AWS Regions with an active-passive failover configuration to meet\ncompany’s disaster recovery strategy.\n\nThe developer needs a solution to store the credentials outside the code. The\nsolution must comply with the company’s disaster recovery strategy.\n\nWhich solution will meet these requirements in the MOST secure way?""]",Store the credentials in AWS Secrets Manager in the primary Region. Enable secret replication to the secondary Region. Update the application to use the Amazon Resource Name (ARN) based on the Region.,Store credentials in AWS Systems Manager Parameter Store in the primary Region. Enable parameter replication to the secondary Region. Update the application to use the Amazon Resource Name (ARN) based on the Region.,"Store credentials in a config file. Upload the config file to an S3 bucket in the primary Region. Enable Cross-Region Replication (CRR) to an S3 bucket in the secondary region. Update the application to access the config file from the S3 bucket, based on the Region.",Store credentials in a config file. Upload the config file to an Amazon Elastic File System (Amazon EFS) file system. Update the application to use the Amazon EFS file system Regional endpoints to access the config file in the primary and secondary Regions.,"# Answer
- **Correct option:** A

- **Reason:** Storing the credentials in AWS Secrets Manager with secret replication to the secondary Region ensures that credentials are stored securely and are accessible in both Regions, complying with the disaster recovery strategy. Secrets Manager provides automatic encryption, fine-grained access control, and auditing capabilities, making it the most secure option for confidential information like database credentials.

# Example / Analogy
- Think of AWS Secrets Manager like a secure locker for important keys (credentials), where you can keep duplicates in two separate locations (Regions). If the primary locker is compromised or lost, you still have access to the backup locker, ensuring you can always retrieve your keys when needed.

# Common Mistakes / Traps
- A common mistake is choosing AWS Systems Manager Parameter Store or other storage options that may not offer the same level of security features as Secrets Manager. While these options are viable, they may not provide the necessary encryption and management capabilities expected for sensitive information like database credentials.

# Memory Tip
- Remember: ""Secrets need a secure keeper"" - use AWS Secrets Manager for storing sensitive information safely!"
202,"[""A developer is receiving HTTP 400: ThrottlingException errors intermittently\nwhen calling the Amazon CloudWatch API. When a call fails, no data is retrieved.\n\nWhat best practice should first be applied to address this issue?""]",Contact AWS Support for a limit increase.,Use the AWS CLI to get the metrics.,Analyze the applications and remove the API call.,Retry the call with exponential backoff.,"# Answer
- **Correct option:** D : Retry the call with exponential backoff.

- **Reason:** This answer is correct because AWS services, including the CloudWatch API, often impose rate limits. A ThrottlingException indicates that the requests exceed these limits. Using exponential backoff is a standard best practice in distributed systems, where a service will retry failed requests after progressively longer waiting periods. This method helps to avoid overwhelming the service with repeated requests and increases the chances of the requests succeeding after waiting for a while.

# Example / Analogy
- Think of it like trying to get into a busy restaurant. If you keep trying to barge through the door when it’s full, you’ll just be turned away (throttled). Instead, if you wait a moment (retry after a short pause) and try again (exponential backoff), you will have a better chance of being let in when the crowd disperses.

# Common Mistakes / Traps
- A common trap is to think that simply contacting AWS Support for a limit increase (Option A) is the best initial action. This solution can take time and is not always necessary if implementing retry logic resolves the issue promptly.

# Memory Tip
- Remember: When facing throttling, ""Pause and Retry"" with exponential backoff to succeed."
203,"[""An application needs to use the IP address of the client in its processing. The\napplication has been moved into AWS and has been placed behind an Application\nLoad Balancer (ALB). However, all the client IP addresses now appear to be the\nsame. The application must maintain the ability to scale horizontally.\n\nBased on this scenario, what is the MOST cost-effective solution to this\nproblem?""]",Remove the application from the ALB. Delete the ALB and change Amazon Route 53 to direct traffic to the instance running the application.,Remove the application from the ALCreate a Classic Load Balancer in its place. Direct traffic to the application using the HTTP protocol.,Alter the application code to inspect the X-Forwarded-For header. Ensure that the code can work properly if a list of IP addresses is passed in the header.,Alter the application code to inspect a custom header. Alter the client code to pass the IP address in the custom header.,"# Answer
- **Correct option:** C
- **Reason:** The application needs to access the original client IP addresses that are obscured by the Application Load Balancer (ALB). The ALB forwards the actual client IP in the `X-Forwarded-For` header. By altering the application code to inspect this header, the application can extract and utilize the original client IP addresses while still utilizing the benefits of the ALB for scaling and managing traffic.

# Example / Analogy
- Consider a ticket counter at a concert where attendees line up in a queue. The ticket counter represents the ALB, while each individual in line represents a client. When attendees check in at the counter, they each provide a name (representing their IP). However, if the counter just records everyone's names as “Attendee,” it loses track of who is who. By instructing the counter to also take note of the original names of the attendees (using the `X-Forwarded-For`), it can identify individuals for issues like refunds or special requests without losing the scaling benefits of the queue.

# Common Mistakes / Traps
- A common mistake is to think that removing the ALB or replacing it with a Classic Load Balancer (Choices A and B) will solve the issue. This not only removes the benefits of the ALB but also will not provide the original client IPs. Another mistake is to assume that creating a custom header (Choice D) is necessary when the standard `X-Forwarded-For` header is already provided by AWS services.

# Memory Tip
- Remember: ""X-Forwarded-For holds the key to original client IP."""
204,"[""A web application is using Amazon Kinesis Data Streams for clickstream data that\nmay not be consumed for up to 12 hours.\n\nHow can the developer implement encryption at rest for data within the Kinesis\nData Streams?""]",Enable SSL connections to Kinesis.,Use Amazon Kinesis Consumer Library.,Encrypt the data once it is at rest with a Lambda function.,Enable server-side encryption in Kinesis Data Streams.,"# Answer
- **Correct option:** D : Enable server-side encryption in Kinesis Data Streams.
  
- **Reason:** This option directly allows for encryption of the data at rest within Kinesis Data Streams. Server-side encryption (SSE) automatically encrypts your data when it is stored in the stream, ensuring that sensitive clickstream data is protected without additional code or custom encryption logic. This is a built-in feature that provides peace of mind regarding data security.

# Example / Analogy
- Think of server-side encryption like a bank vault for your cash. Just as a bank secures your money when you deposit it, SSE secures your data as it rests in the stream. You lock it up to prevent unauthorized access while it's stored.

# Common Mistakes / Traps
- A common trap is to confuse encryption at rest with securing data in transit. Options like A (Enable SSL connections) secure the data while it's being transmitted but do not provide encryption for data stored in Kinesis Data Streams. Another common mistake is considering custom solutions (like using a Lambda function) instead of leveraging built-in features like SSE.

# Memory Tip
- Remember: ""Encrypting data at rest? Use Kinesis’ server-side encryption – keeping it locked up tight!"""
205,"[""An application is real-time processing millions of events that are received\nthrough an API.\n\nWhat service could be used to allow multiple consumers to process the data\nconcurrently and MOST cost-effectively?""]",Amazon SNS with fanout to an SQS queue for each application,"Amazon SNS with fanout to an SQS FIFO (first-in, first-out) queue for each application",Amazon Kinesis Firehose,Amazon Kinesis Data Streams,"# Answer
- **Correct option:** D : Amazon Kinesis Data Streams
- **Reason:** Amazon Kinesis Data Streams allows for real-time processing of millions of events by enabling multiple consumers to process the data concurrently. It is designed for high-throughput applications and is the most cost-effective option for scenarios requiring real-time event streaming and analysis.

# Example / Analogy
- Imagine a highway (Kinesis Data Streams) where cars (events) can travel at high speed and multiple cars can drive side by side (concurrent consumers). It's an efficient way for many vehicles to move simultaneously, unlike a one-lane road where only one car can go at a time (like an SQS FIFO queue).

# Common Mistakes / Traps
- A common mistake is to confuse Amazon Kinesis Data Streams with Kinesis Firehose or SQS. Firehose is great for loading data to destinations like S3 but is not meant for concurrent processing by multiple consumers. SQS FIFO queues are used for ordered message processing, not optimal for high-throughput real-time processing needed here.

# Memory Tip
- Think of Kinesis Data Streams as a “multi-lane highway” for streaming data, allowing many applications to process data simultaneously."
206,"[""A developer needs to manage AWS infrastructure as code and must be able to\ndeploy multiple identical copies of the infrastructure, stage changes, and\nrevert to previous versions.\n\nWhich approach addresses these requirements?""]",Use cost allocation reports and AWS OpsWorks to deploy and manage the infrastructure.,Use Amazon CloudWatch metrics and alerts along with resource tagging to deploy and manage the infrastructure.,Use AWS Elastic Beanstalk and AWS CodeCommit to deploy and manage the infrastructure.,Use AWS CloudFormation and AWS CodeCommit to deploy and manage the infrastructure.,"# Answer
- **Correct option:** D : Use AWS CloudFormation and AWS CodeCommit to deploy and manage the infrastructure.

- **Reason:** AWS CloudFormation allows developers to define their infrastructure as code, enabling the deployment of multiple identical environments and facilitating version control for changes. This means you can stage changes and revert to previous infrastructures easily. AWS CodeCommit complements this by providing a Git-based version control system for managing the CloudFormation templates, allowing you to track changes and collaborate with others.

# Example / Analogy
- Think of AWS CloudFormation like a blueprint for a house. You can make changes to the blueprint (like moving a wall or adding a room), and with AWS CodeCommit, you can keep track of all the versions of these blueprints. If you decide you want to revert back to an older version of the design, you can do that easily, just like going back to a previous version of your house design.

# Common Mistakes / Traps
- A common mistake is to confuse AWS OpsWorks and Elastic Beanstalk with CloudFormation. While both can manage application deployments, they don’t provide the same level of infrastructure-as-code capabilities and version tracking for infrastructure itself like CloudFormation does.

# Memory Tip
- **Think CloudFormation = Blueprint + CodeCommit = Version Control.**"
207,"[""A developer needs to launch a new Amazon EC2 instance by using the AWS CLI.\n\nWhich AWS CLI command should the developer use to meet this requirement?""]",aws ec2 bundle-instance,aws ec2 start-instances,aws ec2 confirm-product-instance,aws ec2 run-instances,"# Answer
- **Correct option:** D : aws ec2 run-instances
- **Reason:** The `aws ec2 run-instances` command is specifically designed to launch one or more Amazon EC2 instances from an Amazon Machine Image (AMI). This command provisions the instance with specified configurations such as instance type, key pair, and security groups.

# Example / Analogy
- Think of launching an EC2 instance like ordering a custom sandwich. When you use the `run-instances` command, you're placing an order with specific ingredients (instance type, AMI, security groups) to create your ideal sandwich (instance) on a fresh loaf of bread (the underlying hardware).

# Common Mistakes / Traps
- A common mistake is confusing launching a new instance with starting an existing one. The `start-instances` command only starts instances that have already been created and are in a stopped state.

# Memory Tip
- Remember: ""Run a new instance, don’t just start what's stopped."""
208,"[""A company is hosting a workshop for external users and wants to share the\nreference documents with the external users for 7 days. The company stores the\nreference documents in an Amazon S3 bucket that the company owns.\n\nWhat is the MOST secure way to share the documents with the external users?""]",Use S3 presigned URLs to share the documents with the external users. Set an expiration time of 7 days.,Move the documents to an Amazon WorkDocs folder. Share the links of the WorkDocs folder with the external users.,Create temporary IAM users that have read-only access to the S3 bucket. Share the access keys with the external users. Expire the credentials after 7 days.,Create a role that has read-only access to the S3 bucket. Share the Amazon Resource Name (ARN) of this role with the external users.,"# Answer
- **Correct option:** A: Use S3 presigned URLs to share the documents with the external users. Set an expiration time of 7 days.

- **Reason:** S3 presigned URLs allow you to provide temporary access to objects in your S3 bucket without needing to expose your AWS credentials or manage user access. By setting an expiration time of 7 days, you ensure that the links become invalid after this period, which enhances security.

# Example / Analogy
- Think of a presigned URL like a ticket to a concert. You can give someone a ticket that allows them entry for a limited time. Once the concert is over (or the time expires), the ticket is no longer valid, and they can't use it to enter again.

# Common Mistakes / Traps
- A common trap is to consider using IAM users or roles for external sharing. This introduces complexity and potential security risks, as managing access to AWS credentials can be challenging and unwieldy, especially for external users.

# Memory Tip
- Remember: ""Presigned URLs are like temporary tickets – valid for a limited time, easy to share, and secure."""
209,"[""A developer needs to troubleshoot an AWS Lambda function in a development\nenvironment. The Lambda function is configured in VPC mode and needs to connect\nto an existing Amazon RDS for SQL Server DB instance. The DB instance is\ndeployed in a private subnet and accepts connections by using port 1433.\n\nWhen the developer tests the function, the function reports an error when it\ntries to connect to the database.\n\nWhich combination of steps should the developer take to diagnose this issue?\n(Choose two.)""]",Check that the function’s security group has outbound access on port 1433 to the DB instance’s security group. Check that the DB instance’s security group has inbound access on port 1433 from the function’s security group.,Check that the function’s security group has inbound access on port 1433 from the DB instance’s security group. Check that the DB instance’s security group has outbound access on port 1433 to the function’s security group.,Check that the VPC is set up for a NAT gateway. Check that the DB instance has the public access option turned on.,"Check that the function’s execution role permissions include rds:DescribeDBInstances, rds:ModifyDBInstance. and rds:DescribeDBSecurityGroups for the DB instance.","# Answer
- **Correct option:** A

- **Reason:**  
To troubleshoot the connection issue between the Lambda function and the RDS instance, the developer needs to ensure that the security group settings are correctly configured. Specifically, the function's security group needs to have outbound rules allowing traffic on port 1433 to the RDS instance's security group. Likewise, the RDS instance's security group must permit inbound traffic on port 1433 from the Lambda function's security group. If these permit rules aren’t in place, the Lambda function will be unable to establish a connection to the database, leading to errors when trying to access it.

# Example / Analogy
- Think of security groups like a club's bouncer system. If you’re a member (the Lambda function), you need to be allowed to leave the club (send data) to visit another member's house (the RDS instance). To visit the house, the owner (the RDS security group) must also allow members from the club (Lambda's security group) to come inside. If both aren't aligned, you won’t get through.

# Common Mistakes / Traps
- A common mistake is confusing inbound and outbound rules. Remember, for a Lambda function to access an RDS instance, the Lambda's security group must have an outbound rule and the RDS’s security group must have an inbound rule. Also, options that mention settings not relevant to the direct connection issue, like NAT gateway settings (option C), can be misleading.

# Memory Tip
- ""Access requires clearance both ways: outbound for the function and inbound for the database."""
210,"[""A developer has built a market application that stores pricing data in Amazon\nDynamoDB with Amazon ElastiCache in front. The prices of items in the market\nchange frequently. Sellers have begun complaining that, after they update the\nprice of an item, the price does not actually change in the product listing.\n\nWhat could be causing this issue?""]",The cache is not being invalidated when the price of the item is changed.,The price of the item is being retrieved using a write-through ElastiCache cluster.,The DynamoDB table was provisioned with insufficient read capacity.,The DynamoDB table was provisioned with insufficient write capacity.,"# Answer
- **Correct option:** A : The cache is not being invalidated when the price of the item is changed.
- **Reason:** In this scenario, when the seller updates the price of an item in the DynamoDB table, if the cache (Amazon ElastiCache) is not invalidated or refreshed, it will continue serving the stale data (the old price) to users. This is a common issue known as “cache stale data,” where changes in the source of truth (DynamoDB) are not reflected in the cache stored in ElastiCache.

# Example / Analogy
- Think of a library that has a section of books (DynamoDB) and a reading room (ElastiCache) where people go to read the latest books. If someone returns a book to the library with an updated title, but the staff forgets to replace the old book in the reading room with the new one, visitors will still see the outdated title. Similarly, if the cache isn't updated after a price change in the database, users will see the incorrect pricing.

# Common Mistakes / Traps
- A common mistake is to assume that simply changing the data in DynamoDB automatically updates the cache. However, cache management requires explicit invalidation or refresh procedures to ensure that updated data is correctly reflected.

# Memory Tip
- Always remember: *Cache must be invalidated to reflect changes in the database.*"
211,"[""A developer is working on an application that is deployed on an Amazon EC2\ninstance. The developer needs a solution that will securely transfer files from\nthe application to an Amazon S3 bucket.\n\nWhat should the developer do to meet these requirements in the MOST secure way?""]",Create an IAM user. Create an access key for the IAM user. Store the access key in the application’s environment variables.,Create an IAM role. Create an access key for the IAM role. Store the access key in the application’s environment variables.,Create an IAM role. Configure the IAM role to access the specific Amazon S3 API calls the application requires. Associate the IAM role with the EC2 instance.,Configure an S3 bucket policy for the S3 bucket. Configure the S3 bucket policy to allow access for the EC2 instance ID.,"# Answer
- **Correct option:** C
- **Reason:** Creating an IAM role with appropriate permissions and associating it with the EC2 instance is the most secure way to allow the application to access the S3 bucket. This method avoids hardcoding credentials and eliminates the risk of exposing access keys.

# Example / Analogy
- Think of an IAM role as a keycard that only allows authorized personnel to enter a building. Instead of giving each person a permanent key (which can be lost or copied), you provide them temporary access through a keycard (IAM role) that can be easily revoked. Similarly, when the EC2 instance is terminated, the IAM role permissions are automatically removed, enhancing security.

# Common Mistakes / Traps
- A common mistake is to choose option A or B, which involve hardcoding access keys or storing them in environment variables. This practice is insecure because if the access keys are exposed, anyone with those keys can access the AWS resources.

# Memory Tip
- Remember: ""Use IAM roles for temporary, safe access – no keys to lose!"""
212,"[""An AWS Lambda function is running in a company’s shared AWS account. The\nfunction needs to perform an additional ec2:DescribeInstances action that is\ndirected at the company’s development accounts. A developer must configure the\nrequired permissions across the accounts.\n\nHow should the developer configure the permissions to adhere to the principle of\nleast privilege?""]",Create an IAM role in the shared account. Add the ec2:DescribeInstances permission to the role. Establish a trust relationship between the development accounts for this role. Update the Lambda function IAM role in the shared account by adding the ec2:DescribeInstances permission to the role.,Create an IAM role in the development accounts. Add the ec2:DescribeInstances permission to the role. Establish a trust relationship with the shared account for this role. Update the Lambda function IAM role in the shared account by adding the iam:AssumeRole permissions.,Create an IAM role in the shared account. Add the ec2:DescribeInstances permission to the role. Establish a trust relationship between the development accounts for this role. Update the Lambda function IAM role in the shared account by adding the iam:AssumeRole permissions.,Create an IAM role in the development accounts. Add the ec2:DescribeInstances permission to the role. Establish a trust relationship with the shared account for this role. Update the Lambda function IAM role in the shared account by adding the ec2:DescribeInstances permission to the role.,"# Answer
- **Correct option:** B

- **Reason:** This option correctly aligns with the principle of least privilege by creating an IAM role in the development accounts that has the required `ec2:DescribeInstances` permission. By establishing a trust relationship with the shared account, the Lambda function can assume this role to perform that action. The emphasis on allowing only the necessary permissions within the necessary account structures maintains minimal access rights and security.

# Example / Analogy
- Think of it like a library system. Instead of giving everyone in the neighborhood access to the entire library, you create a special visitor pass for people working on the development of a library section. This pass allows them to look at only the books they are specifically assigned to. By doing this, you restrict access to sensitive information while allowing the necessary work to be done effectively.

# Common Mistakes / Traps
- A common mistake is assuming that giving permissions directly in the Lambda function IAM role (as done in option A or C) is sufficient without utilizing cross-account role assumptions. Participants might overlook the importance of trust relationships in establishing proper permissions.

# Memory Tip
- ""Allow roles to talk, restrict actions to needed tasks."""
213,"[""A developer is designing a serverless application that customers use to select\nseats for a concert venue. Customers send the ticket requests to an Amazon API\nGateway API with an AWS Lambda function that acknowledges the order and\ngenerates an order ID. The application includes two additional Lambda functions:\none for inventory management and one for payment processing. These two Lambda\nfunctions run in parallel and write the order to an Amazon Dynamo DB table.\n\nThe application must provide seats to customers according to the following\nrequirements. If a seat is accidently sold more than once, the first order that\nthe application received must get the seat. In these cases, the application must\nprocess the payment for only the first order. However, if the first order is\nrejected during payment processing, the second order must get the seat. In these\ncases, the application must process the payment for the second order.\n\nWhich solution will meet these requirements?""]",Send the order ID to an Amazon Simple Notification Service (Amazon SNS) FIFO topic that fans out to one Amazon Simple Queue Service (Amazon SQS) FIFO queue for inventory management and another SQS FIFO queue for payment processing.,Change the Lambda function that generates the order ID to initiate the Lambda function for inventory management. Then initiate the Lambda function for payment processing.,Send the order ID to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the Lambda functions for inventory management and payment processing to the topic.,Deliver the order ID to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda functions for inventory management and payment processing to poll the queue.,"# Answer
- **Correct option:** A
- **Reason:** Option A uses Amazon SNS FIFO topics and SQS FIFO queues, which ensure that the orders (messages) are processed in the exact order they were received. This order is essential for the application’s requirement that if a seat is sold more than once, the first order gets priority in payment processing. By using FIFO topics and queues, you can handle the payment processing appropriately and ensure that only the relevant order gets its payment processed first. Additionally, SQS FIFO queues help manage concurrency, meaning multiple orders can be processed without conflicts.

# Example / Analogy
- Think of a concert ticket sale as a line at a ticket booth. Customers wait in line and the first person in line receives the ticket (seat) first. If that first person decides not to buy, then the next person in line gets their chance. Using FIFO messaging systems ensures that the same order is followed, just like a physical line at the box office. 

# Common Mistakes / Traps
- A common mistake would be choosing option C, which uses standard SNS and SQS options. Standard queues do not guarantee order, which could result in an improper seating assignment. Option B suggests initiating the functions in a non-parallel way, losing the concurrent aspect that is crucial for the application, while option D could lead to timing issues and race conditions without ordered processing.

# Memory Tip
- **Remember:** FIFO means ""First In, First Out"" – the first request gets processed first, just as in a queue!"
214,"[""An application uses AWS X-Ray to generate a large amount of trace data on an\nhourly basis. A developer wants to use filter expressions to limit the returned\nresults through user-specified custom attributes.\n\nHow should the developer use filter expressions to filter the results in X-Ray?""]",Add custom attributes as annotations in the segment document.,Add custom attributes as metadata in the segment document.,Add custom attributes as new segment fields in the segment document.,Create new sampling rules that are based on custom attributes.,"# Answer
- **Correct option:** A : Add custom attributes as annotations in the segment document.

- **Reason:** In AWS X-Ray, filter expressions can be utilized to limit query results based on custom attributes that are defined in annotations. Annotations are indexed, which allows for efficient searching and filtering, while metadata does not support this. Therefore, to effectively use filter expressions based on user-specified custom attributes, the developer must add those attributes as annotations in the segment document.

# Example / Analogy
- Think of annotations as labels on a box of files. Each label (annotation) helps you quickly find and filter through the contents of the box (trace data) based on certain criteria (custom attributes). In comparison, metadata would be the details written inside the files themselves – they are not easily searchable without opening each one.

# Common Mistakes / Traps
- A common mistake is confusing metadata with annotations. While both can contain additional information, only annotations can be indexed and thus be effectively used for filtering results in queries.

# Memory Tip
- Remember: **Annotations are the key to filtering in X-Ray!**"
215,"[""A developer is planning to use an Amazon API Gateway and AWS Lambda to provide a\nREST API. The developer will have three distinct environments to manage:\ndevelopment, test, and production.\n\nHow should the application be deployed while minimizing the number of resources\nto manage?""]",Create a separate API Gateway and separate Lambda function for each environment in the same Region.,Assign a Region for each environment and deploy API Gateway and Lambda to each Region.,Create one API Gateway with multiple stages with one Lambda function with multiple aliases.,"Create one API Gateway and one Lambda function, and use a REST parameter to identify the environment.","# Answer
- **Correct option:** C : Create one API Gateway with multiple stages with one Lambda function with multiple aliases.
- **Reason:** This approach minimizes the number of resources to manage by utilizing a single API Gateway that can handle different stages (development, test, and production) and a single Lambda function that can have multiple aliases pointing to different versions. Stages allow you to differentiate between environments without the overhead of managing multiple resources, thereby simplifying deployment and maintenance.

# Example / Analogy
- Think of a restaurant with one kitchen (the Lambda function) and different dining areas for various occasions (stages like development, test, and production). Instead of building multiple kitchens (Lambda functions) for each dining area (environment), the restaurant uses the same kitchen to prepare different dishes on different menus (versions of the Lambda function), serving customers in a way that suits the occasion.

# Common Mistakes / Traps
- A common mistake is to think that each environment must have completely separate resources, leading to increased complexity and cost. Developers may also overlook the flexibility of using stages and aliases, focusing instead on having distinct API Gateway instances.

# Memory Tip
- Use one API Gateway and Lambda function with stages to simplify environment management."
216,"[""A developer has built an application that inserts data into an Amazon DynamoDB\ntable. The table is configured to use provisioned capacity. The application is\ndeployed on a burstable nano Amazon EC2 instance. The application logs show that\nthe application has been failing because of a\nProvisionedThroughputExceededException error.\n\nWhich actions should the developer take to resolve this issue? (Choose two.)""]",Move the application to a larger EC2 instance.,Increase the number of read capacity units (RCUs) that are provisioned for the DynamoDB table.,Reduce the frequency of requests to DynamoDB by implementing exponential backoff.,Increase the frequency of requests to DynamoDB by decreasing the retry delay.,"# Answer
- **Correct options:** B, C
- **Reason:** 
  - Option B (Increase the number of read capacity units (RCUs) that are provisioned for the DynamoDB table) can help alleviate the ProvisionedThroughputExceededException by providing more capacity to handle incoming requests. By increasing the provisioned capacity, the application can handle more read and write traffic without failing.
  - Option C (Reduce the frequency of requests to DynamoDB by implementing exponential backoff) helps manage the retry logic in a situation where requests are exceeding the provisioned throughput. This reduces the load during peak times and prevents the application from overwhelming the DynamoDB table, which can help mitigate the error.

# Example / Analogy
- Think of the DynamoDB table like a restaurant with a limited number of tables (i.e., capacity). If too many diners (requests) show up at once and the tables are full (capacity exceeded), some customers will have to wait or leave. Increasing the number of tables (RCUs) allows more diners to be seated, while staggering reservations (exponential backoff) ensures that the restaurant doesn't get overwhelmed.

# Common Mistakes / Traps
- A common mistake is to assume that simply increasing the instance size (Option A) will solve the issue without addressing the underlying problem of provisioned capacity. Additionally, increasing the frequency of requests (Option D) is counterproductive and can worsen the situation.

# Memory Tip
- Always remember: Increase capacity or pace yourself to avoid exceeding limits!"
217,"[""Given the following AWS CloudFormation template:\n\n[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image14.png]\n\nWhat is the MOST efficient way to reference the new Amazon S3 bucket from\nanother AWS CloudFormation template?""]",Add an Export declaration to the Outputs section of the original template and use ImportValue in other templates.,Add Exported: true to the Content.Bucket in the original template and use ImportResource in other templates.,Create a custom AWS CloudFormation resource that gets the bucket name from the ContentBucket resource of the first stack.,Use Fn::Include to include the existing template in other templates and use the ContentBucket resource directly.,"# Answer
- **Correct option:** A: Add an Export declaration to the Outputs section of the original template and use ImportValue in other templates.

- **Reason:** By using the Export declaration in the Outputs section of the original CloudFormation template, you can make the S3 bucket name available for other CloudFormation stacks to reference. The `ImportValue` function allows other stacks to import this value efficiently and manage dependencies between stacks without needing to hard-code values or have other less efficient references.

# Example / Analogy
- Think of the Export and Import system like a library where the original template is a book that includes a summary in the back for important titles (exports) that other books (templates) can reference. When a new author (a different template) wants to refer to a certain title (like your S3 bucket), they can simply look up the summary provided in the ""library"" of original books rather than writing a new chapter (resource) from scratch.

# Common Mistakes / Traps
- A common mistake is confusing how to access resources in multiple templates. Some may think they can directly reference resources without using Imports/Exports or might consider complex custom resources when simpler solutions like `Export` and `ImportValue` are available.

# Memory Tip
- Remember: ""Export and Import are the keys to sharing values between CloudFormation stacks."""
218,"[""A developer registered an AWS Lambda function as a target for an Application\nLoad Balancer (ALB) using a CLI command. However, the Lambda function is not\nbeing invoked when the client sends requests through the ALB.\n\nWhy is the Lambda function not being invoked?""]",A Lambda function cannot be registered as a target for an ALB.,A Lambda function can be registered with an ALB using AWS Management Console only.,The permissions to invoke the Lambda function are missing.,Cross-zone is not enabled on the ALB.,"# Answer
- **Correct option:** A
- **Reason:** A Lambda function cannot be registered as a target for an ALB. While you can use ALB to route requests to Lambda functions, you must configure the ALB listeners and rules properly to trigger the Lambda, not register it as a traditional target.

# Example / Analogy
- Think of the ALB as a restaurant and the Lambda function as a chef. You can't simply tell the restaurant to register the chef as a part of the staff without specifying the order they will prepare. Instead, you need to establish the rules (like the menu) that dictate which requests lead to invoking that chef's services.

# Common Mistakes / Traps
- A common mistake is to assume that any resource can directly be registered as a target for an ALB without understanding the configuration requirements. Additionally, some might confuse the concept of routing requests with the idea of directly registering resources.

# Memory Tip
- Remember: ALB routes to Lambda via rules, not by direct registration!"
219,"[""A developer is creating an AWS Lambda function that will connect to an Amazon\nRDS for MySQL instance. The developer wants to store the database credentials.\nThe database credentials need to be encrypted and the database password needs to\nbe automatically rotated.\n\nWhich solution will meet these requirements?""]",Store the database credentials as environment variables for the Lambda function. Set the environment variables to rotate automatically.,Store the database credentials in AWS Secrets Manager. Set up managed rotation on the database credentials.,Store the database credentials in AWS Systems Manager Parameter Store as secure string parameters. Set up managed rotation on the parameters.,Store the database credentials in the X-Amz-Security-Token parameter. Set up managed rotation on the parameter.,"# Answer
- **Correct option:** B
- **Reason:** Storing the database credentials in AWS Secrets Manager allows for secure storage, encryption, and automatic rotation of the database credentials, meeting all the specified requirements.

# Example / Analogy
- Imagine you have a locker where you store your valuables. You can have a personal lock (encryption) to make sure no one can access it without your permission. Now, what if you have a service that not only keeps your valuables safe but also sends a locksmith every month to change your lock (automatic rotation)? That’s like using AWS Secrets Manager for storing database credentials.

# Common Mistakes / Traps
- A common mistake is thinking that environment variables or systems manager parameters can provide the same level of security and automatic rotation as AWS Secrets Manager. While they can store sensitive data, they lack built-in features for automatic rotation and do not enforce encryption automatically.

# Memory Tip
- Remember: **Secrets Manager = Secure storage + Automatic rotation.**"
220,"[""A developer wants to reduce risk when deploying a new version of an existing AWS\nLambda function. To test the Lambda function, the developer needs to split the\ntraffic between the existing version and the new version of the Lambda function.\n\nWhich solution will meet these requirements?""]",Configure a weighted routing policy in Amazon Route 53. Associate the versions of the Lambda function with the weighted routing policy.,Create a function alias. Configure the alias to split the traffic between the two versions of the Lambda function.,Create an Application Load Balancer (ALB) that uses the Lambda function as a target. Configure the ALB to split the traffic between the two versions of the Lambda function.,Create the new version of the Lambda function as a Lambda layer on the existing version. Configure the function to split the traffic between the two layers.,"# Answer
- **Correct option:** B
- **Reason:** Creating a function alias for the Lambda function allows you to point to specific versions of the function. By configuring the alias to split the traffic between the existing version and the new version, you can test the new version in production while minimizing risk. AWS Lambda supports traffic shifting using aliases out of the box, significantly simplifying the deployment process.

# Example / Analogy
- Think of the alias as a theater manager. When a new show (the new version of the Lambda function) is introduced, the manager can decide how many seats to fill with audience members interested in the new show, while still allowing attendees to enjoy the existing show. This way, the manager tests the new show with a portion of the audience and can gauge their reaction without fully committing to it yet.

# Common Mistakes / Traps
- A common mistake is to think that routing policies in Route 53 or Application Load Balancer configurations can directly split traffic between Lambda versions. While they are powerful tools, they don't manage Lambda version traffic splitting specifically, which needs to be handled by Lambda aliases.

# Memory Tip
- Remember: **Aliases are the traffic managers for your Lambda versions!**"
221,"[""A developer is troubleshooting an application in an integration environment. In\nthe application, an Amazon Simple Queue Service (Amazon SQS) queue consumes\nmessages and then an AWS Lambda function processes the messages. The Lambda\nfunction transforms the messages and makes an API call to a third-party service.\n\nThere has been an increase in application usage. The third-party API frequently\nreturns an HTTP 429 Too Many Requests error message. The error message prevents\na significant number of messages from being processed successfully.\n\nHow can the developer resolve this issue?""]",Increase the SQS event source’s batch size setting.,Configure provisioned concurrency for the Lambda function based on the third-party API’s documented rate limits.,Increase the retry attempts and maximum event age in the Lambda function’s asynchronous configuration.,Configure maximum concurrency on the SQS event source based on the third-party service’s documented rate limits.,"# Answer
- **Correct option:** D : Configure maximum concurrency on the SQS event source based on the third-party service’s documented rate limits.
- **Reason:** By configuring the maximum concurrency on the SQS event source, the developer can limit the number of concurrent Lambda executions that process messages from the SQS queue. This ensures that the API calls made to the third-party service do not exceed its rate limits, thereby reducing the occurrence of the HTTP 429 error.

# Example / Analogy
- Imagine a popular fast-food restaurant where only a limited number of cars can go through the drive-thru at any given time. If too many cars try to order at once, the restaurant cannot handle them, leading to delays and frustration. By controlling the number of cars allowed in the drive-thru, the restaurant ensures efficient service and a better experience for customers—similar to managing Lambda execution concurrency to prevent overwhelming the third-party API.

# Common Mistakes / Traps
- A common mistake is choosing options related to increasing batch sizes or concurrency settings that do not directly control the rate at which API calls are made. The developer might mistakenly think that these adjustments can alleviate the 429 error without recognizing the need for rate limiting aligned with the external API's constraints.

# Memory Tip
- Control the flow: Limit Lambda concurrency to match external API limits!"
222,"[""A company has a three-tier application that is deployed in Amazon Elastic\nContainer Service (Amazon ECS). The application is using an Amazon RDS for MySQL\nDB instance. The application performs more database reads than writes.\n\nDuring times of peak usage, the application’s performance degrades. When this\nperformance degradation occurs, the DB instance’s ReadLatency metric in Amazon\nCloudWatch increases suddenly.\n\nHow should a developer modify the application to improve performance?""]",Use Amazon ElastiCache to cache query results.,Scale the ECS cluster to contain more ECS instances.,Add read capacity units (RCUs) to the DB instance.,Modify the ECS task definition to increase the task memory.,"# Answer
- **Correct option:** A : Use Amazon ElastiCache to cache query results.
- **Reason:** Using Amazon ElastiCache to cache query results can significantly improve application performance by reducing the number of database reads. Since the application is experiencing increased read latency during peak times, caching frequently accessed data can alleviate pressure on the Amazon RDS instance, allowing it to serve write requests more efficiently.

# Example / Analogy
- Think of a library (Amazon RDS) where people go to find information. When many people are trying to find the same book (data) at once, it gets crowded, and it takes longer for everyone to get what they need. If the library had a separate small shelf (Amazon ElastiCache) with popular books already available, people could take the book from the shelf without waiting in line, greatly speeding up their experience.

# Common Mistakes / Traps
- A common mistake is to assume that simply scaling up the ECS cluster or DB instance will resolve performance issues. However, if the underlying problem is too many read requests hitting the database, scaling may only provide a temporary solution without addressing the root cause. 

# Memory Tip
- Use caching to lighten the load on databases during high traffic!"
223,"[""A developer is writing an application to encrypt files outside of AWS before\nuploading the files to an Amazon S3 bucket. The encryption must be symmetric and\nmust be performed inside the application.\n\nHow can the developer implement the encryption in the application to meet these\nrequirements?""]",Create a data key in AWS Key Management Service (AWS KMS). Use the AWS Encryption SDK to encrypt the files.,Create a Hash-Based Message Authentication Code (HMAC) key in AWS Key Management Service (AWS KMS). Use the AWS Encryption SDK to encrypt the files.,Create a data key pair in AWS Key Management Service (AWS KMS). Use the AWS CLI to encrypt the files.,Create a data key in AWS Key Management Service (AWS KMS). Use the AWS CLI to encrypt the files.,"# Answer
- **Correct option:** A : Create a data key in AWS Key Management Service (AWS KMS). Use the AWS Encryption SDK to encrypt the files.
  
- **Reason:** This option meets the requirement for symmetric encryption within the application as it utilizes a data key generated in AWS KMS to perform encryption. The AWS Encryption SDK provides a high-level interface for encrypting and decrypting data using that data key, making it suitable for developers looking to integrate encryption directly into their applications before uploading files to S3.

# Example / Analogy
- Real-life example or memorable comparison: Think of encryption like putting a letter into a locked box (the data key). You can create the box (data key) using a trusted locksmith (AWS KMS), and the AWS Encryption SDK acts as the mechanism to securely lock and unlock the box before putting it in a postal service (S3) for delivery.

# Common Mistakes / Traps
- A common mistake is choosing options that imply using authentication (like HMAC in option B) or using command-line interfaces (like options C and D), which do not fulfill the requirement of performing encryption within the application itself.

# Memory Tip
- To remember: ""Data key + Encryption SDK = In-App Encryption Solution."""
224,"[""A company has an online web application that includes a product catalog. The\ncatalog is stored in an Amazon S3 bucket that is named DOC-EXAMPLE-BUCKET. The\napplication must be able to list the objects in the S3 bucket and must be able\nto download objects through an IAM policy.\n\nWhich policy allows MINIMUM access to meet these requirements?""]",[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image15.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image16.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image17.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image18.png],"# Answer
- **Correct option:** A

- **Reason:** 
  The policy in choice A grants the necessary permissions to list objects (`s3:ListBucket`) in the bucket and allows the actions required to download the objects (`s3:GetObject`). Importantly, it specifies the actions only for a particular bucket and objects within it, adhering to the principle of least privilege by only allowing what is necessary.

# Example / Analogy
- Think of a library where you can only look at the catalog of books and borrow books that you're permitted to access. The library (S3 bucket) has a listing of all available books (objects), and you need specific permissions to see the catalog (list the objects) and to check out a book (download the object).

# Common Mistakes / Traps
- A common trap in such questions is to select a more permissive option that allows for actions not required (like batch operations or additional actions). It's essential to focus on permissions that meet only the stated requirements for minimum access.

# Memory Tip
- “List and Get: Just the right permissions to see and take.”"
225,"[""A developer created a web API that receives requests by using an internet-facing\nApplication Load Balancer (ALB) with an HTTPS listener. The developer configures\nan Amazon Cognito user pool and wants to ensure that every request to the API is\nauthenticated through Amazon Cognito.\n\nWhat should the developer do to meet this requirement?""]",Add a listener rule to the listener to return a fixed response if the Authorization header is missing. Set the fixed response to 401 Unauthorized.,Create an authentication action for the listener rules of the ALSet the rule action type to authenticate-cognito. Set the OnUnauthenticatedRequest field to “deny.”,Create an Amazon API Gateway API. Configure all API methods to be forwarded to the ALB endpoint. Create an authorizer of the COGNITO_USER_POOLS type. Configure every API method to use that authorizer.,Create a new target group that includes an AWS Lambda function target that validates the Authorization header by using Amazon Cognito. Associate the target group with the listener.,"# Answer
- **Correct option:** B
- **Reason:** The correct choice is to create an authentication action for the listener rules of the ALB, setting the rule action type to authenticate-cognito and the OnUnauthenticatedRequest field to ""deny."" This configuration ensures that the Application Load Balancer only forwards requests that are authenticated with Amazon Cognito, providing a secure gateway to the API.

# Example / Analogy
- Think of the ALB as a nightclub entrance. The authentication action is like a bouncer who checks IDs at the door. If someone doesn't have an ID (i.e., an Authorization header), the bouncer denies them entry (returns a 401 Unauthorized). This way, only verified customers (authenticated users) can get in.

# Common Mistakes / Traps
- A common mistake in this scenario is to assume that simply checking for the Authorization header or returning a fixed response is sufficient for security. Returning a fixed 401 status (Option A) does not provide the proper level of authentication; it only indicates an error without enforcing access control. Similarly, using API Gateway (Option C) may introduce unnecessary complexity if the ALB is already handling traffic. 

# Memory Tip
- Remember: ""Authenticate at the door – Only allow the VIPs in!"""
226,"[""A company recently deployed an AWS Lambda function. A developer notices an\nincrease in the function throttle metrics in Amazon CloudWatch.\n\nWhat are the MOST operationally efficient solutions to reduce the function\nthrottling? (Choose two.)""]",Migrate the function to Amazon Elastic Kubernetes Service (Amazon EKS).,Increase the maximum age of events in Lambda.,Increase the function’s reserved concurrency.,Add the lambda:GetFunctionConcurrency action to the execution role.,"# Answer
- **Correct options:** C and D

- **Reason:** 
  - **C:** Increasing the function’s reserved concurrency allows you to allocate a specific number of concurrent instances for your Lambda function. This ensures that the function can scale up to meet demand without throttling, as it guarantees the availability of a certain number of execution contexts.  
  - **D:** Adding the `lambda:GetFunctionConcurrency` action to the execution role helps provide proper permissions to change the concurrency settings if needed but does not directly address throttling. **This option is somewhat misleading since adjusting concurrency directly impacts throttling (option C), but this option does not actually reduce throttling itself. However, ensuring proper permissions for managing concurrency can make it operationally efficient.** 

# Example / Analogy
- Think of a Lambda function like a restaurant kitchen. If the kitchen has a fixed number of chefs (equivalent to concurrency), no matter how many orders come in (events), only that number of chefs can cook at the same time. By increasing the number of chefs (reserved concurrency), the kitchen can handle more orders simultaneously without making customers wait (throttling). The permissions (D) are like having a manager who can hire more chefs when needed but doesn’t cook himself.

# Common Mistakes / Traps
- A common mistake is to select option A (migrating to Amazon EKS) as a solution for Lambda function throttling, which is incorrect. EKS is meant for container orchestration, not for directly resolving Lambda throttling issues. Also, options concerning event age (B) don't directly influence throttling performance.

# Memory Tip
- ""Increase reserved concurrency to meet demand and avoid Lambda throttling."""
227,"[""A company is creating a REST service using an Amazon API Gateway with AWS Lambda\nintegration. The service must run different versions for testing purposes.\n\nWhat would be the BEST way to accomplish this?""]",Use an X-Version header to denote which version is being called and pass that header to the Lambda function(s).,Create an API Gateway Lambda authorizer to route API clients to the correct API version.,Create an API Gateway resource policy to isolate versions and provide context to the Lambda function(s).,Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context.,"# Answer
- **Correct option:** D: Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context.

- **Reason:** This option allows you to create separate stages in API Gateway for each version of your API. Each stage can have its own unique endpoint, allowing for easier management and testing of different versions. Additionally, stage variables can be used to pass specific configuration or context to the Lambda functions, making it a flexible solution for versioning.

# Example / Analogy
- Think of a multi-stage rocket. Each stage represents a different version of your API. Just like each stage can be launched independently for testing different payloads (or versions in this case), deploying versions as unique stages allows for clear separation and testing without affecting the other versions.

# Common Mistakes / Traps
- A common trap is to think that using headers or an authorizer is sufficient for versioning. While those methods can work, they complicate the API design and do not provide the clear separation that stages give.

# Memory Tip
- Remember: ""Stages are like lanes on a race track - each version runs separately without interference."""
228,"[""A company is using AWS CodePipeline to deliver one of its applications. The\ndelivery pipeline is triggered by changes to the main branch of an AWS\nCodeCommit repository and uses AWS CodeBuild to implement the test and build\nstages of the process and AWS CodeDeploy to deploy the application.\n\nThe pipeline has been operating successfully for several months and there have\nbeen no modifications. Following a recent change to the application’s source\ncode, AWS CodeDeploy has not deployed the updated application as expected.\n\nWhat are the possible causes? (Choose two.)""]",The change was not made in the main branch of the AWS CodeCommit repository.,One of the earlier stages in the pipeline failed and the pipeline has terminated.,One of the Amazon EC2 instances in the company’s AWS CodePipeline cluster is inactive.,The AWS CodePipeline is incorrectly configured and is not invoking AWS CodeDeploy.,"# Answer
- **Correct options:** A, B
- **Reason:**  
  - **A:** If the change was made in a branch other than the main branch, the pipeline would not trigger since it is configured to listen only for changes on the main branch. This means that CodeDeploy wouldn’t deploy the updated application as there was no new commit detected in the monitored branch.
  - **B:** If any stage earlier in the pipeline fails (e.g., the build or test stage), the pipeline stops running, and the later stages (including the deployment by CodeDeploy) do not get executed. Thus, if there was an error in those stages, the application won't be updated.

# Example / Analogy
- Think of a pipeline as a water delivery system in your home. If you've left the faucet closed (not made changes in the correct branch), or if there's a blockage somewhere in the system (like a stage failing), no water (or updated code) will reach your desired destination (the deployed application).

# Common Mistakes / Traps
- A common mistake is to assume that CodeDeploy will attempt a deployment regardless of the pipeline state. It's essential to remember that every step in the CodePipeline must succeed for the deployment to occur.

# Memory Tip
- ""Changes only trigger the pipeline if they flow from the main branch and all stages must succeed to deploy."""
229,"[""A developer is building a new application that will be deployed on AWS. The\ndeveloper has created an AWS CodeCommit repository for the application. The\ndeveloper has initialized a new project for the application by invoking the AWS\nCloud Development Kit (AWS CDK) cdk init command.\n\nThe developer must write unit tests for the infrastructure as code (IaC)\ntemplates that the AWS CDK generates. The developer also must run a validation\ntool across all constructs in the CDK application to ensure that critical\nsecurity configurations are activated.\n\nWhich combination of actions will meet these requirements with the LEAST\ndevelopment overhead? (Choose two.)""]",Use a unit testing framework to write custom unit tests against the cdk.out file that the AWS CDK generates. Run the unit tests in a continuous integration and continuous delivery (CI/CD) pipeline that is invoked after any commit to the repository.,Use the CDK assertions module to integrate unit tests with the application. Run the unit tests in a continuous integration and continuous delivery (CI/CD) pipeline that is invoked after any commit to the repository.,Use the CDK runtime context to set key-value pairs that must be present in the cdk.out file that the AWS CDK generates. Fail the stack synthesis if any violations are present.,Write a script that searches the application for specific key configuration strings. Configure the script to produce a report of any security violations.,"# Answer
- **Correct options:** B and D
- **Reason:** Option B leverages the CDK assertions module, which is specifically designed for unit testing AWS CDK constructs, resulting in minimal development overhead for creating and validating tests. It allows developers to efficiently assert the presence of required configurations or resources within the infrastructure. Option D is relevant for validating security configurations, allowing developers to write scripts that can automate the review process for key configuration strings. This combination ensures both unit testing and security validations with the least amount of additional overhead.

# Example / Analogy
- Think of building an architectural model (the infrastructure) and wanting to ensure it’s both structurally sound (unit tests with assertions) and complies with safety regulations (security validations through scripting). Just like you'd check both aspects before finalizing a building plan, using the CDK assertions module and a custom script allows you to verify both functionality and security in your cloud application.

# Common Mistakes / Traps
- A common mistake might be to overlook the specific tools available for the CDK, such as the assertions module, opting instead for more generic solutions like just unit testing the generated files (as suggested in option A). This can lead to more complexity and unnecessary overhead.

# Memory Tip
- Use CDK assertions for testing constructs and scripts for security checks to simplify your development process."
230,"[""A company deploys a photo-processing application to an Amazon EC2 instance. The\napplication needs to process each photo in less than 5 seconds. If processing\ntakes longer than 5 seconds, the company’s development team must receive a\nnotification.\n\nHow can a developer implement the required time measurement and notification\nwith the LEAST operational overhead?""]","Create an Amazon CloudWatch custom metric. Each time a photo is processed, publish the processing time as a metric value. Create a CloudWatch alarm that is based on a static threshold of 5 seconds. Notify the development team by using an Amazon Simple Notification Service (Amazon SNS) topic.","Create an Amazon Simple Queue Service (Amazon SQS) queue. Each time a photo is processed, publish the processing time to the queue. Create an application to consume from the queue and to determine whether any values are more than 5 seconds. Notify the development team by using an Amazon Simple Notification Service (Amazon SNS) topic.","Create an Amazon CloudWatch custom metric. Each time a photo is processed, publish the processing time as a metric value. Create a CloudWatch alarm that enters ALARM state if the average of values is greater than 5 seconds. Notify the development team by sending an Amazon Simple Email Service (Amazon SES) message.","Create an Amazon Kinesis data stream. Each time a photo is processed, publish the processing time to the data stream. Create an Amazon CloudWatch alarm that enters ALARM state if any values are more than 5 seconds. Notify the development team by using an Amazon Simple Notification Service (Amazon SNS) topic.","# Answer
- **Correct option:** A

- **Reason:** Option A provides the least operational overhead by utilizing Amazon CloudWatch to create a custom metric for processing time. When the processing exceeds 5 seconds, a CloudWatch alarm can quickly notify the development team via Amazon SNS. This solution requires minimal infrastructure and no additional applications to monitor the processing times, making it efficient and straightforward.

# Example / Analogy
- Think of this like using a stopwatch to track the time it takes to bake a cake. If it takes longer than a set duration (5 seconds in this case), you have an alarm (like a timer) that alerts you. In this scenario, CloudWatch acts as both your stopwatch and timer, allowing you to efficiently monitor and receive notifications based on your specified time threshold.

# Common Mistakes / Traps
- A common trap is assuming that more complex solutions (like using SQS or Kinesis) will provide better monitoring or notification insights. However, they add unnecessary complexity and operational overhead for this straightforward requirement.

# Memory Tip
- ""Use CloudWatch to monitor time; alarms are your friends!"""
231,"[""A company is using AWS Elastic Beanstalk to manage web applications that are\nrunning on Amazon EC2 instances. A developer needs to make configuration\nchanges. The developer must deploy the changes to new instances only.\n\nWhich types of deployment can the developer use to meet this requirement?\n(Choose two.)""]",All at once,Immutable,Rolling,Blue/green,"# Answer
- **Correct option:** B (Immutable) and D (Blue/green)

- **Reason:**  
Both Immutable and Blue/green deployment strategies deploy changes to new instances while keeping the existing instances running. In Immutable deployment, new instances with the updated version are launched, and once they are running, traffic is switched to them. In Blue/green deployment, two separate environments (blue and green) exist; one is live (frontend) while the other is idle (back-end). Changes are made in the idle environment, and once validated, traffic is routed to it. This approach allows for smooth transitions and minimal downtime.

# Example / Analogy
- Imagine you are renovating a restaurant (the application). Instead of closing it down (disruption), you build a new dining area (new instances with updated code) next door while the original remains open. Once the new area is finished, you invite customers to the new space (traffic switch) smoothly, ensuring they have a great experience without interruptions.

# Common Mistakes / Traps
- A common mistake is to think that ""All at once"" (which deploys changes to all instances simultaneously) can meet the requirement since it does not involve deploying to new instances only, leading to potential downtimes. ""Rolling"" is also a misconception since it updates instances gradually, which might not comply with the request for new deployments only.

# Memory Tip
- Remember: Use Immutable or Blue/green for zero-downtime deployments on new instances!"
232,"[""A developer needs to use Amazon DynamoDB to store customer orders. The\ndeveloper’s company requires all customer data to be encrypted at rest with a\nkey that the company generates.\n\nWhat should the developer do to meet these requirements?""]",Create the DynamoDB table with encryption set to None. Code the application to use the key to decrypt the data when the application reads from the table. Code the application to use the key to encrypt the data when the application writes to the table.,Store the key by using AWS Key Management Service (AWS KMS). Choose an AWS KMS customer managed key during creation of the DynamoDB table. Provide the Amazon Resource Name (ARN) of the AWS KMS key.,Store the key by using AWS Key Management Service (AWS KMS). Create the DynamoDB table with default encryption. Include the kms:Encrypt parameter with the Amazon Resource Name (ARN) of the AWS KMS key when using the DynamoDB software development kit (SDK).,Store the key by using AWS Key Management Service (AWS KMS). Choose an AWS KMS AWS managed key during creation of the DynamoDB table. Provide the Amazon Resource Name (ARN) of the AWS KMS key.,"# Answer
- **Correct option:** B 
- **Reason:** In order to meet the requirement of encrypting customer data at rest with a key that the company generates, the developer should use AWS Key Management Service (AWS KMS) to store a customer managed key. By selecting this key when creating the DynamoDB table, the developer ensures that the data is encrypted correctly using the specific key controlled by the company.

# Example / Analogy
- Think of AWS KMS like a safe deposit box. If you have a key that only your company controls (customer managed key), you can safely store your valuable items (customer data) in the box (DynamoDB table). Unlike a shared key (AWS managed key), your custom key ensures that only you and your permitted users can access the contents of the box.

# Common Mistakes / Traps
- A common trap is choosing options that do not involve customer managed keys when the requirement clearly states that the company generates the key. Options involving AWS managed keys (like option D) do not allow the company full control, which is against the requirement.

# Memory Tip
- Remember: **""Control your keys, control your data.""**  Use AWS KMS customer managed keys for unique encryption needs."
233,"[""A company uses AWS CloudFormation to deploy an application that uses an Amazon\nAPI Gateway REST API with AWS Lambda function integration. The application uses\nAmazon DynamoDB for data persistence. The application has three stages:\ndevelopment, testing, and production. Each stage uses its own DynamoDB table.\n\nThe company has encountered unexpected issues when promoting changes to the\nproduction stage. The changes were successful in the development and testing\nstages. A developer needs to route 20% of the traffic to the new production\nstage API with the next production release. The developer needs to route the\nremaining 80% of the traffic to the existing production stage. The solution must\nminimize the number of errors that any single customer experiences.\n\nWhich approach should the developer take to meet these requirements?""]",Update 20% of the planned changes to the production stage. Deploy the new production stage. Monitor the results. Repeat this process five times to test all planned changes.,Update the Amazon Route 53 DNS record entry for the production stage API to use a weighted routing policy. Set the weight to a value of 80. Add a second record for the production domain name. Change the second routing policy to a weighted routing policy. Set the weight of the second policy to a value of 20. Change the alias of the second policy to use the testing stage API.,"Deploy an Application Load Balancer (ALB) in front of the REST API. Change the production API Amazon Route 53 record to point traffic to the ALB. Register the production and testing stages as targets of the ALB with weights of 80% and 20%, respectively.",Configure canary settings for the production stage API. Change the percentage of traffic directed to canary deployment to 20%. Make the planned updates to the production stage. Deploy the changes,"# Answer
- **Correct option:** D 
- **Reason:** This option utilizes AWS API Gateway's canary deployment feature, allowing the developer to route a specific percentage (20%) of traffic to a new version of the API while the remaining traffic (80%) continues to hit the existing production stage. This minimizes errors for users since only a subset of traffic is directed toward the new changes, enabling monitoring and testing before a full rollout.

# Example / Analogy
- Think of a restaurant introducing a new dish. Instead of offering it to every customer right away, the chef first serves the new dish to 20% of the customers while 80% continue to order their regular meals. This allows the chef to gather feedback and make adjustments based on initial reactions before officially adding the new dish to the menu.

# Common Mistakes / Traps
- A common mistake is to think DNS-based routing (like option B) is sufficient for canary deployment. While DNS can route traffic, it lacks the integration and feedback mechanism of canary deployments—if something goes wrong, users may already face issues before the developer can react.

# Memory Tip
- **""Canary deployments keep the majority safe while testing the new changes.""**"
234,"[""A developer has created a data collection application that uses Amazon API\nGateway, AWS Lambda, and Amazon S3. The application’s users periodically upload\ndata files and wait for the validation status to be reflected on a processing\ndashboard. The validation process is complex and time-consuming for large files.\n\nSome users are uploading dozens of large files and have to wait and refresh the\nprocessing dashboard to see if the files have been validated. The developer must\nrefactor the application to immediately update the validation result on the\nuser’s dashboard without reloading the full dashboard.\n\nWhat is the MOST operationally efficient solution that meets these requirements?""]",Integrate the client with an API Gateway WebSocket API. Save the user-uploaded files with the WebSocket connection ID. Push the validation status to the connection ID when the processing is complete to initiate an update of the user interface.,"Launch an Amazon EC2 micro instance, and set up a WebSocket server. Send the user-uploaded file and user detail to the EC2 instance after the user uploads the file. Use the WebSocket server to send updates to the user interface when the uploaded file is processed.","Save the user’s email address along with the user-uploaded file. When the validation process is complete, send an email notification through Amazon Simple Notification Service (Amazon SNS) to the user who uploaded the file.",Save the user-uploaded file and user detail to Amazon DynamoDB. Use Amazon DynamoDB Streams with Amazon Simple Notification Service (Amazon SNS) push notifications to send updates to the browser to update the user interface.,"# Answer
- **Correct option:** A
- **Reason:** Integrating the client with an API Gateway WebSocket API allows for real-time communication. When a user uploads a file, the connection ID is stored, and updates can be pushed to this connection when the validation process is complete. This eliminates the need for users to refresh their dashboard for updates, providing an instant and operationally efficient solution.

# Example / Analogy
- Think of it like a push notification on your smartphone. Instead of checking your email continuously (like refreshing a dashboard), you receive a notification as soon as the validation is complete, allowing you to react immediately without manual effort.

# Common Mistakes / Traps
- A common trap is to choose options that do not provide real-time updates (like option C), assuming that asynchronous notifications (like emails) are sufficient for user experience. Users prefer immediate feedback over delays.

# Memory Tip
- ""WebSockets are like instant messages; they keep the conversation going without refreshing."""
235,"[""A company’s developer is creating an application that uses Amazon API Gateway.\nThe company wants to ensure that only users in the Sales department can use the\napplication. The users authenticate to the application by using federated\ncredentials from a third-party identity provider (IdP) through Amazon Cognito.\nThe developer has set up an attribute mapping to map an attribute that is named\nDepartment and to pass the attribute to a custom AWS Lambda authorizer.\n\nTo test the access limitation, the developer sets their department to\nEngineering in the IdP and attempts to log in to the application. The developer\nis denied access. The developer then updates their department to Sales in the\nIdP and attempts to log in. Again, the developer is denied access. The developer\nchecks the logs and discovers that access is being denied because the\ndeveloper’s access token has a department value of Engineering.\n\nWhich of the following is a possible reason that the developer’s department is\nstill being reported as Engineering instead of Sales?""]",Authorization caching is enabled in the custom Lambda authorizer.,Authorization caching is enabled on the Amazon Cognito user pool.,The IAM role for the custom Lambda authorizer does not have a Department tag.,The IAM role for the Amazon Cognito user pool does not have a Department tag.,"# Answer
- **Correct option:** A : Authorization caching is enabled in the custom Lambda authorizer.
- **Reason:** Authorization caching in the custom Lambda authorizer means that once an authorization decision is made, the results are stored for a certain period. If the developer had previously authenticated with the value of ""Engineering"" as their department, that decision would remain cached, and subsequent authentication attempts would not re-evaluate the updated department value of ""Sales.""

# Example / Analogy
- Imagine a bouncer at a club who checks IDs (your department in this case) and then writes down who is allowed in (caching the decision). If the bouncer sees your ID showing ""Engineering"" and lets you in, they might not check your ID again for a while. Even if you later change your ID to show ""Sales,"" the bouncer still believes you are from ""Engineering"" because that was cached feedback, preventing you from entering again.

# Common Mistakes / Traps
- A common mistake is assuming that changes in the identity provider will immediately reflect in the authorization logic. Many don't take into account that caching mechanisms exist, which can lead to confusion about why updates aren't recognized right away.

# Memory Tip
- Remember: ""Cached decisions stick; update may not reflect."""
236,"[""A company has migrated an application to Amazon EC2 instances. Automatic scaling\nis working well for the application user interface. However, the process to\ndeliver shipping requests to the company’s warehouse staff is encountering\nissues. Duplicate shipping requests are arriving, and some requests are lost or\narrive out of order.\n\nThe company must avoid duplicate shipping requests and must process the requests\nin the order that the requests arrive. Requests are never more than 250 KB in\nsize and take 5-10 minutes to process. A developer needs to rearchitect the\napplication to improve the reliability of the delivery and processing of the\nrequests.\n\nWhat should the developer do to meet these requirements?""]",Create an Amazon Kinesis Data Firehose delivery stream to process the requests. Create an Amazon Kinesis data stream. Modify the application to write the requests to the Kinesis data stream.,Create an AWS Lambda function to process the requests. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the Lambda function to the SNS topic. Modify the application to write the requests to the SNS topic.,Create an AWS Lambda function to process the requests. Create an Amazon Simple Queue Service (Amazon SQS) standard queue. Set the SQS queue as an event source for the Lambda function. Modify the application to write the requests to the SQS queue.,Create an AWS Lambda function to process the requests. Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the SQS queue as an event source for the Lambda function. Modify the application to write the requests to the SQS queue.,"# Answer
- **Correct option:** D
- **Reason:** An Amazon SQS FIFO (First-In-First-Out) queue ensures that the requests are processed in the exact order they arrive, which is crucial for the shipping process. It also avoids duplicate messages, addressing the issue of duplicate requests. By integrating the FIFO queue with an AWS Lambda function, the system can handle the requests seamlessly while maintaining both order and uniqueness.

# Example / Analogy
- Imagine a restaurant where orders are taken at the counter and served in the exact order they were placed. If two customers order the same dish at the same time, the kitchen prepares the dish just once, avoiding confusion and ensuring that customers receive their meals in the correct sequence.

# Common Mistakes / Traps
- A common mistake is to choose an SQS standard queue (Choice C), which does not guarantee the order of message delivery and can result in duplicates. Many might also overlook the need for processing in strict order when thinking only about scalability or the speed of delivery.

# Memory Tip
- Remember: ""FIFO queues keep it orderly and unique!"""
237,"[""A developer is creating a machine learning (ML) pipeline in AWS Step Functions\nthat contains AWS Lambda functions. The developer has configured an Amazon\nSimple Queue Service (Amazon SQS) queue to deliver ML model parameters to the ML\npipeline to train ML models. The developer uploads the trained models are\nuploaded to an Amazon S3 bucket.\n\nThe developer needs a solution that can locally test the ML pipeline without\nmaking service integration calls to Amazon SQS and Amazon S3.\n\nWhich solution will meet these requirements?""]",Use the Amazon CodeGuru Profiler to analyze the Lambda functions used in the AWS Step Functions pipeline.,Use the AWS Step Functions Local Docker Image to run and locally test the Lambda functions.,Use the AWS Serverless Application Model (AWS SAM) CLI to run and locally test the Lambda functions.,Use AWS Step Functions Local with mocked service integrations.,"# Answer
- **Correct option:** D: Use AWS Step Functions Local with mocked service integrations.

- **Reason:** AWS Step Functions Local allows developers to run state machines and local Lambda functions without invoking actual AWS services. This enables the developer to test the ML pipeline, including the logic flow and transitions, without making calls to Amazon SQS or Amazon S3, thus reducing costs and speeding up the development process.

# Example / Analogy
- Think of it like a rehearsal for a play. Actors can practice their lines and actions without the actual audience or set in place. Similarly, by using AWS Step Functions Local with mocked service integrations, developers can test their workflows without interacting with live resources, ensuring they are ready for the actual performance.

# Common Mistakes / Traps
- A common mistake is choosing options that do not emphasize the requirement of mocking service integrations, such as options that involve running code in the cloud (like AWS SAM CLI), which would still involve live services.

# Memory Tip
- Remember: ""Mock it before you rock it!""—test locally to ensure your flow is correct before deploying to AWS."
238,"[""A developer has created a large AWS Lambda function. Deployment of the function\nis failing because of an InvalidParameterValueException error. The error message\nindicates that the unzipped size of the function exceeds the maximum supported\nvalue.\n\nWhich actions can the developer take to resolve this error? (Choose two.)""]",Submit a quota increase request to AWS Support to increase the function to the required size.,Use a compression algorithm that is more efficient than ZIP.,Break up the function into multiple smaller functions.,Zip the .zip file twice to compress the file more.,"# Answer
- **Correct options:** C and A

- **Reason:** 
   - **C** (Break up the function into multiple smaller functions) is correct because AWS Lambda has limits on the unzipped deployment package size, which is 250 MB for the uncompressed code. If a function exceeds this size, it is recommended to split the code into smaller, manageable pieces, which can facilitate better functionality and maintenance as well. 
   - **A** (Submit a quota increase request to AWS Support to increase the function to the required size) is technically correct, but it may not be feasible for Lambda’s maximum limits—as they are strict at 250 MB unzipped, and the request may not be successful. Generally, it's better to adapt the code to fit within existing limits.

# Example / Analogy
- Think of packing a suitcase. If your suitcase (the Lambda function) exceeds weight limits (unzip size limit), rather than asking the airport to increase the limit, a more practical solution is to divide your clothes into two smaller suitcases. This way, each suitcase fits within the limits while still containing all your important items.

# Common Mistakes / Traps
- A common mistake is to assume that just compressing the code more (like option B, using a more efficient compression algorithm) would solve the problem. However, AWS Lambda has specific limits on unzipped sizes, and effective code organization is necessary. Also, attempting to compress the file twice (like option D) will not help resolve the deployment size issue, as AWS Lambda evaluates the unzipped size upon upload.

# Memory Tip
- Think ""smaller pieces, bigger impact"" to remember to break down large functions rather than only trying to get around size limits."
239,"[""A company runs an application on Amazon EC2 instances. The EC2 instances open\nconnections to an Amazon RDS for SQL Server database. A developer needs to store\nand access the credentials and wants to automatically rotate the credentials.\nThe developer does not want to store the credentials for the database in the\ncode.\n\nWhich solution will meet these requirements in the MOST secure way?""]",Create an IAM role that has permissions to access the database. Attach the IAM role to the EC2 instances.,Store the credentials as secrets in AWS Secrets Manager. Create an AWS Lambda function to update the secrets and the database. Retrieve the credentials from Secrets Manager as needed.,Store the credentials in an encrypted text file in an Amazon S3 bucket. Configure the EC2 instance launch template to download the credentials from Amazon S3 as the instance launches. Create an AWS Lambda function to update the secrets and the database.,Store the credentials in an Amazon DynamoDB table. Configure an Amazon CloudWatch Events rule to invoke an AWS Lambda function to periodically update the secrets and database.,"# Answer
- **Correct option:** B
- **Reason:** Storing the credentials as secrets in AWS Secrets Manager allows for secure storage and automatic rotation of credentials without hardcoding them in your application code. Secrets Manager is specifically designed for managing sensitive information and integrates well with other AWS services, making it the best choice for this scenario.

# Example / Analogy
- Think of AWS Secrets Manager like a safe where you can store your most valuable information—similar to how a bank securely keeps your money. You can access it when needed (like retrieving credentials) and automate updates (like getting new secure codes or locks), ensuring that your valuables are always secure and up-to-date.

# Common Mistakes / Traps
- A common mistake is to consider storing credentials in S3 or DynamoDB. While these options are technically feasible, they lack the automated management and security features that AWS Secrets Manager offers, such as encryption, versioning, and automatic rotation.

# Memory Tip
- **Remember:** Secrets Manager = Secure storage + Automatic rotation = Best practice for sensitive information.
"
240,"A company has a web application that is hosted on Amazon EC2 instances. The EC2
instances are configured to stream logs to Amazon CloudWatch Logs. The company
needs to receive an Amazon Simple Notification Service (Amazon SNS) notification
when the number of application error messages exceeds a defined threshold within
a 5-minute period.

Which solution will meet these requirements?",Rewrite the application code to stream application logs to Amazon SNS. Configure an SNS topic to send a notification when the number of errors exceeds the defined threshold within a 5-minute period.,Configure a subscription filter on the CloudWatch Logs log group. Configure the filter to send an SNS notification when the number of errors exceeds the defined threshold within a 5-minute period.,Install and configure the Amazon Inspector agent on the EC2 instances to monitor for errors. Configure Amazon Inspector to send an SNS notification when the number of errors exceeds the defined threshold within a 5-minute period.,Create a CloudWatch metric filter to match the application error pattern in the log data. Set up a CloudWatch alarm based on the new custom metric. Configure the alarm to send an SNS notification when the number of errors exceeds the defined threshold within a 5-minute period.,"# Answer
- **Correct option:** D 
- **Reason:** Creating a CloudWatch metric filter allows you to analyze the log data and generate a custom metric based on specific criteria (in this case, application errors). By setting up a CloudWatch alarm on this custom metric, you can configure it to send an SNS notification when the defined threshold for errors is exceeded within a 5-minute window. This method directly connects log data with alerting mechanisms without needing to modify application code or over-complicate the architecture.

# Example / Analogy
- Imagine you own a bakery, and you want to know when you sell a certain number of loaves of bread in 5 minutes. Instead of counting loaves yourself, you could install a tally system that automatically counts every loaf sold and alerts you when you reach your target. This is similar to the CloudWatch metric filter and alarm—it automatically tracks application errors and alerts you when they exceed your threshold, so you can respond quickly.

# Common Mistakes / Traps
- A common mistake is choosing option A, which involves rewriting application code to send notifications directly via SNS. This is unnecessary because existing log data can be analyzed using CloudWatch, making the solution more efficient and keeping the application code clean.

# Memory Tip
- ""Use CloudWatch metrics for log alerts, no need to rewrite your app!"""
241,"A company is adopting serverless computing for some of its new services. A
development team needs to create a serverless infrastructure by using AWS
Serverless Application Model (AWS SAM). All infrastructure must be deployed by
using AWS CloudFormation templates.

What should the development team do to meet these requirements?",Add a Resources section to the CloudFormation templates that contains AWS::Lambda::Function resources.,Add a Mappings section to the CloudFormation templates that contains AWS::Serverless::Function and AWS::Serverless::API.,Add a Transform section to the CloudFormation templates. Use the AWS SAM syntax to define the resources.,Add a Parameters section to the CloudFormation templates that specifies the relevant AWS SAM Globals section.,"# Answer
- **Correct option:** C : Add a Transform section to the CloudFormation templates. Use the AWS SAM syntax to define the resources.  
- **Reason:** In order to utilize AWS SAM (Serverless Application Model) within CloudFormation, the `Transform` section is required. This section allows you to leverage AWS SAM's simplified syntax and capabilities for defining serverless resources, such as AWS Lambda functions and APIs, making the deployment process more straightforward and efficient. 

# Example / Analogy
- Think of the `Transform` section as a specialized tool that transforms your regular language into a serverless language that AWS understands better. Similar to how a translator helps people communicate by converting languages, the `Transform` section helps AWS CloudFormation understand and configure serverless applications using AWS SAM syntax. 

# Common Mistakes / Traps
- A common mistake is to assume that simply adding AWS SAM resource types directly into the `Resources` section will work without specifying the `Transform`. This can lead to template errors since CloudFormation will not recognize the AWS SAM resource types unless the `Transform` directive is included.

# Memory Tip
- Remember: **""Transform for SAM"" - always include the Transform section to enable serverless syntax!**"
242,"A developer is building an application that invokes AWS Lambda functions
asynchronously to process events. The developer notices that a Lambda function
fails to process some events at random times. The developer needs to investigate
the failed events and capture the events that the Lambda function fails to
process.

Which solution will meet these requirements?",Add an Amazon EventBridge rule for the Lambda function. Configure the EventBridge rule to react to failed events and to store the events in an Amazon DynamoDB table.,Configure the Lambda function with a dead-letter queue based in Amazon Kinesis. Update the Lambda function's execution role with the required permissions.,Configure the Lambda function with an Amazon Simple Queue Service (Amazon SQS) dead-letter queue. Update the Lambda function's execution role with the required permissions.,Configure the Lambda function with an Amazon Simple Queue Service (Amazon SQS) FIFO dead-letter queue. Update the Lambda function's execution role with the required permissions.,"# Answer
- **Correct option:** C

- **Reason:** Configuring the Lambda function with an Amazon Simple Queue Service (Amazon SQS) dead-letter queue allows you to capture and store events that fail to process. When a Lambda function fails after exhausting its retry attempts, the unprocessed event is sent to the specified SQS dead-letter queue. This makes it easy for developers to investigate and reprocess failed events later.

# Example / Analogy
- Think of a mail delivery system. If a letter cannot be delivered (perhaps the address is incorrect), the letter needs to be sent to a return-to-sender box (the dead-letter queue) instead of being thrown away. The post office can then review and resolve issues with the undelivered letters later, just as you would investigate failed events stored in your dead-letter queue.

# Common Mistakes / Traps
- One common mistake is to confuse the use of Amazon Kinesis or using EventBridge for dead-letter queues. While Kinesis can handle stream data and EventBridge is great for rules and event-driven architecture, neither is suitable for directly capturing Lambda failures like SQS does with its dead-letter queues.

# Memory Tip
- ""Dead-letter queues catch missed deliveries from Lambda."""
243,"A company has built a serverless application for its ecommerce website. The
application includes a REST API in Amazon API Gateway that invokes an AWS Lambda
function. The Lambda function processes data and stores the data in Amazon
DynamoDB table. The Lambda function calls a third-party stock application API to
process the order. After the ordered is processed, the Lambda function returns
an HTTP 200 status code with no body to the client.

During peak usage when the API calls exceeds a certain threshold, the
third-party stock application sometimes fails to process the data and responds
with error messages. The company needs a solution that will not overwhelm the
third-party stock application.

Which solution will meet these requirements?",Configure the REST API in API Gateway to write the requests directly into DynamoDB. Configure a DynamoDB intrinsic function to perform the transformation. Set up a DynamoDB stream to call the third-party stock application API with each new row. Delete the Lambda function.,Configure the REST API in API Gateway to write the requests directly into an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function with a reserved concurrency equal to the third-party stock application's threshold. Set Lambda function to process the messages from the SQS queue.,Configure the REST API in API Gateway to write the requests directly into an Amazon Simple Notification Service (Amazon SNS) topic. Configure the Lambda function with a provisioned concurrency equal to the third-party stock application's threshold. Set the Lambda function to process the messages from the SNS topic.,Configure the REST API in API Gateway to write the requests directly into Amazon Athena. Configure the transformation of the data by using SQL with multiple query result locations set up to point to the DynamoDB table and the third-party stock fulfilment application API. Delete the Lambda function.,"# Answer
- **Correct option:** B

- **Reason:** Option B suggests configuring the REST API in API Gateway to write requests directly into an Amazon Simple Queue Service (Amazon SQS) queue. This approach allows for decoupling the processing of requests and alleviates the pressure on the third-party stock application by delaying the invocations to the Lambda function according to the SQS queue's flow. Additionally, by setting a reserved concurrency on the Lambda function equal to the third-party stock application's threshold, it ensures that you won't overwhelm the external API, thereby meeting the requirements.

# Example / Analogy
- Imagine a busy restaurant where a set number of chefs can handle food orders at any given time. Instead of sending all the customer orders directly to the chefs (which could overwhelm them), the restaurant takes orders and places them into a ""waiting area"" (SQS). The chefs can then pick orders from this waiting area at a manageable pace, ensuring they can maintain quality without being swamped. Similarly, using SQS allows the Lambda function to process requests at a controlled rate, avoiding overwhelming the third-party service.

# Common Mistakes / Traps
- A common trap is to overlook the need for throttling when dealing with external APIs. Options A, C, and D either eliminate the Lambda function, incorrectly use services like SNS or Athena for this purpose, or introduce complexity without managing request loads correctly.

# Memory Tip
- Remember: Use **SQS** to **queue** orders and avoid overwhelming external APIs!"
244,"A company hosts its application on AWS. The application runs on an Amazon
Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. The
cluster runs behind an Application Load Balancer. The application stores data in
an Amazon Aurora database. A developer encrypts and manages database credentials
inside the application.

The company wants to use a more secure credential storage method and implement
periodic credential rotation.

Which solution will meet these requirements with the LEAST operational overhead?",Migrate the secret credentials to Amazon RDS parameter groups. Encrypt the parameter by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant AWS KMS permissions to access Amazon RDS.,Migrate the credentials to AWS Systems Manager Parameter Store. Encrypt the parameter by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager.,Migrate the credentials to ECS Fargate environment variables. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager.,Migrate the credentials to AWS Secrets Manager. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager by using keys.,"# Answer
- **Correct option:** D 

- **Reason:** AWS Secrets Manager is specifically designed for storing and managing secrets such as database credentials. It provides built-in features for encryption, access management, and, importantly, automatic secret rotation. By migrating the credentials to Secrets Manager, the application can securely retrieve the secrets with minimal operational overhead. 

# Example / Analogy
- Think of Secrets Manager as a secure vault for your valuable items (secrets like passwords). Just as a vault has strong locks and systems in place to ensure that only authorized persons can access or change what’s inside, Secrets Manager ensures that only authorized services and users can access the sensitive information you store (like database credentials). The added benefit is that the vault can automatically replace the items at regular intervals (automatic rotation), so you don’t have to remember to update them manually.

# Common Mistakes / Traps
- A common mistake is to choose a solution that requires more manual management or lacks integrated secret rotation features. Options like AWS Systems Manager Parameter Store (option B) may not provide the same ease of secret management and automatic rotation as Secrets Manager. Additionally, focusing solely on encryption without considering ease of access and management can lead to overlooking better solutions. 

# Memory Tip
- For managing secrets securely and with automatic rotation, ""Choose Secrets Manager, your vault for credentials."""
245,"A company has a mobile app. The app includes an Amazon API Gateway REST API that
invokes AWS Lambda functions. The Lambda functions process data from the app.

The company needs to test updated Lambda functions that have new features. The
company must conduct these tests with a subset of users before deployment. The
tests must not affect other users of the app.

Which solution will meet these requirements with the LEAST amount of operational
effort?",Create a new version of each Lambda function with a weighted alias. Configure a weight value for each version of the Lambda function. Update the new weighted alias Amazon Resource Name (ARN) in the REST API.,Create a new REST API in API Gateway. Set up a Lambda proxy integration to connect to multiple Lambda functions. Enable canary settings on the deployment stage. Specify a smaller percentage of API traffic to go to the new version of the Lambda function.,Create a new version of each Lambda function. Integrate a predefined canary deployment in AWS CodeDeploy to slowly shift the traffic to the new versions automatically.,Create a new REST API in API Gateway. Set up a Lambda non-proxy integration to connect to multiple Lambda functions. Specify the necessary parameters and properties in API Gateway. Enable canary settings on the deployment stage. Specify a smaller percentage of API traffic to go to the new version of the Lambda function.,"# Answer
- **Correct option:** A
- **Reason:** Creating a new version of each Lambda function with a weighted alias allows the company to control the traffic going to the new version with minimal operational effort. This means they can gradually test the new features with a subset of users without affecting others, and updating the REST API to use the new weighted alias is straightforward.

# Example / Analogy
- Consider it like a restaurant offering a new dish as a special. Instead of switching the whole menu at once, the restaurant can let customers know that only a fraction of them will get to try the new dish. If the feedback is good, they can increase the number of servings; if not, they can retract it quickly. Similarly, using a weighted alias allows for a staged rollout of the new Lambda function version.

# Common Mistakes / Traps
- A common mistake is to overlook the operational simplicity that weighted aliases provide compared to more complex setups like canary deployments with AWS CodeDeploy or creating new REST APIs. Some might assume a full deployment or new API is necessary, which increases the complexity.

# Memory Tip
- Remember: **Weighted aliases are the easy switch for gradual rollouts in Lambda.**"
246,"A developer works for a company that only has a single pre-production AWS
account with an AWS CloudFormation AWS Serverless Application Model (AWS SAM)
stack. The developer made changes to an existing AWS Lambda function specified
in the AWS SAM template and additional Amazon Simple Notification service
(Amazon SNS) topics.

The developer wants to do a one-time deploy of the changes to test if the
changes are working. The developer does not want to impact the existing
pre-production application that is currently being used by other team members as
part of the release pipeline.

Which solution will meet these requirements?",Use the AWS SAM CLI to package and deploy the SAM application to the pre-production AWS account. Specify the debug parameter.,Use the AWS SAM CLI to package and create a change set against the pre-production AWS account. Execute the change set in a new AWS account designated for a development environment.,Use the AWS SAM CLI to package and deploy the SAM application to a new AWS account designated for a development environment.,Update the CloudFormation stack in the pre-production account. Add a separate stage that points to a new AWS account designated for a development environment.,"# Answer
- **Correct option:** C
- **Reason:** This option allows the developer to deploy the changes to a new AWS account that is specifically designated for a development environment. This prevents any potential disruption to the existing pre-production application, ensuring that other team members can continue using it without interruption.

# Example / Analogy
- Think of this scenario like a chef testing a new recipe. Instead of making changes in the restaurant kitchen where customers are being served, the chef uses a separate test kitchen to try out new ingredients and techniques. This way, the main restaurant operations remain unaffected while the chef perfects the dish.

# Common Mistakes / Traps
- A common mistake is choosing options that suggest modifying the existing pre-production stack (like option D), as this introduces risk. Some might also overlook the importance of using separate environments for testing changes and might consider using the same account.

# Memory Tip
- Always test new features in a dedicated development environment to avoid disrupting live applications."
247,"A company runs its website on AWS. The company posts daily polls on its website
and publishes the poll results next day. The website stores user responses in an
Amazon DynamoDB table. After the poll results are published, the company does
not need to keep the user responses.

A developer needs to implement a solution that will automatically remove old
user responses from the DynamoDB table. The developer adds a new expiration_date
attribute to the DynamoDB table. The developer plans to use the expiration_date
attribute for the automation.

Which solution will meet these requirements with the LEAST development effort?",Create an AWS Lambda function to delete old user responses based on the expiration_date attribute. Create an Amazon EventBridge schedule to run the Lambda function daily.,Create an AWS Fargate task in Amazon Elastic Container Service (Amazon ECS) to delete old user responses based on the expiration_date attribute. Create an Amazon EventBridge schedule to run the Fargate task daily.,Create an AWS Glue job to delete old user responses based on the expiration_date attribute. Create an AWS Glue trigger schedule to run the job daily.,Enable TTL on the DynamoDB table and specify the expiration_date attribute. Expire old user responses by using DynamoDB TTL.,"# Answer
- **Correct option:** D : Enable TTL on the DynamoDB table and specify the expiration_date attribute. Expire old user responses by using DynamoDB TTL.

- **Reason:** Enabling TTL (Time to Live) on a DynamoDB table allows you to automatically delete items after a specified timestamp, defined by the expiration_date attribute. This built-in feature requires minimal development effort since it automatically handles the deletion process without the need for additional services such as Lambda, ECS, or AWS Glue.

# Example / Analogy
- Think of TTL like a day-old sandwich at a café that automatically gets removed from the display after a certain time. The café doesn't need staff to monitor and take it away; it's done automatically due to a preset policy.

# Common Mistakes / Traps
- A common mistake is underestimating the power of built-in features like TTL and opting for more complex solutions (like Lambda or Fargate) that require additional configuration and maintenance. Always consider if there’s a simpler solution available first.

# Memory Tip
- Remember: Enable TTL for automatic cleanup – it saves time and effort!"
248,"A developer is creating a simple proof-of-concept demo by using AWS
CloudFormation and AWS Lambda functions. The demo will use a CloudFormation
template to deploy an existing Lambda function. The Lambda function uses
deployment packages and dependencies stored in Amazon S3. The developer defined
an AWS::Lambda::Function resource in a CloudFormation template. The developer
needs to add the S3 bucket to the CloudFormation template.

What should the developer do to meet these requirements with the LEAST
development effort?",Add the function code in the CloudFormation template inline as the code property.,Add the function code in the CloudFormation template as the ZipFile property.,Find the S3 key for the Lambda function. Add the S3 key as the ZipFile property in the CloudFormation template.,Add the relevant key and bucket to the S3Bucket and S3Key properties in the CloudFormation template.,"# Answer
- **Correct option:** D
- **Reason:** The AWS::Lambda::Function resource in a CloudFormation template supports specifying the location of the deployment package in Amazon S3 using the S3Bucket and S3Key properties. This is the standard practice for deploying Lambda functions that use external code packages stored in S3, allowing you to keep your template clean and reference the pre-existing code directly.

# Example / Analogy
- Imagine you want to cook a meal using a recipe that requires ingredients stored in a pantry (your S3 bucket). Instead of writing down each ingredient (code), you simply reference its location in your pantry (by specifying S3Bucket and S3Key). This way, you don't have to duplicate effort and can quickly access what you need for your dish (deployment).

# Common Mistakes / Traps
- A common mistake would be to select A or B, thinking that inline code is easier or sufficient. However, inline code is not practical for existing Lambda functions with dependencies that are already in S3. Additionally, selecting C may lead to confusion because the ZipFile property is only for small inline code snippets, not for deployment packages stored in S3.

# Memory Tip
- Always use S3Bucket and S3Key for existing Lambda packages in CloudFormation."
249,"A developer is building a microservices-based application by using Python on AWS
and several AWS services. The developer must use AWS X-Ray. The developer views
the service map by using the console to view the service dependencies. During
testing, the developer notices that some services are missing from the service
map.

What can the developer do to ensure that all services appear in the X-Ray
service map?",Modify the X-Ray Python agent configuration in each service to increase the sampling rate.,Instrument the application by using the X-Ray SDK for Python. Install the X-Ray SDK for all the services that the application uses.,Enable X-Ray data aggregation in Amazon CloudWatch Logs for all the services that the application uses.,Increase the X-Ray service map timeout value in the X-Ray console.,"# Answer
- **Correct option:** B : Instrument the application by using the X-Ray SDK for Python. Install the X-Ray SDK for all the services that the application uses.

- **Reason:** To ensure that all services appear in the AWS X-Ray service map, each service must be instrumented with the X-Ray SDK. This involves explicitly integrating the SDK into the application's code to enable tracing for requests and responses. Without this instrumentation, X-Ray won’t have the necessary data to display those services in the service map.

# Example / Analogy
- Think of AWS X-Ray as a GPS tracking system for your car (your microservices). If you want to track all your stops (services), you need to install a GPS tracker in each car (service). If only some cars are equipped with trackers (instrumented), you’ll only see their paths on the map, missing the routes taken by the others.

# Common Mistakes / Traps
- A common mistake is to assume simply increasing the sampling rate or making configuration changes is enough. It is essential to actually instrument the code to enable tracing rather than just adjusting settings.

# Memory Tip
- **""Instrument to visualize!""** Remember that each service needs instrumentation for X-Ray to capture its data."
250,"A developer is building a containerized application on AWS. The application
communicates with a third-party service by using API keys. The developer needs a
secure way to store the API keys and pass the API keys to the containerized
application.

Which solutions will meet these requirements? (Choose two.)",Store the API keys as a SecureString parameter in AWS Systems Manager Parameter Store. Grant the application access to retrieve the value from Parameter Store.,Store the API keys in AWS CloudFormation templates by using base64 encoding. Pass the API keys to the application through container definition environment variables.,Add a new AWS CloudFormation parameter to the CloudFormation template. Pass the API keys to the application by using the container definition environment variables.,Embed the API keys in the application. Build the container image on-premises. Upload the container image to Amazon Elastic Container Registry (Amazon ECR).,"# Answer
- **Correct options:** A, C

- **Reason:**  
Option A is correct because storing API keys as a SecureString parameter in AWS Systems Manager Parameter Store ensures they are encrypted and can be accessed securely by the application when needed. This approach adheres to security best practices by preventing hardcoding sensitive information. Option C is also correct as adding a CloudFormation parameter to provide API keys allows for dynamic configuration of the application during deployment, ensuring the keys remain externalized from the code while still being supplied at runtime.

# Example / Analogy
- Think of AWS Systems Manager Parameter Store like a safe where you can securely keep important documents (API keys). By using a secure safe, you ensure that only authorized individuals or applications can access the contents, whereas hardcoding the keys is like putting them on a sticky note next to your computer—easy to access but also easily compromised.

# Common Mistakes / Traps
- A common mistake is choosing options that involve hardcoding sensitive data (like in option D) or passing keys directly through environment variables without proper management, which can expose them to potential security breaches. Option B, while using base64 encoding, does not provide real security and is still vulnerable to exposure since anyone can decode base64 easily.

# Memory Tip
- **""Secure the keys, don’t expose them!""**"
251,"A company runs a payment application on Amazon EC2 instances behind an
Application Load Balance. The EC2 instances run in an Auto Scaling group across
multiple Availability Zones. The application needs to retrieve application
secrets during the application startup and export the secrets as environment
variables. These secrets must be encrypted at rest and need to be rotated every
month.

Which solution will meet these requirements with the LEAST development effort?",Save the secrets in a text file and store the text file in Amazon S3. Provision a customer managed key. Use the key for secret encryption in Amazon S3. Read the contents of the text file and read the export as environment variables. Configure S3 Object Lambda to rotate the text file every month.,Save the secrets as strings in AWS Systems Manager Parameter Store and use the default AWS Key Management Service (AWS KMS) key. Configure an Amazon EC2 user data script to retrieve the secrets during the startup and export as environment variables. Configure an AWS Lambda function to rotate the secrets in Parameter Store every month.,Save the secrets as base64 encoded environment variables in the application properties. Retrieve the secrets during the application startup. Reference the secrets in the application code. Write a script to rotate the secrets saved as environment variables.,Store the secrets in AWS Secrets Manager. Provision a new customer master key. Use the key to encrypt the secrets. Enable automatic rotation. Configure an Amazon EC2 user data script to programmatically retrieve the secrets during the startup and export as environment variables.,"# Answer
- **Correct option:** D
- **Reason:** Storing secrets in AWS Secrets Manager is specifically designed for managing sensitive information like application secrets. It handles encryption at rest automatically, allows for easy rotation of secrets through built-in functionalities, and provides a straightforward way to retrieve these secrets during application startup. Using user data scripts with EC2 simplifies the process of exporting these secrets as environment variables.

# Example / Analogy
- Think of AWS Secrets Manager as a ""vault"" where you can store your important documents (secrets) securely. Not only is it locked (encrypted), but it also has a system that checks and refreshes those documents every month (automated rotation), so you never have to worry about outdated or compromised information.

# Common Mistakes / Traps
- A common mistake is underestimating the importance of using a solution that is designed for secrets management (like Secrets Manager). Solutions like storing secrets in S3 (Option A) or managing them manually (Option C) can lead to increased complexity, security risks, and operational overhead.

# Memory Tip
- Remember: **""Secrets Manager is a vault that rotates, while others might just be a box left locked.""**"
252,"A developer is building a serverless application that runs on AWS. The developer
wants to create an accelerated development workflow that deploys incremental
changes to AWS for testing. The developer wants to deploy the incremental
changes but does not want to fully deploy the entire application to AWS for
every code commit.

What should the developer do to meet these requirements?",Use the AWS Serverless Application Model (AWS SAM) to build the application. Use the sam sync command to deploy the incremental changes.,Use the AWS Serverless Application Model (AWS SAM) to build the application. Use the sam init command to deploy the incremental changes.,Use the AWS Cloud Development Kit (AWS CDK) to build the application. Use the cdk synth command to deploy the incremental changes.,Use the AWS Cloud Development Kit (AWS CDK) to build the application. Use the cdk bootstrap command to deploy the incremental changes.,"# Answer
- **Correct option:** A
- **Reason:** The AWS Serverless Application Model (AWS SAM) provides a streamlined way to build serverless applications. The `sam sync` command specifically allows developers to deploy only the changes made to their application without requiring a full deployment, which is ideal for an incremental development workflow. This feature significantly speeds up testing and development cycles by avoiding the overhead of re-deploying the entire application.

# Example / Analogy
- Think of a painter who doesn't need to repaint the entire canvas every time they want to change a single color. Instead, they only touch up sections that need work. Similarly, `sam sync` lets a developer modify and deploy just the portions of their application that have changed, saving time and resources.

# Common Mistakes / Traps
- A common mistake is confusing `sam init` with deploying changes. `sam init` is used to initialize a new serverless application, not for deploying changes. Also, it’s easy to think that all commands will always deploy the entire application, but specific commands like `sam sync` exist to facilitate incremental deployments.

# Memory Tip
- Remember: **""Sync to change, not full paint.""** Use `sam sync` for swift, incremental deployments!"
253,"A developer is building an application that will use an Amazon API Gateway API
with an AWS Lambda backend. The team that will develop the frontend requires
immediate access to the API endpoints to build the UI. To prepare the backend
application for integration, the developer needs to set up endpoints. The
endpoints need to return predefined HTTP status codes and JSON responses for the
frontend team. The developer creates a method for an API resource.

Which solution will meet these requirements?",Set the integration type to AWS_PROXY. Provision Lambda functions to return hardcoded JSON data.,Set the integration type to MOCK. Configure the method's integration request and integration response to associate a JSON responses with specific HTTP status codes.,Set the integration type to HTTP_PROXY. Configure API Gateway to pass all requests to an external placeholder API. which the team will build.,Set the integration type to MOCK. Use a method request to define HTTP status codes. Use an integration request to define JSON responses.,"# Answer
- **Correct option:** B

- **Reason:** Setting the integration type to MOCK allows the developer to simulate responses from the API Gateway without involving any backend resources such as Lambda functions. This setup enables the developer to define specific HTTP status codes and return JSON responses, which is exactly what the frontend team needs for their UI development. It’s a quick way to provide immediate access to endpoints without needing a functional backend yet.

# Example / Analogy
- Think of a mock API as a stage play where actors perform a script. The actors (API Gateway) present predefined lines (HTTP status codes and JSON responses) even if the actual backend (the real script) isn't ready. This lets the frontend team see and use the “performance” for their development while the backend is still being built.

# Common Mistakes / Traps
- A common mistake is choosing AWS_PROXY, thinking it's easier or quicker. However, AWS_PROXY sends requests to actual Lambda functions, which requires them to be coded and deployed, making it less suitable for quick mock responses required by the frontend team.

# Memory Tip
- Remember: **MOCK setups provide quick responses without a working backend.**"
254,"A company runs a serverless application on AWS. The application includes an AWS
Lambda function. The Lambda function processes data and stores the data in an
Amazon RDS for PostgreSQL database. A developer created a user credentials in
the database for the application.

The developer needs to use AWS Secrets Manager to manage the user credentials.
The password must to be rotated on a regular basis. The solution needs to ensure
that there is high availability and no downtime for the application during
secret rotation.

What should the developer do to meet these requirements?",Configure managed rotation with the single user rotation strategy.,Configure managed rotation with the alternating users rotation strategy.,Configure automatic rotation with the single user rotation strategy.,Configure automatic rotation with the alternating users rotation strategy.,"# Answer
- **Correct option:** B: Configure managed rotation with the alternating users rotation strategy.

- **Reason:** The alternating users rotation strategy allows for high availability by maintaining both the old and new credentials valid during the rotation process. It ensures that the application can seamlessly switch from the old password to the new one without any downtime, as both credentials are valid temporarily, reducing the risk of service disruption.

# Example / Analogy
- Imagine a scenario in a restaurant where a chef is accustomed to working with a specific set of tools (the old credentials). When a new set of tools (the new credentials) arrives, instead of disposing of the old tools immediately, the chef keeps both sets at hand. This way, the chef can gradually transition to the new tools while still being able to prepare dishes using the old ones if needed. This strategy ensures that dinner service continues uninterrupted!

# Common Mistakes / Traps
- A common mistake is assuming that all rotation strategies guarantee no downtime. Many may confuse the single user rotation strategy to be sufficient, but it can lead to downtime as the old user credential becomes invalid right after rotation. 

# Memory Tip
- **Think of “alternating” as “always ready” – two users meaning no downtime!**"
255,"A developer is building an application on AWS. The application includes an AWS
Lambda function that processes messages from an Amazon Simple Queue Service
(Amazon SQS) queue.

The Lambda function sometimes fails or times out. The developer needs to figure
out why the Lambda function fails to process some messages.

Which solution will meet these requirements with the LEAST operational overhead?",Increase the maximum timeout of the Lambda function to 15 minutes. Check the AWS CloudTrail event history for error details.,Increase the visibility timeout of the SQS queue. Check logs in Amazon CloudWatch Logs for error details.,Create a dead-letter queue. Configure the Lambda function to send the failed messages to the dead-letter queue.,Create an Amazon DynamoDB table. Update the Lambda function to send the failed messages to the DynamoDB table.,"# Answer
- **Correct option:** C

- **Reason:** Creating a dead-letter queue (DLQ) for the Lambda function allows it to handle failed messages without losing them. This means that when processing a message fails, the function automatically sends that message to the DLQ. With the DLQ, the developer can review all failed messages later in a separate queue, analyze the issues that caused failures, and take appropriate actions to debug the Lambda function's handling.

# Example / Analogy
- Think of a dead-letter queue like a lost and found box at a school. When a student can’t find their homework, instead of just getting rid of it, the teacher places it in the lost and found. Later, the student can check the box to retrieve their homework and find out what went wrong.

# Common Mistakes / Traps
- A common mistake is to assume that simply increasing timeouts or changing configurations of the SQS queue will resolve the core issue. These measures do not provide insights into the nature of the failures, whereas a DLQ gives direct visibility into what messages are failing.

# Memory Tip
- **Remember**: A dead-letter queue catches lost messages for review, helping you debug and fix failures efficiently."
256,"A developer needs to deploy an application in three AWS Regions by using AWS
CloudFormation. Each Region will use an AWS Elastic Beanstalk environment with
an Application Load Balancer (ALB). The developer wants to use AWS Certificate
Manager (ACM) to deploy SSL certificates to each ALB.

Which solution will meet these requirements?",Create a certificate in ACM in any one of the Regions. Import the certificate into the ALB that is in each Region.,Create a global certificate in ACM. Update the CloudFormation template to deploy the global certificate to each ALB.,Create a certificate in ACM in each Region. Import the certificate into the ALB for each Region.,Create a certificate in ACM in the us-east-1 Region. Update the CloudFormation template to deploy the certificate to each ALB.,"# Answer
- **Correct option:** C : Create a certificate in ACM in each Region. Import the certificate into the ALB for each Region.

- **Reason:** AWS Certificate Manager (ACM) manages certificates that are region-specific for AWS services. For an SSL certificate to be used with an ALB in a specific AWS Region, the certificate must be created in that same Region. Therefore, the correct approach is to create a certificate in ACM for each Region where the application will be deployed and then associate each certificate with the respective ALB.

# Example / Analogy
- Think of it like getting a driver’s license. You must apply for and obtain a driver’s license in the state where you reside. If you move to a different state, you can't use your old driver's license; you need to get a new one from the new state. Similarly, SSL certificates must be created in the specific region they will be used.

# Common Mistakes / Traps
- A common mistake is thinking that SSL certificates can be shared across regions or assuming a single certificate can apply to multiple regions. This misconception could lead to deployment issues.

# Memory Tip
- ""Certificates are regional: always create them where you deploy!"""
257,"A company built an online event platform. For each event, the company organizes
quizzes and generates leaderboards that are based on the quiz scores. The
company stores the leaderboard data in Amazon DynamoDB and retains the data for
30 days after an event is complete. The company then uses a scheduled job to
delete the old leaderboard data.

The DynamoDB table is configured with a fixed write capacity. During the months
when many events occur, the DynamoDB write API requests are throttled when the
scheduled delete job runs.

A developer must create a long-term solution that deletes the old leaderboard
data and optimizes write throughput.

Which solution meets these requirements?",Configure a TTL attribute for the leaderboard data.,Use DynamoDB Streams to schedule and delete the leaderboard data.,Use AWS Step Functions to schedule and delete the leaderboard data.,Set a higher write capacity when the scheduled delete job runs.,"# Answer
- **Correct option:** A : Configure a TTL attribute for the leaderboard data.
- **Reason:** Configuring a Time to Live (TTL) attribute for the leaderboard data allows DynamoDB to automatically delete items after a specified time period (30 days in this case). This solution optimizes write throughput because it eliminates the need for a scheduled job that can cause throttling during peak periods, as DynamoDB manages the deletion process in the background.

# Example / Analogy
- Real-life example or memorable comparison: Think of TTL as setting an expiration date on food. Just as you automatically throw away expired food to maintain space in your pantry without needing to plan a dedicated trash day, TTL automatically removes old data from DynamoDB without you needing to schedule and run a separate deletion job.

# Common Mistakes / Traps
- Common trap points, if any: A common mistake could be thinking that a scheduled delete job is the only solution. Candidates may overlook the efficiency and automation provided by TTL. Additionally, some may confuse TTL with DynamoDB Streams or AWS Step Functions, which are not built specifically for automatic data expiration.

# Memory Tip
- 1 quick tip: ""TTL is like a food expiration date—automatically clears out the old stuff!"""
258,"A company uses an AWS Lambda function that reads messages from an Amazon Simple
Queue Service (Amazon SQS) standard queue. The Lambda function makes an HTTP
call to a third-party API for each message. The company wants to ensure that the
Lambda function does not overwhelm the third-party API with more than two
concurrent requests.

Which solution will meet these requirements?",Configure a provisioned concurrency of two on the Lambda function.,Configure a batch size of two on the Amazon SQS event source mapping for the Lambda function.,Configure Lambda event filtering to process two messages from Amazon SQS at every invocations.,Configure a maximum concurrency of two on the Amazon SQS event source mapping for the Lambda function.,"# Answer
- **Correct option:** D : Configure a maximum concurrency of two on the Amazon SQS event source mapping for the Lambda function.
- **Reason:** Setting a maximum concurrency of two on the event source mapping directly limits the number of concurrent executions of the Lambda function that can be triggered by messages from the SQS queue. This ensures that only two requests at a time are sent to the third-party API, thus preventing overload.

# Example / Analogy
- Imagine a restaurant kitchen where chefs can only handle two orders at a time. If three or more orders come in, the restaurant won't accept more until one of the chefs has finished. This way, they ensure high quality and timely service, much like how the maximum concurrency setting in Lambda ensures the third-party API isn't overwhelmed with requests.

# Common Mistakes / Traps
- A common mistake is confusing ""provisioned concurrency"" with ""maximum concurrency."" Provisioned concurrency guarantees a certain number of instances running, while maximum concurrency limits how many can be running concurrently for a specific event source.

# Memory Tip
- **Limit requests like a busy kitchen: set a maximum to avoid overwhelming the chef!**"
259,"A company is using Amazon API Gateway to develop an API for its application on
AWS. A developer needs to test and generate API responses. Other teams are
required to test the API immediately.

What should the developer do to meet these requirements?",Set up a mock integration request in API Gateway. Configure the method's integration request and integration response to associate a response with a given status code.,Set up the request validators in the API's OpenAPI definition file. Import the OpenAPI definitions into API Gateway to test the API.,Set up a gateway response for the API in API Gateway. Configure response headers with hardcoded HTTP status codes and responses.,Set up a request parameter-based Lambda authorizer to control access to the API. Configure the Lambda function with the necessary mapping template.,"# Answer
- **Correct option:** A

- **Reason:** Setting up a mock integration request in API Gateway allows the developer to create simulated responses without needing to implement the backend logic immediately. This enables quick testing and immediate access for other teams to interact with the API, as they can see what responses to expect for given requests.

# Example / Analogy
- Imagine you're a chef preparing a new dish. Before the main ingredient (like the meat) arrives, you can still create a mock version of the dish with similar flavors using substitutes. This allows your team (the taste testers) to experience the dish and provide feedback before the final version is ready.

# Common Mistakes / Traps
- A common mistake may be to think that setting up a Lambda function for responses (Option D) might also suffice; however, it involves more complexity and is not necessary for immediate testing purposes. Also, some might confuse mock integration with more complex methods like API Gateway integration with AWS services, which aren't needed in this scenario.

# Memory Tip
- **Mock your API responses to speed up testing!**"
260,"A developer built an application that calls an external API to obtain data,
processes the data, and saves the result to Amazon S3. The developer built a
container image with all of the necessary dependencies to run the application as
a container.

The application runs locally and requires minimal CPU and RAM resources. The
developer has created an Amazon ECS cluster. The developer needs to run the
application hourly in Amazon Elastic Container Service (Amazon ECS).

Which solution will meet these requirements with the LEAST amount of
infrastructure management overhead?",Add a capacity provider to manage instances.,Add an Amazon EC2 instance that runs the application.,Define a task definition with an AWS Fargate launch type.,Create an Amazon ECS cluster and add the managed node groups feature to run the application.,"# Answer
- **Correct option:** C : Define a task definition with an AWS Fargate launch type.

- **Reason:** Using AWS Fargate for this application allows the developer to run the container without having to manage the underlying EC2 instances. Fargate automatically provisions the compute resources needed to run containers, which minimizes the operational overhead. Since the application requires minimal CPU and RAM, Fargate is a cost-effective and efficient solution for running it on a schedule, such as hourly.

# Example / Analogy
- Think of AWS Fargate like a ride-share service (like Uber or Lyft) for your containers. Instead of owning a vehicle (EC2 instances), you simply request a ride (compute resources) when you need it, and you don't worry about vehicle maintenance or fuel costs. This allows you to focus on your application rather than infrastructure.

# Common Mistakes / Traps
- A common mistake is overestimating the need for infrastructure management. Some may think they need to use EC2 instances (options B and D) without realizing that Fargate provides a simpler, scalable option with less hassle. Another trap is underestimating the flexibility and ease of Fargate for running containers on a schedule.

# Memory Tip
- Remember: Fargate = No servers to manage!"
261,"A company runs an application on AWS. The application stores data in an Amazon
DynamoDB table. Some queries are taking a long time to run. These slow queries
involve an attribute that is not the table's partition key or sort key.

The amount of data that the application stores in the DynamoDB table is expected
to increase significantly. A developer must increase the performance of the
queries.

Which solution will meet these requirements?",Increase the page size for each request by setting the Limit parameter to be higher than the default value. Configure the application to retry any request that exceeds the provisioned throughput.,Create a global secondary index (GSI). Set query attribute to be the partition key of the index.,"Perform a parallel scan operation by issuing individual scan requests. In the parameters, specify the segment for the scan requests and the total number of segments for the parallel scan.",Turn on read capacity auto scaling for the DynamoDB table. Increase the maximum read capacity units (RCUs).,"# Answer
- **Correct option:** B : Create a global secondary index (GSI). Set query attribute to be the partition key of the index.

- **Reason:** A global secondary index (GSI) allows you to query the data in a DynamoDB table using an alternative key. Since the slow queries involve an attribute that is not the table's primary key (the partition key or sort key), creating a GSI with the desired attribute as the partition key will significantly enhance the query performance. This is because the GSI grants you the ability to retrieve data based on that non-key attribute, facilitating faster lookups and reducing query times, especially as the amount of data grows.

# Example / Analogy
- Think of a library where books are organized only by author names (the primary key). If you want to find books by genre (a non-primary attribute), it would take a long time to search through all the shelves. However, if the library created a separate section where books are organized by genre (like a GSI), you could find them quickly without having to search through every shelf.

# Common Mistakes / Traps
- A common mistake is to assume that simply increasing the limits or read capacity will address the performance issue without restructuring how the data is accessed. Also, many might think that parallel scans will improve performance, but scans are generally less efficient than queries, especially on large datasets.

# Memory Tip
- **Remember:** Use a GSI for fast queries on non-key attributes!"
262,"A company is using Amazon API Gateway to invoke a new AWS Lambda function. The
company has Lambda function versions in its PROD and DEV environments. In each
environment, there is a Lambda function alias pointing to the corresponding
Lambda function version. API Gateway has one stage that is configured to point
at the PROD alias.

The company wants to configure API Gateway to enable the PROD and DEV Lambda
function versions to be simultaneously and distinctly available.

Which solution will meet these requirements?",Enable a Lambda authorizer for the Lambda function alias in API Gateway. Republish PROD and create a new stage for DEV. Create API Gateway stage variables for the PROD and DEV stages. Point each stage variable to the PROD Lambda authorizer to the DEV Lambda authorizer.,Set up a gateway response in API Gateway for the Lambda function alias. Republish PROD and create a new stage for DEV. Create gateway responses in API Gateway for PROD and DEV Lambda aliases.,Use an environment variable for the Lambda function alias in API Gateway. Republish PROD and create a new stage for development. Create API gateway environment variables for PROD and DEV stages. Point each stage variable to the PROD Lambda function alias to the DEV Lambda function alias.,Use an API Gateway stage variable to configure the Lambda function alias. Republish PROD and create a new stage for development. Create API Gateway stage variables for PROD and DEV stages. Point each stage variable to the PROD Lambda function alias and to the DEV Lambda function alias.,"# Answer
- **Correct option:** D
- **Reason:** Option D correctly indicates the use of API Gateway stage variables to configure the Lambda function alias. By creating separate stages for PROD and DEV in API Gateway and using stage variables, the company can easily point to the respective Lambda function aliases for production and development environments simultaneously.

# Example / Analogy
- Imagine a restaurant that has two menus, one for regular customers (PROD) and another for special tasting events (DEV). By having two distinct menus (API Gateway stages) but using the same kitchen (Lambda function aliases), customers can choose what they want without issues, and the restaurant can manage culinary creations distinctly.

# Common Mistakes / Traps
- A common mistake is to confuse stage variables with environment variables or to think they work in the same way. Environment variables are typically used inside Lambda functions for configurations, while stage variables in API Gateway are not interchangeable with them.

# Memory Tip
- Remember: **“Stage variables in API Gateway = distinct paths for different environments!”**"
263,"A company runs a batch processing application by using AWS Lambda functions and
Amazon API Gateway APIs with deployment stages for development, user acceptance
testing, and production. A development team needs to configure the APIs in the
deployment stages to connect to third-party service endpoints.

Which solution will meet this requirement?",Store the third-party service endpoints in Lambda layers that correspond to the stage.,Store the third-party service endpoints in API Gateway stage variables that correspond to the stage.,Encode the third-party service endpoints as query parameters in the API Gateway request URL.,Store the third-party service endpoint for each environment in AWS AppConfig.,"# Answer
- **Correct option:** B : Store the third-party service endpoints in API Gateway stage variables that correspond to the stage.

- **Reason:** API Gateway stage variables are designed specifically for storing configuration values that can differ between deployment stages (like development, testing, and production). By using stage variables, you can easily switch between different third-party service endpoints based on the deployment stage without changing your API's code. This makes configuration management simpler and reduces the risk of errors.

# Example / Analogy
- Think of stage variables as different room setups for a conference. In one room (development), you might have a projector, while in another room (production), you might have a large screen. Each setup is tailored to its purpose, and you don't need to change the presentation itself; you simply choose a room that has the right setup.

# Common Mistakes / Traps
- A common mistake is to think that hardcoding service endpoints into the Lambda function or making them as query parameters is a good practice. This approach can lead to error-prone deployments since it requires code changes for environment transitions and can expose sensitive information in URLs.

# Memory Tip
- ""Use stage variables in API Gateway for clean environment configuration."""
264,"A developer is migrating an application to Amazon Elastic Kubernetes Service
(Amazon EKS). The developer migrates the application to Amazon Elastic Container
Registry (Amazon ECR) with an EKS cluster. As part of the application migration
to a new backend, the developer creates a new AWS account. The developer makes
configuration changes to the application to point the application to the new AWS
account and to use new backend resources. The developer successfully tests the
changes within the application by deploying the pipeline.

The Docker image build and the pipeline deployment are successful, but the
application is still connecting to the old backend. The developer finds that the
application's configuration is still referencing the original EKS cluster and
not referencing the new backend resources.

Which reason can explain why the application is not connecting to the new
resources?",The developer did not successfully create the new AWS account.,The developer added a new tag to the Docker image.,The developer did not update the Docker image tag to a new version.,The developer pushed the changes to a new Docker image tag.,"# Answer
- **Correct option:** C : The developer did not update the Docker image tag to a new version.

- **Reason:** The application is likely still using the previous version of the Docker image that references the old backend configuration. In Kubernetes, when you deploy a new version of a containerized application, you need to specify the new image tag that corresponds to the updated application code. If the tag remains unchanged, Kubernetes will continue to deploy the existing image, thus connecting to the old backend.

# Example / Analogy
- Imagine you are a chef in a restaurant, and you have two versions of a recipe: one for spaghetti with sauce A and one for spaghetti with sauce B. If you plan to serve the new spaghetti with sauce B but mistakenly use the old recipe meant for sauce A, the customers will receive the same old dish. Similarly, in this case, without updating the Docker image tag, the application continues to use the old ""recipe"" pointing to the previous backend.

# Common Mistakes / Traps
- A common misconception is assuming that changes in the application code or environment variables are sufficient to point to new resources. It's crucial to also ensure that the Docker image is updated with the correct tag that contains these changes.

# Memory Tip
- Always remember: **Update the Docker image tag to serve the latest recipe!**"
265,"A developer is creating an application that reads and writes to multiple Amazon
S3 buckets. The application will be deployed to an Amazon EC2 instance. The
developer wants to make secure API requests from the EC2 instances without the
need to manage the security credentials for the application. The developer needs
to apply the principle of least privilege.

Which solution will meet these requirements?",Create an IAM user. Create access keys and secret keys for the user. Associate the user with an IAM policy that allows s3:* permissions.,Associate the EC2 instance with an IAM role that has an IAM policy that allows s3:ListBucket and s3:*Object permissions for specific S3 buckets.,Associate the EC2 instance with an IAM role that has an AmazonS3FullAccess AWS managed policy.,Create a bucket policy on the S3 bucket that allows s3:ListBucket and s3:*Object permissions to the EC2 instance.,"# Answer
- **Correct option:** B
- **Reason:** Associating the EC2 instance with an IAM role allows the application to securely obtain temporary AWS credentials that grant the necessary permissions (like s3:ListBucket and s3:*Object) without hardcoding access keys or managing them. This approach adheres to the principle of least privilege by giving only the necessary permissions to the specific S3 buckets needed.

# Example / Analogy
- Imagine you are a guest at a hotel (the EC2 instance) that provides you with a temporary keycard (the IAM role) that only works for your room (the specific S3 buckets). You don’t have to worry about losing a permanent key or having to check-in and out all the time; your access is limited to what you need and is managed by the hotel.

# Common Mistakes / Traps
- A common mistake is to choose option A and create an IAM user with static access keys. This approach requires managing and rotating credentials, which is not secure and does not follow best practices. Option C grants excessive permissions using AmazonS3FullAccess, violating the least privilege principle. Option D incorrectly assumes bucket policies can replace IAM roles, which they cannot for instance-level permissions.

# Memory Tip
- Always use IAM roles for EC2 instances to avoid hardcoded credentials and ensure least privilege!"
266,"A developer is writing an application that will retrieve sensitive data from a
third-party system. The application will format the data into a PDF file. The
PDF file could be more than 1 MB. The application will encrypt the data to disk
by using AWS Key Management Service (AWS KMS). The application will decrypt the
file when a user requests to download it. The retrieval and formatting portions
of the application are complete.

The developer needs to use the GenerateDataKey API to encrypt the PDF file so
that the PDF file can be decrypted later. The developer needs to use an AWS KMS
symmetric customer managed key for encryption.

Which solutions will meet these requirements?",Write the encrypted key from the GenerateDataKey API to disk for later use. Use the plaintext key from the GenerateDataKey API and a symmetric encryption algorithm to encrypt the file.,Write the plain text key from the GenerateDataKey API to disk for later use. Use the encrypted key from the GenerateDataKey API and a symmetric encryption algorithm to encrypt the file.,Write the encrypted key from the GenerateDataKey API to disk for later use. Use the plaintext key from the GenerateDataKey API to encrypt the file by using the KMS Encrypt API.,Write the plain text key from the GenerateDataKey API to disk for later use. Use the encrypted key from the GenerateDataKey API to encrypt the file by using the KMS Encrypt API.,"# Answer
- **Correct option:** A
- **Reason:** The GenerateDataKey API call returns two keys: a plaintext data key and an encrypted copy of the data key. To securely encrypt the PDF file, you would use the plaintext key to perform the encryption directly. You then store the encrypted key on disk for later use to decrypt the data key when you need to access it. This ensures that the plaintext key, which is used to encrypt sensitive data, is never written to disk.

# Example / Analogy
- Imagine you have a very valuable jewelry box (the PDF file) that you want to lock. Instead of writing down the combination to the lock and leaving it next to the box (which would be like writing the plaintext key to disk), you write down the secret in a secure vault (storing the encrypted key). When you need to open the box, you can retrieve your combination from the vault (decrypt the key) and safely access your jewelry (the encrypted PDF).

# Common Mistakes / Traps
- A common mistake is to confuse which key to store and how to use them. Some may think that the plaintext key can be stored safely, but it should never be written to disk. Participants should focus on how the keys are intended to be used and secured.

# Memory Tip
- ""Encrypt with the key you hold, store the lock securely."""
267,"A company wants to test its web application more frequently. The company deploys
the application by using a separate AWS CloudFormation stack for each
environment. The company deploys the same CloudFormation template to each stack
as the application progresses through the development lifecycle.

A developer needs to build in notifications for the quality assurance (QA) team.
The developer wants the notifications to occur for new deployments in the final
preproduction environment.

Which solution will meet these requirements?",Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the QA team to the Amazon SNS topic. Update the CloudFormation stack options to point to the SNS topic in the pre-production environment.,Create an AWS Lambda function that notifies the QA team. Create an Amazon EventBridge rule to invoke the Lambda function on the default event bus. Filter the events on the CloudFormation service and on the CloudFormation stack Amazon Resource Name (ARN).,Create an Amazon CloudWatch alarm that monitors the metrics from CloudFormation. Filter the metrics on the stack name and the stack status. Configure the CloudWatch alarm to notify the QA team.,Create an AWS Lambda function that notifies the QA team. Configure the event source mapping to receive events from CloudFormation. Specify the filtering values to limit invocations to the desired CloudFormation stack.,"# Answer
- **Correct option:** A

- **Reason:**  
Option A is the most straightforward solution for sending notifications to the QA team upon deploying the CloudFormation stack in the pre-production environment. By creating an Amazon Simple Notification Service (Amazon SNS) topic and subscribing the QA team to it, the developer can configure the CloudFormation stack to publish notifications to that SNS topic whenever there is a new deployment. This keeps the process simple and directly tied to CloudFormation without additional complexity.

# Example / Analogy
- Real-life example or memorable comparison: Think of SNS as a public announcement system in a building. When a new office is opened (a new deployment), the building manager makes an announcement over the PA system that alerts everyone, including the QA team, about this new change. It’s an efficient and direct way to ensure everyone is informed.

# Common Mistakes / Traps
- Common trap points, if any: A common mistake would be to overcomplicate the solution by trying to use Lambda functions when simpler solutions like SNS notification can be directly integrated. Some may also overlook the need for clear communication to be established upon deployment, thinking more complex systems will suffice when a straightforward alert mechanism is available.

# Memory Tip
- 1 quick tip: ""Use SNS for simple notifications and keep it direct!"""
268,"A developer manages three AWS accounts. Each account contains an Amazon RDS DB
instance in a private subnet. The developer needs to define users in each
database in a consistent way. The developer must ensure that the same users are
created and updated later in all three accounts.

Which solution will meet these requirements with the MOST operational
efficiency?",Create an AWS CloudFormation template. Declare the users in the template. Attach the users to the database. Deploy the template in each account.,Create an AWS CloudFormation template that contains a custom resource to create the users in the database. Deploy the template in each account.,Write a script that creates the users. Deploy an Amazon EC2 instance in each account to run the script on the databases. Run the script in each account.,Implement an AWS Lambda function that creates the users in the database. Provide the function with the details of all three accounts.,"# Answer
- **Correct option:** B 
- **Reason:** Creating an AWS CloudFormation template that includes a custom resource to handle user creation in the database allows for consistent, repeatable deployment across different accounts. CloudFormation manages resources declaratively and can encapsulate the logic required to create and update users, ensuring that the same users are maintained properly in all instances.

# Example / Analogy
- Think of CloudFormation as a blueprint for building a house. If you want the same house in three different neighborhoods (accounts), you can simply share the same blueprint with a builder (CloudFormation). If you need to update something about the house (users), you only need to modify the blueprint, and the builders will know how to implement that change efficiently without needing to remember all the specific details of each house.

# Common Mistakes / Traps
- A common mistake would be to think that deploying scripts manually or through EC2 instances (options C and D) is sufficient. This approach introduces more operational overhead and inconsistency, as it relies on manual execution and could lead to errors or discrepancies across the accounts.

# Memory Tip
- Use CloudFormation for consistent resource management—it's your uniform blueprint for AWS resources!"
269,"A company is building a new application that runs on AWS and uses Amazon API
Gateway to expose APIs. Teams of developers are working on separate components
of the application in parallel. The company wants to publish an API without an
integrated backend so that teams that depend on the application backend can
continue the development work before the API backend development is complete.

Which solution will meet these requirements?",Create API Gateway resources and set the integration type value to MOCK. Configure the method integration request and integration response to associate a response with an HTTP status code. Create an API Gateway stage and deploy the API.,Create an AWS Lambda function that returns mocked responses and various HTTP status codes. Create API Gateway resources and set the integration type value to AWS_PROXY. Deploy the API.,Create an EC2 application that returns mocked HTTP responses. Create API Gateway resources and set the integration type value to AWS. Create an API Gateway stage and deploy the API.,Create API Gateway resources and set the integration type value set to HTTP_PROXY. Add mapping templates and deploy the API. Create an AWS Lambda layer that returns various HTTP status codes. Associate the Lambda layer with the API deployment.,"# Answer
- **Correct option:** A 
- **Reason:** Creating API Gateway resources with the integration type set to MOCK allows developers to simulate responses for their APIs without needing a functioning backend. This helps teams continue working and testing their components while the actual backend is still under development.

# Example / Analogy
- Think of it like building a movie set without the actors. You can create the scenery (API with mocked responses) to test how the camera angles (API calls) will work before the actors (backend services) arrive. This way, the film crew can work on lighting and scene setups while waiting for the performers.

# Common Mistakes / Traps
- A common mistake is to assume that we need to have a Lambda function or a specific backend setup to mock responses when API Gateway's built-in MOCK integration can efficiently fulfill the requirement without any additional setup.

# Memory Tip
- Remember: ""MOCK integration = build the set for the actors to come."""
270,"An application that runs on AWS receives messages from an Amazon Simple Queue
Service (Amazon SQS) queue and processes the messages in batches. The
application sends the data to another SQS queue to be consumed by another legacy
application. The legacy system can take up to 5 minutes to process some
transaction data.

A developer wants to ensure that there are no out-of-order updates in the legacy
system. The developer cannot alter the behavior of the legacy system.

Which solution will meet these requirements?",Use an SQS FIFO queue. Configure the visibility timeout value.,Use an SQS standard queue with a SendMessageBatchRequestEntry data type. Configure the DelaySeconds values.,Use an SQS standard queue with a SendMessageBatchRequestEntry data type. Configure the visibility timeout value.,Use an SQS FIFO queue. Configure the DelaySeconds value.,"# Answer
- **Correct option:** A: Use an SQS FIFO queue. Configure the visibility timeout value.

- **Reason:** A FIFO (First-In-First-Out) queue ensures that messages are processed in the exact order they are received. This is crucial for the legacy system that cannot process messages in parallel without risking out-of-order updates. The visibility timeout setting allows you to manage how long a message remains invisible to other consumers after being read, giving the legacy system ample time (up to 5 minutes) to process the message before it becomes visible again in the queue.

# Example / Analogy
- Imagine you’re at a ticket counter where people must be served in the order they arrived. If you take a number and wait, you are guaranteed that when your number is called, no one who arrived later will cut in line. Similarly, a FIFO queue processes messages in the exact order they are received, preventing confusion and ensuring that everyone (or in this case, every message) is handled properly without interruptions.

# Common Mistakes / Traps
- A common mistake is to assume that a standard SQS queue can handle ordered processing due to its ability to send messages in batches. Standard queues may lead to out-of-order delivery, which does not meet the requirements specified for the legacy application.

# Memory Tip
- Always remember: FIFO = First-In-First-Out, for ordered and reliable message processing!"
271,"A company is building a compute-intensive application that will run on a fleet
of Amazon EC2 instances. The application uses attached Amazon Elastic Block
Store (Amazon EBS) volumes for storing data. The Amazon EBS volumes will be
created at time of initial deployment. The application will process sensitive
information. All of the data must be encrypted. The solution should not impact
the application's performance.

Which solution will meet these requirements?",Configure the fleet of EC2 instances to use encrypted EBS volumes to store data.,Configure the application to write all data to an encrypted Amazon S3 bucket.,Configure a custom encryption algorithm for the application that will encrypt and decrypt all data.,Configure an Amazon Machine Image (AMI) that has an encrypted root volume and store the data to ephemeral disks.,"# Answer
- **Correct option:** A : Configure the fleet of EC2 instances to use encrypted EBS volumes to store data.
- **Reason:** Encrypted EBS volumes provide a way to secure data at rest while maintaining high performance. AWS manages encryption transparently without significantly affecting I/O performance, making it ideal for compute-intensive applications that process sensitive information.

# Example / Analogy
- Think of encrypted EBS volumes like locking your files in a safe while still being able to access them quickly. Just as you can open the safe and retrieve your documents without delay, EBS encryption allows your application to read and write data securely without introducing significant overhead.

# Common Mistakes / Traps
- A common mistake could be assuming that writing data to S3 (Option B) would be the best answer. While S3 does provide encryption, it introduces latency and would not be suitable for compute-intensive applications that require fast access to data stored directly on EBS volumes.

# Memory Tip
- Remember: ""Encrypted EBS is like a safe with quick access - secure and fast!"""
272,"A developer is updating the production version of an AWS Lambda function to fix
a defect. The developer has tested the updated code in a test environment. The
developer wants to slowly roll out the updates to a small subset of production
users before rolling out the changes to all users. Only 10% of the users should
be initially exposed to the new code in production.

Which solution will meet these requirements?","Update the Lambda code and create a new version of the Lambda function. Create a Lambda function trigger. Configure the traffic weights in the trigger between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version.","Create a new Lambda function that uses the updated code. Create a Lambda alias for the production Lambda function. Configure the Lambda alias to send 90% of the traffic to the production Lambda function, and send 10% of the traffic to the test Lambda function.","Update the Lambda code and create a new version of the Lambda function. Create a Lambda proxy integration. Configure the Lambda proxy to split traffic between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version.","Update the Lambda code and create a new version of the Lambda function. Create a Lambda function alias. Configure the traffic weights in the Lambda alias between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version.","# Answer
- **Correct option:** D
- **Reason:** Using a Lambda alias, you can manage different versions of a Lambda function and control traffic between them. This allows you to send 90% of the traffic to the stable production version and 10% to the new version. This method ensures that you can monitor the new code's behavior in the live environment while keeping most users on the older, more stable version.

# Example / Analogy
- Think of a restaurant introducing a new dish. Instead of serving it to everyone, they offer it to just 10% of customers initially to see how it performs. If it’s a hit, they gradually roll it out to more customers. Similarly, using Lambda aliases allows you to test new code with a small portion of users before full deployment.

# Common Mistakes / Traps
- A common mistake is choosing options that do not use the alias feature, such as A or B. These options create new Lambda functions or use triggers improperly, which can complicate the rollout process and reduce clarity and control over traffic management.

# Memory Tip
- **Remember:** Use Lambda aliases for safe traffic shifting between versions in production!"
273,"A developer is creating an AWS Lambda function that consumes messages from an
Amazon Simple Queue Service (Amazon SQS) standard queue. The developer notices
that the Lambda function processes some messages multiple times.

How should developer resolve this issue MOST cost-effectively?",Change the Amazon SQS standard queue to an Amazon SQS FIFO queue by using the Amazon SQS message deduplication ID.,Set up a dead-letter queue.,Set the maximum concurrency limit of the AWS Lambda function to 1.,Change the message processing to use Amazon Kinesis Data Streams instead of Amazon SQS.,"# Answer
- **Correct option:** A : Change the Amazon SQS standard queue to an Amazon SQS FIFO queue by using the Amazon SQS message deduplication ID. 
- **Reason:** Standard queues in Amazon SQS do not guarantee the order of messages and can deliver messages multiple times, especially in high-throughput scenarios. By switching to a FIFO (First-In-First-Out) queue, the developer can ensure that messages are processed in the exact order they arrive and utilize deduplication IDs to prevent duplicate processing of the same message.

# Example / Analogy
- Think of a FIFO queue like a line at a coffee shop; customers are served in the order they arrive. If you served customers out of order or allowed some to be served twice, it would create confusion. A FIFO queue ensures that each customer (message) is served exactly once and in the right order.

# Common Mistakes / Traps
- A common mistake is to think that using a dead-letter queue will solve the issue of duplicate message processing. While it helps with handling failures, it does not prevent duplicate delivery from happening in the first place.

# Memory Tip
- **""FIFO queues prevent the chaos of double servings!""**"
274,"A developer is optimizing an AWS Lambda function and wants to test the changes
in production on a small percentage of all traffic. The Lambda function serves
requests to a RE ST API in Amazon API Gateway. The developer needs to deploy
their changes and perform a test in production without changing the API Gateway
URL.

Which solution will meet these requirements?","Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. On the production API Gateway stage, define a canary release and set the percentage of traffic to direct to the canary release. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Publish the API to the canary stage.",Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Deploy a new API Gateway stage.,"Define an alias on the $LATEST version of the Lambda function. Update the API Gateway endpoint to reference the new Lambda function alias. Upload and publish the optimized Lambda function code. On the production API Gateway stage, define a canary release and set the percentage of traffic to direct to the canary release. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Publish to the canary stage.",Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Deploy the API to the production API Gateway stage.,"# Answer
- **Correct option:** C

- **Reason:** Option C outlines the correct steps to implement a canary release while maintaining traffic control without changing the API Gateway URL. It defines an alias for the Lambda function to point to the optimized version, which can then use the canary release feature of API Gateway. By using an alias, you can direct a specific percentage of traffic to the newly optimized function while keeping the old version intact for the remaining requests.

# Example / Analogy
- Think of this like trying a new flavor of ice cream in a shop. Instead of replacing the entire batch, the shop scoops out a small amount of the new flavor and allows customers to have a taste while still offering their popular flavors. This way, if the new flavor isn't well-received, they can easily revert back to just serving the old favorites.

# Common Mistakes / Traps
- A common mistake is to overlook the use of aliases and versions in Lambda functions. Many assume that updating to the latest version is enough, but this will redirect all traffic indiscriminately, which is not what the developer wants in a canary release. 

# Memory Tip
- ""Use an alias for gradual changes — it’s your safety net in production!"""
275,"A company notices that credentials that the company uses to connect to an
external software as a service (SaaS) vendor are stored in a configuration file
as plaintext.

The developer needs to secure the API credentials and enforce automatic
credentials rotation on a quarterly basis.

Which solution will meet these requirements MOST securely?",Use AWS Key Management Service (AWS KMS) to encrypt the configuration file. Decrypt the configuration file when users make API calls to the SaaS vendor. Enable rotation.,Retrieve temporary credentials from AWS Security Token Service (AWS STS) every 15 minutes. Use the temporary credentials when users make API calls to the SaaS vendor.,Store the credentials in AWS Secrets Manager and enable rotation. Configure the API to have Secrets Manager access.,Store the credentials in AWS Systems Manager Parameter Store and enable rotation. Retrieve the credentials when users make API calls to the SaaS vendor.,"# Answer
- **Correct option:** C
- **Reason:** Storing the API credentials in AWS Secrets Manager allows for secure storage, management, and automatic rotation of secrets. Secrets Manager is specifically designed for handling credentials and has built-in support for automatic rotation, ensuring that credentials are kept up-to-date without manual intervention.

# Example / Analogy
- Imagine you have a mailbox that you can open safely only when needed, and every month a key is automatically changed, ensuring that only authorized personnel can access it. AWS Secrets Manager works similarly; it securely manages your API keys and changes them automatically, so you don't have to worry about them being old or insecure.

# Common Mistakes / Traps
- A common mistake is to think that encrypting a plaintext file using AWS KMS will provide adequate security. While encryption is good, managing and rotating credentials through Secrets Manager is much more effective and secure in the context of secret management.

# Memory Tip
- Remember: **Secrets Manager = Secure storage + Automatic rotation = Peace of mind!**"
276,"A company has an application that is hosted on Amazon EC2 instances. The
application stores objects in an Amazon S3 bucket and allows users to download
objects from the S3 bucket. A developer turns on S3 Block Public Access for the
S3 bucket. After this change, users report errors when they attempt to download
objects. The developer needs to implement a solution so that only users who are
signed in to the application can access objects in the S3 bucket.

Which combination of steps will meet these requirements in the MOST secure way?
(Choose two.)",Create an EC2 instance profile and role with an appropriate policy. Associate the role with the EC2 instances.,Create an IAM user with an appropriate policy. Store the access key ID and secret access key on the EC2 instances.,Modify the application to use the S3 GeneratePresignedUrl API call.,Modify the application to use the S3 GetObject API call and to return the object handle to the user.,"# Answer
- **Correct options:** A, C 

- **Reason:** 
   - **Option A:** Creating an EC2 instance profile and role allows the EC2 instances to securely access AWS resources, such as S3, without needing to store access keys on the instance itself. This ensures that only authorized actions (like accessing S3 objects) can be performed by the application running on the EC2 instance.
   - **Option C:** Modifying the application to use the S3 GeneratePresignedUrl API call allows the application to generate temporary, time-limited URLs that users can use to download the objects. This approach ensures that only authenticated users (those signed into the application) can get the URL and access the S3 objects securely.

# Example / Analogy
Imagine a VIP concert where only ticket holders can enter. Instead of giving out permanent passes (like IAM users with access keys), the venue gives out temporary wristbands (presigned URLs) that allow access for just a few hours during the show (the expiration time). To get a wristband, attendees must prove they’re on the guest list (authenticating with the application).

# Common Mistakes / Traps
- A common mistake is choosing option B, which suggests using an IAM user's static credentials, compromising security by exposing access keys on the EC2 instance. This goes against best security practices.

# Memory Tip
- Remember: ""Use roles for EC2, presigned URLs for users."""
277,"An Amazon Simple Queue Service (Amazon SQS) queue serves as an event source for
an AWS Lambda function. In the SQS queue, each item corresponds to a video file
that the Lambda function must convert to a smaller resolution. The Lambda
function is timing out on longer video files, but the Lambda function's timeout
is already configured to its maximum value.

What should a developer do to avoid the timeouts without additional code
changes?",Increase the memory configuration of the Lambda function.,Increase the visibility timeout on the SQS queue.,Increase the instance size of the host that runs the Lambda function.,Use multi-threading for the conversion.,"# Answer
- **Correct option:** A: Increase the memory configuration of the Lambda function.

- **Reason:** Increasing the memory allocation for an AWS Lambda function not only provides more memory but also increases the CPU power available for that function. More memory and CPU can lead to faster processing times, which can help the Lambda function finish executing within its timeout limit even with longer video files. Since the function is already at its maximum timeout, optimizing resource allocation is the only possible solution.

# Example / Analogy
- Think of a Lambda function like a baker who can only bake a certain number of cakes at a time. If the baker is overwhelmed with large orders (longer video files), increasing the size of the oven (memory allocation) allows the baker to bake more cakes faster. This way, the baker completes all orders without exceeding their work time limit.

# Common Mistakes / Traps
- A common mistake is to overlook that simply increasing the timeout value will not solve the underlying performance issue if the function is already at its max timeout. Many might consider adjusting the SQS visibility timeout, which does not influence the Lambda execution time.

# Memory Tip
- Remember: ""More memory, more speed!"""
278,"A company is building an application on AWS. The application's backend includes
an Amazon API Gateway REST API. The company's frontend application developers
cannot continue work until the backend API is ready for integration. The company
needs a solution that will allow the frontend application developers to continue
their work.

Which solution will meet these requirements in the MOST operationally efficient
way?",Configure mock integrations for API Gateway API methods.,Integrate a Lambda function with API Gateway and return a mocked response.,Add new API endpoints to the API Gateway stage and returns a mocked response.,Configure a proxy resource for API Gateway API methods.,"# Answer
- **Correct option:** A : Configure mock integrations for API Gateway API methods.

- **Reason:** Mock integrations in API Gateway allow developers to create API endpoints that simulate responses from backend services without needing those services to be implemented. This enables frontend developers to continue their work asynchronously while the backend is being developed.

# Example / Analogy
- Think of a movie script where the actors need to rehearse their lines before the filming starts. By using a ""mock"" script that outlines potential dialogue (even if the actual script isn't ready), the actors can practice and prepare. In this context, a mock integration lets frontend developers ""practice"" making API calls to a simulated response, enabling them to proceed with development.

# Common Mistakes / Traps
- One common trap is choosing options that involve actual backend processes, such as integrating Lambda functions or adding endpoints that expect a real response. These options could create additional dependencies and are more complex than necessary when the goal is to simply allow frontend development to continue.

# Memory Tip
- To remember mock integrations: **""Mock first, build later!""**"
279,"A company is preparing to migrate an application to the company's first AWS
environment. Before this migration, a developer is creating a proof-of-concept
application to validate a model for building and deploying container-based
applications on AWS.

Which combination of steps should the developer take to deploy the containerized
proof-of-concept application with the LEAST operational effort? (Choose two.)",Package the application into a .zip file by using a command line tool. Upload the package to Amazon S3.,Package the application into a container image by using the Docker CLI. Upload the image to Amazon Elastic Container Registry (Amazon ECR).,Deploy the application to an Amazon EC2 instance by using AWS CodeDeploy.,Deploy the application to Amazon Elastic Kubernetes Service (Amazon EKS) on AWS Fargate.,"# Answer
- **Correct options:** B, D  
- **Reason:** 
  - Option B is correct because packaging the application into a container image using Docker allows the application to be deployed consistently across different environments. Uploading the image to Amazon ECR facilitates easy access for deployment.
  - Option D is correct because deploying the application to Amazon EKS on AWS Fargate means that the developer can run containers without needing to manage the underlying EC2 instances, reducing the operational effort significantly. Fargate handles the orchestration and scaling of the container, allowing the developer to focus on the application itself.

# Example / Analogy
- Imagine preparing a meal (the application) and using a meal kit service. Option B is like choosing pre-packaged ingredients (container image) that are sent to you (ECR), and option D is like having a chef (Fargate) who cooks the meal for you without you needing to worry about the kitchen tools or assistant cooks (EC2 instances). This way, you enjoy the meal with minimal effort in preparation and cleanup.

# Common Mistakes / Traps
- A common mistake is to think that deploying directly to EC2 or using CodeDeploy (options C) is the best approach. However, these methods typically involve more operational overhead as they require manual management of infrastructure. This question specifically looks for the **least operational effort**, making B and D the better choices.

# Memory Tip
- **Think containers over VMs for simplicity: Pack it (image) and serve it (Fargate)!**"
280,"A developer supports an application that accesses data in an Amazon DynamoDB
table. One of the item attributes is expirationDate in the timestamp format. The
application uses this attribute to find items, archive them, and remove them
from the table based on the timestamp value.

The application will be decommissioned soon, and the developer must find another
way to implement this functionality. The developer needs a solution that will
require the least amount of code to write.

Which solution will meet these requirements?",Enable TTL on the expirationDate attribute in the table. Create a DynamoDB stream. Create an AWS Lambda function to process the deleted items. Create a DynamoDB trigger for the Lambda function.,Create two AWS Lambda functions: one to delete the items and one to process the items. Create a DynamoDB stream. Use the DeleteItem API operation to delete the items based on the expirationDate attribute. Use the GetRecords API operation to get the items from the DynamoDB stream and process them.,Create two AWS Lambda functions: one to delete the items and one to process the items. Create an Amazon EventBridge scheduled rule to invoke the Lambda functions. Use the DeleteItem API operation to delete the items based on the expirationDate attribute. Use the GetRecords API operation to get the items from the DynamoDB table and process them.,Enable TTL on the expirationDate attribute in the table. Specify an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as the target to delete the items. Create an AWS Lambda function to process the items.,"# Answer
- **Correct option:** A

- **Reason:** Enabling TTL (Time to Live) on the `expirationDate` attribute allows DynamoDB to automatically delete items from the table when the timestamp value has passed, thus requiring the least amount of code to implement. Once TTL is enabled, DynamoDB handles the deletion without requiring any additional Lambda functions or custom code for periodic cleanup. The DynamoDB stream can capture any changes (deletions in this case) made to the items, which can then be processed if necessary using a Lambda function.

# Example / Analogy
- Think of TTL like food expiration dates on products in your pantry. Just as you might choose to throw out expired items automatically, DynamoDB automatically removes items when they ""expire"" based on the `expirationDate`. This means less effort is required to keep your pantry (or database) clean and organized!

# Common Mistakes / Traps
- A common trap is thinking you need to manage item deletions manually with Lambda functions or other complex solutions. People might overlook the built-in simplicity of TTL which can handle automatic deletions without extra code.

# Memory Tip
- Remember: **TTL automates expiration like food labels, saving time on manual cleanup!**"
281,"A developer needs to implement a custom machine learning (ML) library in an
application. The size of the library is 15 GB. The size of the library is
increasing. The application uses AWS Lambda functions. All the Lambda functions
must have access to the library.

Which solution will meet these requirements?",Save the library in Lambda layers. Attach the layers to all Lambda functions.,Save the library in Amazon S3. Download the library from Amazon S3 inside the Lambda function.,Save the library as a Lambda container image. Redeploy the Lambda functions with the new image.,Save the library in an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all the Lambda functions.,"# Answer
- **Correct option:** D : Save the library in an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all the Lambda functions.

- **Reason:** AWS Lambda has a size limit for deployment packages and Lambda layers, which is currently 10 MB for compressed files and 250 MB for uncompressed files. Given that the custom ML library is 15 GB, it exceeds these limits if directly saved as a layer or deployment package. Using Amazon EFS allows you to store large files, and it can be mounted directly to Lambda functions, providing them with access to the library without hitting the size constraints.

# Example / Analogy
- Think of Amazon EFS as a shared drive in a workplace. Just like multiple employees can access large files from a shared drive without needing to duplicate them on their individual computers (which may have limited storage), Lambda functions can access a large ML library stored on EFS without the storage limitations of their individual deployment packages.

# Common Mistakes / Traps
- A common mistake is to assume that Lambda layers can handle large libraries. Many forget that there are size limits on layers. Additionally, the option to use S3 may seem appealing for larger files, but it requires downloading the library at runtime, which may introduce latency and complexity.

# Memory Tip
- ""For large libraries, think EFS for shared access, not layers or packages!"""
282,"A developer is designing a serverless application for a game in which users
register and log in through a web browser. The application makes requests on
behalf of users to a set of AWS Lambda functions that run behind an Amazon API
Gateway HTTP API.

The developer needs to implement a solution to register and log in users on the
application's sign-in page. The solution must minimize operational overhead and
must minimize ongoing management of user identities.

Which solution will meet these requirements?",Create Amazon Cognito user pools for external social identity providers. Configure IAM roles for the identity pools.,Program the sign-in page to create users' IAM groups with the IAM roles attached to the groups.,Create an Amazon RDS for SQL Server DB instance to store the users and manage the permissions to the backend resources in AWS.,Configure the sign-in page to register and store the users and their passwords in an Amazon DynamoDB table with an attached IAM policy.,"# Answer
- **Correct option:** A

- **Reason:** Option A suggests creating Amazon Cognito user pools for user authentication and external social identity provider integration, which greatly simplifies registration and login processes. Amazon Cognito minimizes operational overhead by managing user identities and credentials, supporting features like user sign-up, sign-in, and multi-factor authentication (MFA) out of the box. This aligns with the requirement to minimize ongoing management of user identities, as Cognito handles these aspects effectively.

# Example / Analogy
- Imagine trying to manage a huge guest list for a party yourself: collecting names, tracking RSVPs, and checking in guests at the door. Not only is it time-consuming, but you might forget someone or misplace information. Now, picture using an event management service—where you simply set it up, and the service manages the entire guest experience. Amazon Cognito does this for user identity management in your application, handling registrations and logins so you can focus on building your game.

# Common Mistakes / Traps
- A common mistake is to think that IAM roles or groups can manage user identities for web applications, which is not appropriate as they serve different purposes. IAM is more about granting permissions to AWS resources rather than managing users for a web application.

# Memory Tip
- Use Cognito for streamlined user identities—let it handle registration, authentication, and authorization!"
283,"A photo sharing application uses Amazon S3 to store image files. All user images
are manually audited for inappropriate content by a third-party company. The
audits are completed 1-24 hours after user upload and the results are written to
an Amazon DynamoDB table, which uses the S3 object key as a primary key. The
database items can be queried by using a REST API created by the third-party
company.

An application developer needs to implement an automated process to tag all S3
objects with the results of the content audit.

What should the developer do to meet these requirements in the MOST
operationally efficient way?",Create an AWS Lambda function to run in response to the s3:ObjectCreated event type. Write the S3 key to an Amazon Simple Queue Service (Amazon SQS) queue with a visibility timeout of 24 hours. Create and configure a second Lambda function to read items from the queue. Retrieve the results for each item from the DynamoDB table. Tag each S3 object accordingly.,Create an AWS Lambda function to run in response to the s3:ObjectCreated event type. Integrate the function into an AWS Step Functions standard workflow. Define an AWS Step Functions Wait state and set the value to 24 hours. Create and configure a second Lambda function to retrieve the audit results and tag the S3 objects accordingly after the Wait state is over.,Create an AWS Lambda function to load all untagged S3 objects. Retrieve the results for each item from the REST API and tag each S3 object accordingly. Create and configure an Amazon EventBridge rule to run at regular intervals. Set the Lambda function as a target for the EventBridge rule.,Launch an Amazon EC2 instance. Deploy a script to the EC2 instance to use the external database results to tag the S3 objects accordingly. Configure a crontab file to run the script at regular intervals.,"# Answer
- **Correct option:** B  
- **Reason:** This option utilizes AWS Lambda in response to the S3 object creation event and employs AWS Step Functions to orchestrate a wait period for 24 hours before the second Lambda function retrieves the audit results and tags the S3 objects. This approach is the most operationally efficient as it automates the entire process without requiring manual intervention and ensures that the objects are tagged only after audit results are available, adhering to the time sensitivity of the operations.

# Example / Analogy
- Think of it like a bakery that bakes fresh cakes throughout the day but needs to let them cool for a specific period before frosting them. Instead of frosting them immediately after baking (which could lead to melting and mess), the bakery uses a timer (AWS Step Functions Wait state) to ensure cakes are only frosted after they've cooled, ensuring a perfect end product.

# Common Mistakes / Traps
- A common trap is to overlook the requirement of waiting for the audit results before tagging the images, which could lead to tagging images with the wrong status if done too soon. Options A and C do not effectively ensure that the tagging happens only after the audit results are available.

# Memory Tip
- ""Tag only after the wait – insulate actions with time for accuracy!"""
284,"A company has built an AWS Lambda function to convert large image files into
output files that can be used in a third-party viewer application. The company
recently added a new module to the function to improve the output of the
generated files. However, the new module has increased the bundle size and has
increased the time that is needed to deploy changes to the function code.

How can a developer increase the speed of the Lambda function deployment?",Use AWS CodeDeploy to deploy the function code.,Use Lambda layers to package and load dependencies.,Increase the memory size of the function.,Use Amazon S3 to host the function dependencies.,"# Answer
- **Correct option:** B : Use Lambda layers to package and load dependencies.
- **Reason:** Lambda layers allow you to separate your function code from its dependencies, helping to keep your deployment package smaller. By packaging your dependencies into layers, you can redeploy your function's business logic quickly without having to include these dependencies each time. This structure leads to faster deployment times and makes it easier to manage updates to dependencies separately from your function code.

# Example / Analogy
- Think of a Lambda function as a take-out meal from a restaurant. The main dish represents your function code, while the sides (like sauces or spices) are your dependencies. If you keep adding sides to the main dish, it becomes cumbersome to carry. Instead, if you package all your sides into a separate container (like a layer), you can quickly grab just the main dish when you need it, making your meal more efficient to take out.

# Common Mistakes / Traps
- A common mistake is to think that increasing the memory size (Choice C) will lead to faster deployments. However, while it may improve execution speed, it does not affect deployment times. Also, using AWS CodeDeploy (Choice A) is more suitable for deploying complex applications or managing versioning rather than for speeding up Lambda function deployments.

# Memory Tip
- Remember: ""Layers separate logic from bloat for speedy Lambda success!"""
285,"A developer creates a static website for their department. The developer deploys
the static assets for the website to an Amazon S3 bucket and serves the assets
with Amazon CloudFront. The developer uses origin access control (OAC) on the
CloudFront distribution to access the S3 bucket.

The developer notices users can access the root URL and specific pages but
cannot access directories without specifying a file name. For example,
/products/index.html works, but /products/ returns an error. The developer needs
to enable accessing directories without specifying a file name without exposing
the S3 bucket publicly.

Which solution will meet these requirements?",Update the CloudFront distribution's settings to index.html as the default root object is set.,Update the Amazon S3 bucket settings and enable static website hosting. Specify index.html as the Index document. Update the S3 bucket policy to enable access. Update the CloudFront distribution's origin to use the S3 website endpoint.,Create a CloudFront function that examines the request URL and appends index.html when directories are being accessed. Add the function as a viewer request CloudFront function to the CloudFront distribution's behavior.,Create a custom error response on the CloudFront distribution with the HTTP error code set to the HTTP 404 Not Found response code and the response page path to /index.html. Set the HTTP response code to the HTTP 200 OK response code.,"# Answer
- **Correct option:** A 

- **Reason:** By updating the CloudFront distribution's settings to set `index.html` as the default root object, any requests made to a directory (like `/products/`) will automatically redirect to `/products/index.html`, allowing users to access the content without exposing the S3 bucket publicly. This method effectively serves as a workaround for the limitation of not having the S3 bucket configured for static website hosting and avoids the complexity of other solutions.

# Example / Analogy
- Think of it like a restaurant where the front desk (CloudFront) needs to provide menus (index.html) for each section (directory). By simply indicating that the menu should be provided by default when a section is requested, guests can access the menus without needing to know the exact name to ask for.

# Common Mistakes / Traps
- A common trap is to think that enabling static website hosting on the S3 bucket (Option B) is the way to go. While it can solve the issue, it involves making the bucket publicly accessible, which is against the requirements. This way also compromises security.

# Memory Tip
- Remember: **""Set the default to index for directory access.""**"
286,"A developer is testing a RESTful application that is deployed by using Amazon
API Gateway and AWS Lambda. When the developer tests the user login by using
credentials that are not valid, the developer receives an HTTP 405:
METHOD_NOT_ALLOWED error. The developer has verified that the test is sending
the correct request for the resource.

Which HTTP error should the application return in response to the request?",HTTP 401,HTTP 404,HTTP 503,HTTP 505,"# Answer
- **Correct option:** A : HTTP 401

- **Reason:** The HTTP 401 Unauthorized status code indicates that the request has not been applied because it lacks valid authentication credentials for the target resource. In the context of a user login, invalid credentials should clearly trigger a 401 response, informing the client that authentication is required and that the provided credentials are incorrect.

# Example / Analogy
- Imagine you're trying to enter a secured building but don't have the right keycard. The security guard would deny you entry (HTTP 401), indicating that your credentials are insufficient. In this case, it's not that the building doesn't exist or is unavailable; it's just that you’re not authorized to enter with the keycard you presented.

# Common Mistakes / Traps
- A common mistake is to confuse HTTP 401 with HTTP 403 Forbidden, which indicates that valid credentials are provided, but access to the resource is not permitted. Also, developers might mistakenly think that HTTP 404 Not Found or HTTP 500 series errors (like 503 Service Unavailable) would fit scenarios of authentication failure, but those do not reflect on the user's credentials directly.

# Memory Tip
- Remember: ""401 for Unauthenticated"" – if the login fails due to the wrong credentials, it’s a 401 error."
287,"A developer must use multi-factor authentication (MFA) to access data in an
Amazon S3 bucket that is in another AWS account.

Which AWS Security Token Service (AWS STS) API operation should the developer
use with the MFA information to meet this requirement?",AssumeRoleWithWebIdentity,GetFederationToken,AssumeRoleWithSAML,AssumeRole,"# Answer
- **Correct option:** D : AssumeRole

- **Reason:** The `AssumeRole` API operation allows a user to assume a specified role within AWS, which could include permissions that require multi-factor authentication (MFA). When this operation is used, the developer can provide MFA information, allowing them to gain temporary access to resources in another AWS account, including an S3 bucket. This is ideal for cross-account access where added security with MFA is required.

# Example / Analogy
- Think of `AssumeRole` like getting a temporary key to a safe deposit box at a bank. You can only access the box (the S3 bucket) if you present a valid ID (role) and a security feature (MFA) that proves you are who you say you are. Without the MFA, even if you have the role, you can't access the safe deposit box.

# Common Mistakes / Traps
- A common trap may be confusing `AssumeRole` with other APIs that are used for different purposes like `AssumeRoleWithWebIdentity` or `GetFederationToken`, which might also seem relevant but do not directly involve MFA in cross-account access scenarios. Additionally, `AssumeRoleWithSAML` is specific to federated users using SAML assertions, which would not apply in this question.

# Memory Tip
- **Remember:** ""Use `AssumeRole` to lift the MFA lock when accessing cross-account AWS resources."""
288,"A developer designed an application on an Amazon EC2 instance. The application
makes API requests to objects in an Amazon S3 bucket.

Which combination of steps will ensure that the application makes the API
requests in the MOST secure manner? (Choose two.)",Create an IAM user that has permissions to the S3 bucket. Add the user to an IAM group.,Create an IAM role that has permissions to the S3 bucket.,Add the IAM role to an instance profile. Attach the instance profile to the EC2 instance.,Create an IAM role that has permissions to the S3 bucket. Assign the role to an IAM group.,"# Answer
- **Correct options:** B and C
- **Reason:** Creating an IAM role with permissions to access the S3 bucket allows the EC2 instance to assume this role and gain the necessary permissions securely without hardcoding credentials. By attaching this role to an instance profile and then associating it with the EC2 instance, the application can make API requests to S3 using temporary security credentials that AWS automatically rotates.

# Example / Analogy
- Think of IAM roles like a security badge that allows a worker (in this case, your application on the EC2 instance) to access specific rooms (S3 bucket). Instead of giving the worker a physical key (static credentials), you provide a badge that only works when they need to access those rooms, ensuring access is temporary and limited.

# Common Mistakes / Traps
- A common mistake in questions like this is choosing options that involve creating IAM users instead of roles. IAM users are static and involve more management complexity; they also require you to handle secret keys, which could lead to security vulnerabilities if not managed properly.

# Memory Tip
- **Use IAM roles for temporary credentials and enhanced security when accessing AWS resources.**"
289,"An AWS Lambda function requires read access to an Amazon S3 bucket and requires
read/write access to an Amazon DynamoDB table. The correct IAM policy already
exists.

What is the MOST secure way to grant the Lambda function access to the S3 bucket
and the DynamoDB table?",Attach the existing IAM policy to the Lambda function.,Create an IAM role for the Lambda function. Attach the existing IAM policy to the role. Attach the role to the Lambda function.,Create an IAM user with programmatic access. Attach the existing IAM policy to the user. Add the user access key ID and secret access key as environment variables in the Lambda function.,Add the AWS account root user access key ID and secret access key as encrypted environment variables in the Lambda function.,"# Answer
- **Correct option:** B
- **Reason:** Creating an IAM role for the Lambda function and attaching the existing IAM policy ensures that the function operates with the least privilege principle. This is a secure and flexible way to manage access since Lambda functions can assume the role at runtime without exposing static credentials.

# Example / Analogy
- Think of an IAM role as a guest pass to a corporate event. Instead of giving a permanent employee access (like an IAM user), you give a temporary pass (IAM role) that allows them to access specific areas (Amazon S3 and DynamoDB) when they need it. Once the event (Lambda execution) is over, the pass is no longer active, ensuring security.

# Common Mistakes / Traps
- A common mistake is to choose options that involve static credentials, such as IAM users (option C) or root user credentials (option D), which can lead to security vulnerabilities if exposed. Always prefer temporary credentials through IAM roles for AWS services like Lambda.

# Memory Tip
- Always prefer IAM roles over IAM users or root credentials for temporary access to AWS resources."
290,"A developer is using AWS Step Functions to automate a workflow. The workflow
defines each step as an AWS Lambda function task. The developer notices that
runs of the Step Functions state machine fail in the GetResource task with
either an IllegalArgumentException error or a TooManyRequestsException error.

The developer wants the state machine to stop running when the state machine
encounters an IllegalArgumentException error. The state machine needs to retry
the GetResource task one additional time after 10 seconds if the state machine
encounters a TooManyRequestsException error. If the second attempt fails, the
developer wants the state machine to stop running.

How can the developer implement the Lambda retry functionality without adding
unnecessary complexity to the state machine?",Add a Delay task after the GetResource task. Add a catcher to the GetResource task. Configure the catcher with an error type of TooManyRequestsException. Configure the next step to be the Delay task. Configure the Delay task to wait for an interval of 10 seconds. Configure the next step to be the GetResource task.,"Add a catcher to the GetResource task. Configure the catcher with an error type of TooManyRequestsException, an interval of 10 seconds, and a maximum attempts value of 1. Configure the next step to be the GetResource task.","Add a retrier to the GetResource task. Configure the retrier with an error type of TooManyRequestsException, an interval of 10 seconds, and a maximum attempts value of 1.",Duplicate the GetResource task. Rename the new GetResource task to TryAgain. Add a catcher to the original GetResource task. Configure the catcher with an error type of TooManyRequestsException. Configure the next step to be TryAgain.,"# Answer
- **Correct option:** C
- **Reason:** Option C is correct because it directly utilizes a retrier in the GetResource task specifically designed to handle the TooManyRequestsException error. The retrier is set with a maximum of one additional attempt after a delay of 10 seconds. This meets the requirements of retrying once on the appropriate error while allowing the state machine to stop on an IllegalArgumentException automatically, as that does not require retrying.

# Example / Analogy
- Consider a customer support center where a person calls in but gets a busy signal (TooManyRequestsException). The support system automatically tries again after 10 seconds. However, if the caller provides incorrect information (IllegalArgumentException), the system would stop further attempts and inform them to check their details before calling back.

# Common Mistakes / Traps
- A common mistake is to choose options (like A or B) that complicate the workflow or introduce unnecessary delays. It's important to remember that the retrier configuration in AWS Step Functions simplifies handling specific types of retries without adding additional steps or complexity.

# Memory Tip
- Remember: ""Use a retrier for retries; it's the simplest way to handle errors in Step Functions."""
291,"A developer is creating a serverless application that uses an AWS Lambda
function. The developer will use AWS CloudFormation to deploy the application.
The application will write logs to Amazon CloudWatch Logs. The developer has
created a log group in a CloudFormation template for the application to use. The
developer needs to modify the CloudFormation template to make the name of the
log group available to the application at runtime.

Which solution will meet this requirement?",Use the AWS::Include transform in CloudFormation to provide the log group's name to the application.,Pass the log group's name to the application in the user data section of the CloudFormation template.,Use the CloudFormation template's Mappings section to specify the log group's name for the application.,Pass the log group's Amazon Resource Name (ARN) as an environment variable to the Lambda function.,"# Answer
- **Correct option:** D
- **Reason:** Passing the log group's Amazon Resource Name (ARN) as an environment variable to the Lambda function allows the function to easily reference the log group at runtime. This method provides a straightforward way for the Lambda function to gain access to the necessary log group name without hard-coding it, ensuring that the function remains flexible and maintains best practices for configuration management.

# Example / Analogy
- Imagine you are a chef (the Lambda function) in a restaurant (the serverless application) and you need a specific pantry (log group) to get your ingredients (logs). If someone gives you a key (the ARN) to that pantry, you can always find it easily instead of someone telling you just its location or just giving you a vague description. This way, you can focus on cooking without worrying about how to find the pantry each time.

# Common Mistakes / Traps
- A common mistake is to assume that other options, like Mappings or user data, would work to pass the log group name within the CloudFormation template. However, these methods do not effectively provide the log group name to the Lambda function at runtime. It's essential to recognize that environment variables are specifically designed for this type of dynamic configuration.

# Memory Tip
- Remember: ""Use environment variables to make your Lambda function versatile!"""
292,"A developer is creating an Amazon DynamoDB table by using the AWS CLI. The
DynamoDB table must use server-side encryption with an AWS owned encryption key.

How should the developer create the DynamoDB table to meet these requirements?",Create an AWS Key Management Service (AWS KMS) customer managed key. Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyId parameter during creation of the DynamoDB table.,Create an AWS Key Management Service (AWS KMS) AWS managed key. Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyId parameter during creation of the DynamoDB table.,Create an AWS owned key. Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyId parameter during creation of the DynamoDB table.,Create the DynamoDB table with the default encryption options.,"# Answer
- **Correct option:** D : Create the DynamoDB table with the default encryption options.

- **Reason:** When you create a DynamoDB table, server-side encryption with AWS-owned keys is the default option that AWS uses for managing encryption transparently. AWS-managed keys or customer-managed keys require specifying additional configurations and are not necessary for typical use cases where AWS's default settings are sufficient.

# Example / Analogy
- Think of it like renting a car. When you rent a car, the rental company automatically provides insurance – you don’t need to purchase additional coverage unless you want superior benefits. In this case, using the default encryption options is like relying on the rental company’s included insurance; it’s sufficient for most drivers without any extra cost or hassle.

# Common Mistakes / Traps
- A common error is assuming that you need to provide a key (like in options A, B, or C) when the default settings already provide the necessary level of security, especially if you’re using AWS-owned keys for encryption.

# Memory Tip
- Remember: ""Default settings are often sufficient – trust AWS’s built-in security."""
293,"A company has an application that runs across multiple AWS Regions. The
application is experiencing performance issues at irregular intervals. A
developer must use AWS X-Ray to implement distributed tracing for the
application to troubleshoot the root cause of the performance issues.

What should the developer do to meet this requirement?",Use the X-Ray console to add annotations for AWS services and user-defined services.,Use Region annotation that X-Ray adds automatically for AWS services. Add Region annotation for user-defined services.,Use the X-Ray daemon to add annotations for AWS services and user-defined services.,Use Region annotation that X-Ray adds automatically for user-defined services. Configure X-Ray to add Region annotation for AWS services.,"# Answer
- **Correct option:** B
- **Reason:** Option B is correct because AWS X-Ray automatically adds Region annotations for AWS services, which helps in understanding which AWS Region the requests are traversing through. Additionally, adding Region annotations for user-defined services allows the developer to track the performance across different regions consistently. This is crucial for troubleshooting performance issues in applications that run across multiple AWS Regions.

# Example / Analogy
- Think of it like tracking a package delivery that goes through multiple cities (Regions). You want to know which city (Region) your package (service call) is currently in and where it came from to identify any delays in transit. By annotating with Regions, you can pinpoint where the slowdown occurs.

# Common Mistakes / Traps
- A common mistake would be to assume that only user-defined services need region annotations or that the default behavior of X-Ray is sufficient without additional customization for your specific application architecture. Always ensure to check how annotations can enhance your tracing.

# Memory Tip
- ""Track your regions like you track your packages."""
294,"A company runs an application on AWS. The application uses an AWS Lambda
function that is configured with an Amazon Simple Queue Service (Amazon SQS)
queue called high priority queue as the event source. A developer is updating
the Lambda function with another SQS queue called low priority queue as the
event source. The Lambda function must always read up to 10 simultaneous
messages from the high priority queue before processing messages from low
priority queue. The Lambda function must be limited to 100 simultaneous
invocations.

Which solution will meet these requirements?",Set the event source mapping batch size to 10 for the high priority queue and to 90 for the low priority queue.,Set the delivery delay to 0 seconds for the high priority queue and to 10 seconds for the low priority queue.,Set the event source mapping maximum concurrency to 10 for the high priority queue and to 90 for the low priority queue.,Set the event source mapping batch window to 10 for the high priority queue and to 90 for the low priority queue.,"# Answer
- **Correct option:** C
- **Reason:** Setting the event source mapping maximum concurrency to 10 for the high priority queue ensures that the Lambda function processes messages from the high priority queue first with the limit of 10 simultaneous invocations. Since the total limit of simultaneous invocations is 100, this allows for up to 90 simultaneous invocations for the low priority queue while maintaining the priority.

# Example / Analogy
- Think of it like a restaurant where the chef can handle up to 10 orders at once from the VIP section (high priority queue) before moving to the regular section (low priority queue) where he can process 90 more orders. This ensures that important customers are served first and efficiently.

# Common Mistakes / Traps
- A common mistake is to confuse batch size with concurrency. Batch size controls how many messages are processed at once, while maximum concurrency defines how many instances of the function can run simultaneously. Not acknowledging the distinction can lead to choosing an incorrect option.

# Memory Tip
- **Prioritize with limits: Concurrency controls the number of cooks in the kitchen.**"
295,"A data visualization company wants to strengthen the security of its core
applications. The applications are deployed on AWS across its development,
staging, pre-production, and production environments. The company needs to
encrypt all of its stored sensitive credentials. The sensitive credentials need
to be automatically rotated. A version of the sensitive credentials need to be
stored for each environment.

Which solution will meet these requirements in the MOST operationally efficient
way?",Configure AWS Secrets Manager versions to store different copies of the same credentials across multiple environments.,Create a new parameter version in AWS Systems Manager Parameter Store for each environment. Store the environment-specific credentials in the parameter version.,Configure the environment variables in the application code. Use different names for each environment type.,Configure AWS Secrets Manager to create a new secret for each environment type. Store the environment-specific credentials in the secret.,"# Answer
- **Correct option:** A

- **Reason:** AWS Secrets Manager is specifically designed for managing secrets, including secure storage, automatic rotation, and versioning of secrets. Option A allows you to configure versions for different environments while ensuring the credentials are encrypted, meeting the requirement for operational efficiency and security. Each environment can have different versions of the same secret, making it straightforward to manage.

# Example / Analogy
- Think of AWS Secrets Manager like a high-tech secure vault where you can store different keys (credentials) for multiple doors (environments). Each time you need to change a key (rotate secrets), the vault can do it seamlessly without you needing to remember which key goes to which door, keeping everything organized and secure.

# Common Mistakes / Traps
- A common mistake is to assume that AWS Systems Manager Parameter Store is a better option due to its simplicity. However, it does not provide automatic rotation of secrets, which is critical for the requirements stated in the question. Additionally, creating environment-specific variables in application code (Option C) can lead to security vulnerabilities and operational complexities.

# Memory Tip
- Use Secrets Manager for secrets management: Secure, Rotate, Version."
296,"A developer is investigating an issue in part of a company's application. In the
application, messages are sent to an Amazon Simple Queue Service (Amazon SQS)
queue. The AWS Lambda function polls messages from the SQS queue and sends email
messages by using Amazon Simple Email Service (Amazon SES). Users have been
receiving duplicate email messages during periods of high traffic.

Which reasons could explain the duplicate email messages? (Choose two.)",Standard SQS queues support at-least-once message delivery.,"Standard SQS queues support exactly-once processing, so the duplicate email messages are because of user error.",Amazon SES has the DomainKeys Identified Mail (DKIM) authentication incorrectly configured.,The SQS queue's visibility timeout is lower than or the same as the Lambda function's timeout.,"# Answer
- **Correct options:** A, D
- **Reason:** A Standard SQS queue guarantees at-least-once message delivery, which means that during high traffic, messages can be delivered multiple times. Also, if the visibility timeout of the queue is not set properly (i.e., it is lower than or equal to the Lambda function's timeout), the message may become visible again before it's been processed, causing the Lambda function to process it again and potentially generate a duplicate email.

# Example / Analogy
- Think of it like a delivery service where the same package (message) could be sent multiple times if the delivery person (Lambda function) isn't given enough time to deliver it before they get another request for the same package. In a busy period, the system might ring the doorbell (send a notification) twice before the first package has been collected.

# Common Mistakes / Traps
- A common trap is assuming that Standard SQS queues prevent duplicates or provide exactly-once processing, which they do not. Remembering that ""at-least-once delivery"" leads to potential duplicates is crucial.

# Memory Tip
- Remember: ""SQS Standard = at-least-once = possible duplicates!"""
297,"A developer is deploying a company's application to Amazon EC2 instances. The
application generates gigabytes of data files each day. The files are rarely
accessed, but the files must be available to the application's users within
minutes of a request during the first year of storage. The company must retain
the files for 7 years.

How can the developer implement the application to meet these requirements MOST
cost-effectively?",Store the files in an Amazon S3 bucket. Use the S3 Glacier Instant Retrieval storage class. Create an S3 Lifecycle policy to transition the files to the S3 Glacier Deep Archive storage class after 1 year.,Store the files in an Amazon S3 bucket. Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition the files to the S3 Glacier Flexible Retrieval storage class after 1 year.,Store the files on an Amazon Elastic Block Store (Amazon EBS) volume. Use Amazon Data Lifecycle Manager (Amazon DLM) to create snapshots of the EBS volumes and to store those snapshots in Amazon S3.,Store the files on an Amazon Elastic File System (Amazon EFS) mount. Configure EFS lifecycle management to transition the files to the EFS Standard- Infrequent Access (Standard-IA) storage class after 1 year.,"# Answer
- **Correct option:** A

- **Reason:** Storing the files in an Amazon S3 bucket with the S3 Glacier Instant Retrieval storage class meets the immediate access requirement for the first year, as it allows files to be retrieved within minutes. Transitioning to the S3 Glacier Deep Archive after the first year will significantly lower storage costs while still complying with the 7-year retention requirement.

# Example / Analogy
- Imagine you are renting a storage unit. Initially, you want quick access to your items, so you choose a climate-controlled unit (S3 Glacier Instant Retrieval). After a year, you decide that you rarely need to access them, so you move them to a less expensive, non-climate-controlled unit (S3 Glacier Deep Archive) that you can access whenever needed, even if it takes a bit longer.

# Common Mistakes / Traps
- One common mistake is choosing options that provide immediate access but are not cost-effective for the long-term storage, such as S3 Standard (Option B), which is more expensive for infrequently accessed data over a long period. Additionally, developers may overlook lifecycle policies or the requirements regarding access times and retention durations.

# Memory Tip
- ""Instant access today, less cost tomorrow — choose S3 Glacier with lifecycle policies!"""
298,"A company's developer has deployed an application in AWS by using AWS
CloudFormation. The CloudFormation stack includes parameters in AWS Systems
Manager Parameter Store that the application uses as configuration settings. The
application can modify the parameter values.

When the developer updated the stack to create additional resources with tags,
the developer noted that the parameter values were reset and that the values
ignored the latest changes made by the application. The developer needs to
change the way the company deploys the CloudFormation stack. The developer also
needs to avoid resetting the parameter values outside the stack.

Which solution will meet these requirements with the LEAST development effort?",Modify the CloudFormation stack to set the deletion policy to Retain for the Parameter Store parameters.,Create an Amazon DynamoDB table as a resource in the CloudFormation stack to hold configuration data for the application. Migrate the parameters that the application is modifying from Parameter Store to the DynamoDB table.,Create an Amazon RDS DB instance as a resource in the CloudFormation stack. Create a table in the database for parameter configuration. Migrate the parameters that the application is modifying from Parameter Store to the configuration table.,Modify the CloudFormation stack policy to deny updates on Parameter Store parameters.,"# Answer
- **Correct option:** A : Modify the CloudFormation stack to set the deletion policy to Retain for the Parameter Store parameters.
- **Reason:** Setting the deletion policy to ""Retain"" ensures that when the CloudFormation stack is updated or deleted, the Parameter Store parameters are preserved and not reset to their original values. This way, any modifications made by the application to the parameters will be maintained.

# Example / Analogy
- Imagine you are renting an apartment (CloudFormation stack) but have some personal belongings (Parameter Store parameters) inside that you want to keep even if you move or upgrade to a bigger apartment. By asking the landlord to allow you to keep your belongings no matter what happens with the lease (setting the deletion policy to ""Retain""), you ensure that they are not taken away when changes occur. 

# Common Mistakes / Traps
- A common mistake is underestimating the impact of the deletion policy and assuming that default behaviors will protect critical resources. Always check default behaviors when dealing with resource management in CloudFormation.

# Memory Tip
- **""Retain to sustain""**: Use the deletion policy to retain resources that should persist beyond stack updates."
299,"A company has a social media application that receives large amounts of traffic.
User posts and interactions are continuously updated in an Amazon RDS database.
The data changes frequently, and the data types can be complex. The application
must serve read requests with minimal latency.

The application's current architecture struggles to deliver these rapid data
updates efficiently. The company needs a solution to improve the application's
performance.

Which solution will meet these requirements?",Use Amazon DynamoDB Accelerator (DAX) in front of the RDS database to provide a caching layer for the high volume of rapidly changing data.,Set up Amazon S3 Transfer Acceleration on the RDS database to enhance the speed of data transfer from the databases to the application.,Add an Amazon CloudFront distribution in front of the RDS database to provide a caching layer for the high volume of rapidly changing data.,Create an Amazon ElastiCache for Redis cluster. Update the application code to use a write-through caching strategy and read the data from Redis.,"# Answer
- **Correct option:** D : Create an Amazon ElastiCache for Redis cluster. Update the application code to use a write-through caching strategy and read the data from Redis.

- **Reason:** Using Amazon ElastiCache for Redis creates an in-memory caching layer which significantly reduces latency for read operations. In a write-through caching strategy, data is written to the cache and the database simultaneously, ensuring that cache is up-to-date and serves fast read requests. This is particularly effective for applications with frequent updates and a high volume of read traffic.

# Example / Analogy
- Imagine you’re at a busy restaurant where customers constantly place orders. If the chef cooks everything from scratch for each order (like querying a database directly), it takes longer for the customers to get their food. However, if the restaurant has a prep station (the cache) where common ingredients are pre-prepared and ready to go, orders can be fulfilled much faster, allowing customers to receive their meals almost instantly. That prep station in this analogy is like ElastiCache, which serves as a quick-access layer for data.

# Common Mistakes / Traps
- A common trap is to assume that caching solutions like DAX (Option A) or CloudFront (Option C) can adequately handle dynamic database updates. DAX is tailored for use with DynamoDB, and CloudFront is primarily used for static content delivery, making them unsuitable for the specific use case of serving rapidly changing data from an RDS database.

# Memory Tip
- ""Use ElastiCache to keep data fast and fresh!"""
300,"A developer created an AWS Lambda function that performs a series of operations
that involve multiple AWS services. The function's duration time is higher than
normal. To determine the cause of the issue, the developer must investigate
traffic between the services without changing the function code.

Which solution will meet these requirements?",Enable AWS X-Ray active tracing in the Lambda function. Review the logs in X-Ray.,Configure AWS CloudTrail. View the trail logs that are associated with the Lambda function.,Review the AWS Config logs in Amazon CloudWatch.,Review the Amazon CloudWatch logs that are associated with the Lambda function.,"# Answer
- **Correct option:** A : Enable AWS X-Ray active tracing in the Lambda function. Review the logs in X-Ray.

- **Reason:** AWS X-Ray provides distributed tracing capabilities that allow developers to visualize and analyze the latency of requests as they travel through various AWS services. By enabling active tracing in the Lambda function, the developer can gain insights into how long each service call takes, identify bottlenecks, and understand where delays are occurring without modifying the function code itself. This is essential for diagnosing performance issues in a serverless architecture.

# Example / Analogy
- Imagine running a race where each runner represents a different AWS service. If one runner is significantly slower, you could use a stopwatch to track each runner's time and identify where the delays are happening. AWS X-Ray acts like that stopwatch, allowing you to see the timing of each service call.

# Common Mistakes / Traps
- A common mistake is assuming that CloudTrail or CloudWatch logs are sufficient for monitoring latency. While they provide useful logging information, they do not visualize service performance or latency like X-Ray does. Developers may also mistakenly think that reviewing CloudWatch logs will give them insight into inter-service traffic, but these logs primarily capture execution details rather than service interaction timing.

# Memory Tip
- Think ""X-Ray for insight!"" — it clarifies where time is spent in your AWS architecture."
301,"A company has on-premises data centers that run an image processing service. The
service consists of containerized applications that run on Kubernetes clusters.
All the applications have access to the same NFS share for files and data
storage.

The company is running out of NFS capacity in the data centers and needs to
migrate to AWS as soon as possible. The Kubernetes clusters must be highly
available on AWS.

Which combination of actions will meet these requirements? (Choose two.)",Transfer the information that is in the NFS share to an Amazon Elastic Block Store (Amazon EBS) volume. Upload the container images to Amazon Elastic Container Registry (Amazon ECR).,Transfer the information that is in the NFS share to an Amazon Elastic File System (Amazon EFS) volume. Upload the container images to Amazon Elastic Container Registry (Amazon ECR).,Create an Amazon Elastic Container Service (Amazon ECS) cluster to run the applications. Configure each node of the cluster to mount the Amazon Elastic Block Store (Amazon EBS) volume at the required path for the container images.,Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to run the applications. Configure each node of the cluster to mount the Amazon Elastic Block Store (Amazon EBS) volume at the required path for the container images.,"# Answer
- **Correct options:** B and D
- **Reason:** Transferring the data from the NFS share to Amazon Elastic File System (Amazon EFS) allows the containerized applications in the Kubernetes clusters to access a shared storage solution that is highly available and scalable. Meanwhile, utilizing Amazon ECR for storing container images ensures that the images are securely stored and easily accessible by Amazon Elastic Kubernetes Service (EKS) for deploying the applications.

# Example / Analogy
- Imagine a library (NFS share) that is now running out of shelf space. Instead of cramming books into tight spots (which would be like the NFS in the data center), you move the library to a larger digital space (Amazon EFS), which can expand as you get more books. You also want to ensure that your library is stocked with the latest books (container images), so you keep them stored in a secure repository (Amazon ECR) where they can easily be fetched when needed.

# Common Mistakes / Traps
- A common trap is assuming that Amazon EBS would be the appropriate solution for shared access across multiple containerized applications, as Amazon EBS does not support multi-attach capabilities across multiple instances. This could lead to data inconsistency and failures.

# Memory Tip
- Remember: For shared storage in Kubernetes, use EFS; store images in ECR."