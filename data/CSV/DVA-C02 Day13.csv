No,Content,A,B,C,D,Answered
242,"A developer is building an application that invokes AWS Lambda functions
asynchronously to process events. The developer notices that a Lambda function
fails to process some events at random times. The developer needs to investigate
the failed events and capture the events that the Lambda function fails to
process.

Which solution will meet these requirements?",Add an Amazon EventBridge rule for the Lambda function. Configure the EventBridge rule to react to failed events and to store the events in an Amazon DynamoDB table.,Configure the Lambda function with a dead-letter queue based in Amazon Kinesis. Update the Lambda function's execution role with the required permissions.,Configure the Lambda function with an Amazon Simple Queue Service (Amazon SQS) dead-letter queue. Update the Lambda function's execution role with the required permissions.,Configure the Lambda function with an Amazon Simple Queue Service (Amazon SQS) FIFO dead-letter queue. Update the Lambda function's execution role with the required permissions.,"# Answer
- **Correct option:** C

- **Reason:** Configuring the Lambda function with an Amazon Simple Queue Service (Amazon SQS) dead-letter queue allows you to capture and store events that fail to process. When a Lambda function fails after exhausting its retry attempts, the unprocessed event is sent to the specified SQS dead-letter queue. This makes it easy for developers to investigate and reprocess failed events later.

# Example / Analogy
- Think of a mail delivery system. If a letter cannot be delivered (perhaps the address is incorrect), the letter needs to be sent to a return-to-sender box (the dead-letter queue) instead of being thrown away. The post office can then review and resolve issues with the undelivered letters later, just as you would investigate failed events stored in your dead-letter queue.

# Common Mistakes / Traps
- One common mistake is to confuse the use of Amazon Kinesis or using EventBridge for dead-letter queues. While Kinesis can handle stream data and EventBridge is great for rules and event-driven architecture, neither is suitable for directly capturing Lambda failures like SQS does with its dead-letter queues.

# Memory Tip
- ""Dead-letter queues catch missed deliveries from Lambda."""
243,"A company has built a serverless application for its ecommerce website. The
application includes a REST API in Amazon API Gateway that invokes an AWS Lambda
function. The Lambda function processes data and stores the data in Amazon
DynamoDB table. The Lambda function calls a third-party stock application API to
process the order. After the ordered is processed, the Lambda function returns
an HTTP 200 status code with no body to the client.

During peak usage when the API calls exceeds a certain threshold, the
third-party stock application sometimes fails to process the data and responds
with error messages. The company needs a solution that will not overwhelm the
third-party stock application.

Which solution will meet these requirements?",Configure the REST API in API Gateway to write the requests directly into DynamoDB. Configure a DynamoDB intrinsic function to perform the transformation. Set up a DynamoDB stream to call the third-party stock application API with each new row. Delete the Lambda function.,Configure the REST API in API Gateway to write the requests directly into an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function with a reserved concurrency equal to the third-party stock application's threshold. Set Lambda function to process the messages from the SQS queue.,Configure the REST API in API Gateway to write the requests directly into an Amazon Simple Notification Service (Amazon SNS) topic. Configure the Lambda function with a provisioned concurrency equal to the third-party stock application's threshold. Set the Lambda function to process the messages from the SNS topic.,Configure the REST API in API Gateway to write the requests directly into Amazon Athena. Configure the transformation of the data by using SQL with multiple query result locations set up to point to the DynamoDB table and the third-party stock fulfilment application API. Delete the Lambda function.,"# Answer
- **Correct option:** B

- **Reason:** Option B suggests configuring the REST API in API Gateway to write requests directly into an Amazon Simple Queue Service (Amazon SQS) queue. This approach allows for decoupling the processing of requests and alleviates the pressure on the third-party stock application by delaying the invocations to the Lambda function according to the SQS queue's flow. Additionally, by setting a reserved concurrency on the Lambda function equal to the third-party stock application's threshold, it ensures that you won't overwhelm the external API, thereby meeting the requirements.

# Example / Analogy
- Imagine a busy restaurant where a set number of chefs can handle food orders at any given time. Instead of sending all the customer orders directly to the chefs (which could overwhelm them), the restaurant takes orders and places them into a ""waiting area"" (SQS). The chefs can then pick orders from this waiting area at a manageable pace, ensuring they can maintain quality without being swamped. Similarly, using SQS allows the Lambda function to process requests at a controlled rate, avoiding overwhelming the third-party service.

# Common Mistakes / Traps
- A common trap is to overlook the need for throttling when dealing with external APIs. Options A, C, and D either eliminate the Lambda function, incorrectly use services like SNS or Athena for this purpose, or introduce complexity without managing request loads correctly.

# Memory Tip
- Remember: Use **SQS** to **queue** orders and avoid overwhelming external APIs!"
244,"A company hosts its application on AWS. The application runs on an Amazon
Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. The
cluster runs behind an Application Load Balancer. The application stores data in
an Amazon Aurora database. A developer encrypts and manages database credentials
inside the application.

The company wants to use a more secure credential storage method and implement
periodic credential rotation.

Which solution will meet these requirements with the LEAST operational overhead?",Migrate the secret credentials to Amazon RDS parameter groups. Encrypt the parameter by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant AWS KMS permissions to access Amazon RDS.,Migrate the credentials to AWS Systems Manager Parameter Store. Encrypt the parameter by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager.,Migrate the credentials to ECS Fargate environment variables. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager.,Migrate the credentials to AWS Secrets Manager. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager by using keys.,"# Answer
- **Correct option:** D 

- **Reason:** AWS Secrets Manager is specifically designed for storing and managing secrets such as database credentials. It provides built-in features for encryption, access management, and, importantly, automatic secret rotation. By migrating the credentials to Secrets Manager, the application can securely retrieve the secrets with minimal operational overhead. 

# Example / Analogy
- Think of Secrets Manager as a secure vault for your valuable items (secrets like passwords). Just as a vault has strong locks and systems in place to ensure that only authorized persons can access or change what’s inside, Secrets Manager ensures that only authorized services and users can access the sensitive information you store (like database credentials). The added benefit is that the vault can automatically replace the items at regular intervals (automatic rotation), so you don’t have to remember to update them manually.

# Common Mistakes / Traps
- A common mistake is to choose a solution that requires more manual management or lacks integrated secret rotation features. Options like AWS Systems Manager Parameter Store (option B) may not provide the same ease of secret management and automatic rotation as Secrets Manager. Additionally, focusing solely on encryption without considering ease of access and management can lead to overlooking better solutions. 

# Memory Tip
- For managing secrets securely and with automatic rotation, ""Choose Secrets Manager, your vault for credentials."""
245,"A company has a mobile app. The app includes an Amazon API Gateway REST API that
invokes AWS Lambda functions. The Lambda functions process data from the app.

The company needs to test updated Lambda functions that have new features. The
company must conduct these tests with a subset of users before deployment. The
tests must not affect other users of the app.

Which solution will meet these requirements with the LEAST amount of operational
effort?",Create a new version of each Lambda function with a weighted alias. Configure a weight value for each version of the Lambda function. Update the new weighted alias Amazon Resource Name (ARN) in the REST API.,Create a new REST API in API Gateway. Set up a Lambda proxy integration to connect to multiple Lambda functions. Enable canary settings on the deployment stage. Specify a smaller percentage of API traffic to go to the new version of the Lambda function.,Create a new version of each Lambda function. Integrate a predefined canary deployment in AWS CodeDeploy to slowly shift the traffic to the new versions automatically.,Create a new REST API in API Gateway. Set up a Lambda non-proxy integration to connect to multiple Lambda functions. Specify the necessary parameters and properties in API Gateway. Enable canary settings on the deployment stage. Specify a smaller percentage of API traffic to go to the new version of the Lambda function.,"# Answer
- **Correct option:** A
- **Reason:** Creating a new version of each Lambda function with a weighted alias allows the company to control the traffic going to the new version with minimal operational effort. This means they can gradually test the new features with a subset of users without affecting others, and updating the REST API to use the new weighted alias is straightforward.

# Example / Analogy
- Consider it like a restaurant offering a new dish as a special. Instead of switching the whole menu at once, the restaurant can let customers know that only a fraction of them will get to try the new dish. If the feedback is good, they can increase the number of servings; if not, they can retract it quickly. Similarly, using a weighted alias allows for a staged rollout of the new Lambda function version.

# Common Mistakes / Traps
- A common mistake is to overlook the operational simplicity that weighted aliases provide compared to more complex setups like canary deployments with AWS CodeDeploy or creating new REST APIs. Some might assume a full deployment or new API is necessary, which increases the complexity.

# Memory Tip
- Remember: **Weighted aliases are the easy switch for gradual rollouts in Lambda.**"
246,"A developer works for a company that only has a single pre-production AWS
account with an AWS CloudFormation AWS Serverless Application Model (AWS SAM)
stack. The developer made changes to an existing AWS Lambda function specified
in the AWS SAM template and additional Amazon Simple Notification service
(Amazon SNS) topics.

The developer wants to do a one-time deploy of the changes to test if the
changes are working. The developer does not want to impact the existing
pre-production application that is currently being used by other team members as
part of the release pipeline.

Which solution will meet these requirements?",Use the AWS SAM CLI to package and deploy the SAM application to the pre-production AWS account. Specify the debug parameter.,Use the AWS SAM CLI to package and create a change set against the pre-production AWS account. Execute the change set in a new AWS account designated for a development environment.,Use the AWS SAM CLI to package and deploy the SAM application to a new AWS account designated for a development environment.,Update the CloudFormation stack in the pre-production account. Add a separate stage that points to a new AWS account designated for a development environment.,"# Answer
- **Correct option:** C
- **Reason:** This option allows the developer to deploy the changes to a new AWS account that is specifically designated for a development environment. This prevents any potential disruption to the existing pre-production application, ensuring that other team members can continue using it without interruption.

# Example / Analogy
- Think of this scenario like a chef testing a new recipe. Instead of making changes in the restaurant kitchen where customers are being served, the chef uses a separate test kitchen to try out new ingredients and techniques. This way, the main restaurant operations remain unaffected while the chef perfects the dish.

# Common Mistakes / Traps
- A common mistake is choosing options that suggest modifying the existing pre-production stack (like option D), as this introduces risk. Some might also overlook the importance of using separate environments for testing changes and might consider using the same account.

# Memory Tip
- Always test new features in a dedicated development environment to avoid disrupting live applications."
247,"A company runs its website on AWS. The company posts daily polls on its website
and publishes the poll results next day. The website stores user responses in an
Amazon DynamoDB table. After the poll results are published, the company does
not need to keep the user responses.

A developer needs to implement a solution that will automatically remove old
user responses from the DynamoDB table. The developer adds a new expiration_date
attribute to the DynamoDB table. The developer plans to use the expiration_date
attribute for the automation.

Which solution will meet these requirements with the LEAST development effort?",Create an AWS Lambda function to delete old user responses based on the expiration_date attribute. Create an Amazon EventBridge schedule to run the Lambda function daily.,Create an AWS Fargate task in Amazon Elastic Container Service (Amazon ECS) to delete old user responses based on the expiration_date attribute. Create an Amazon EventBridge schedule to run the Fargate task daily.,Create an AWS Glue job to delete old user responses based on the expiration_date attribute. Create an AWS Glue trigger schedule to run the job daily.,Enable TTL on the DynamoDB table and specify the expiration_date attribute. Expire old user responses by using DynamoDB TTL.,"# Answer
- **Correct option:** D : Enable TTL on the DynamoDB table and specify the expiration_date attribute. Expire old user responses by using DynamoDB TTL.

- **Reason:** Enabling TTL (Time to Live) on a DynamoDB table allows you to automatically delete items after a specified timestamp, defined by the expiration_date attribute. This built-in feature requires minimal development effort since it automatically handles the deletion process without the need for additional services such as Lambda, ECS, or AWS Glue.

# Example / Analogy
- Think of TTL like a day-old sandwich at a café that automatically gets removed from the display after a certain time. The café doesn't need staff to monitor and take it away; it's done automatically due to a preset policy.

# Common Mistakes / Traps
- A common mistake is underestimating the power of built-in features like TTL and opting for more complex solutions (like Lambda or Fargate) that require additional configuration and maintenance. Always consider if there’s a simpler solution available first.

# Memory Tip
- Remember: Enable TTL for automatic cleanup – it saves time and effort!"
248,"A developer is creating a simple proof-of-concept demo by using AWS
CloudFormation and AWS Lambda functions. The demo will use a CloudFormation
template to deploy an existing Lambda function. The Lambda function uses
deployment packages and dependencies stored in Amazon S3. The developer defined
an AWS::Lambda::Function resource in a CloudFormation template. The developer
needs to add the S3 bucket to the CloudFormation template.

What should the developer do to meet these requirements with the LEAST
development effort?",Add the function code in the CloudFormation template inline as the code property.,Add the function code in the CloudFormation template as the ZipFile property.,Find the S3 key for the Lambda function. Add the S3 key as the ZipFile property in the CloudFormation template.,Add the relevant key and bucket to the S3Bucket and S3Key properties in the CloudFormation template.,"# Answer
- **Correct option:** D
- **Reason:** The AWS::Lambda::Function resource in a CloudFormation template supports specifying the location of the deployment package in Amazon S3 using the S3Bucket and S3Key properties. This is the standard practice for deploying Lambda functions that use external code packages stored in S3, allowing you to keep your template clean and reference the pre-existing code directly.

# Example / Analogy
- Imagine you want to cook a meal using a recipe that requires ingredients stored in a pantry (your S3 bucket). Instead of writing down each ingredient (code), you simply reference its location in your pantry (by specifying S3Bucket and S3Key). This way, you don't have to duplicate effort and can quickly access what you need for your dish (deployment).

# Common Mistakes / Traps
- A common mistake would be to select A or B, thinking that inline code is easier or sufficient. However, inline code is not practical for existing Lambda functions with dependencies that are already in S3. Additionally, selecting C may lead to confusion because the ZipFile property is only for small inline code snippets, not for deployment packages stored in S3.

# Memory Tip
- Always use S3Bucket and S3Key for existing Lambda packages in CloudFormation."
249,"A developer is building a microservices-based application by using Python on AWS
and several AWS services. The developer must use AWS X-Ray. The developer views
the service map by using the console to view the service dependencies. During
testing, the developer notices that some services are missing from the service
map.

What can the developer do to ensure that all services appear in the X-Ray
service map?",Modify the X-Ray Python agent configuration in each service to increase the sampling rate.,Instrument the application by using the X-Ray SDK for Python. Install the X-Ray SDK for all the services that the application uses.,Enable X-Ray data aggregation in Amazon CloudWatch Logs for all the services that the application uses.,Increase the X-Ray service map timeout value in the X-Ray console.,"# Answer
- **Correct option:** B : Instrument the application by using the X-Ray SDK for Python. Install the X-Ray SDK for all the services that the application uses.

- **Reason:** To ensure that all services appear in the AWS X-Ray service map, each service must be instrumented with the X-Ray SDK. This involves explicitly integrating the SDK into the application's code to enable tracing for requests and responses. Without this instrumentation, X-Ray won’t have the necessary data to display those services in the service map.

# Example / Analogy
- Think of AWS X-Ray as a GPS tracking system for your car (your microservices). If you want to track all your stops (services), you need to install a GPS tracker in each car (service). If only some cars are equipped with trackers (instrumented), you’ll only see their paths on the map, missing the routes taken by the others.

# Common Mistakes / Traps
- A common mistake is to assume simply increasing the sampling rate or making configuration changes is enough. It is essential to actually instrument the code to enable tracing rather than just adjusting settings.

# Memory Tip
- **""Instrument to visualize!""** Remember that each service needs instrumentation for X-Ray to capture its data."
250,"A developer is building a containerized application on AWS. The application
communicates with a third-party service by using API keys. The developer needs a
secure way to store the API keys and pass the API keys to the containerized
application.

Which solutions will meet these requirements? (Choose two.)",Store the API keys as a SecureString parameter in AWS Systems Manager Parameter Store. Grant the application access to retrieve the value from Parameter Store.,Store the API keys in AWS CloudFormation templates by using base64 encoding. Pass the API keys to the application through container definition environment variables.,Add a new AWS CloudFormation parameter to the CloudFormation template. Pass the API keys to the application by using the container definition environment variables.,Embed the API keys in the application. Build the container image on-premises. Upload the container image to Amazon Elastic Container Registry (Amazon ECR).,"# Answer
- **Correct options:** A, C

- **Reason:**  
Option A is correct because storing API keys as a SecureString parameter in AWS Systems Manager Parameter Store ensures they are encrypted and can be accessed securely by the application when needed. This approach adheres to security best practices by preventing hardcoding sensitive information. Option C is also correct as adding a CloudFormation parameter to provide API keys allows for dynamic configuration of the application during deployment, ensuring the keys remain externalized from the code while still being supplied at runtime.

# Example / Analogy
- Think of AWS Systems Manager Parameter Store like a safe where you can securely keep important documents (API keys). By using a secure safe, you ensure that only authorized individuals or applications can access the contents, whereas hardcoding the keys is like putting them on a sticky note next to your computer—easy to access but also easily compromised.

# Common Mistakes / Traps
- A common mistake is choosing options that involve hardcoding sensitive data (like in option D) or passing keys directly through environment variables without proper management, which can expose them to potential security breaches. Option B, while using base64 encoding, does not provide real security and is still vulnerable to exposure since anyone can decode base64 easily.

# Memory Tip
- **""Secure the keys, don’t expose them!""**"
251,"A company runs a payment application on Amazon EC2 instances behind an
Application Load Balance. The EC2 instances run in an Auto Scaling group across
multiple Availability Zones. The application needs to retrieve application
secrets during the application startup and export the secrets as environment
variables. These secrets must be encrypted at rest and need to be rotated every
month.

Which solution will meet these requirements with the LEAST development effort?",Save the secrets in a text file and store the text file in Amazon S3. Provision a customer managed key. Use the key for secret encryption in Amazon S3. Read the contents of the text file and read the export as environment variables. Configure S3 Object Lambda to rotate the text file every month.,Save the secrets as strings in AWS Systems Manager Parameter Store and use the default AWS Key Management Service (AWS KMS) key. Configure an Amazon EC2 user data script to retrieve the secrets during the startup and export as environment variables. Configure an AWS Lambda function to rotate the secrets in Parameter Store every month.,Save the secrets as base64 encoded environment variables in the application properties. Retrieve the secrets during the application startup. Reference the secrets in the application code. Write a script to rotate the secrets saved as environment variables.,Store the secrets in AWS Secrets Manager. Provision a new customer master key. Use the key to encrypt the secrets. Enable automatic rotation. Configure an Amazon EC2 user data script to programmatically retrieve the secrets during the startup and export as environment variables.,"# Answer
- **Correct option:** D
- **Reason:** Storing secrets in AWS Secrets Manager is specifically designed for managing sensitive information like application secrets. It handles encryption at rest automatically, allows for easy rotation of secrets through built-in functionalities, and provides a straightforward way to retrieve these secrets during application startup. Using user data scripts with EC2 simplifies the process of exporting these secrets as environment variables.

# Example / Analogy
- Think of AWS Secrets Manager as a ""vault"" where you can store your important documents (secrets) securely. Not only is it locked (encrypted), but it also has a system that checks and refreshes those documents every month (automated rotation), so you never have to worry about outdated or compromised information.

# Common Mistakes / Traps
- A common mistake is underestimating the importance of using a solution that is designed for secrets management (like Secrets Manager). Solutions like storing secrets in S3 (Option A) or managing them manually (Option C) can lead to increased complexity, security risks, and operational overhead.

# Memory Tip
- Remember: **""Secrets Manager is a vault that rotates, while others might just be a box left locked.""**"
252,"A developer is building a serverless application that runs on AWS. The developer
wants to create an accelerated development workflow that deploys incremental
changes to AWS for testing. The developer wants to deploy the incremental
changes but does not want to fully deploy the entire application to AWS for
every code commit.

What should the developer do to meet these requirements?",Use the AWS Serverless Application Model (AWS SAM) to build the application. Use the sam sync command to deploy the incremental changes.,Use the AWS Serverless Application Model (AWS SAM) to build the application. Use the sam init command to deploy the incremental changes.,Use the AWS Cloud Development Kit (AWS CDK) to build the application. Use the cdk synth command to deploy the incremental changes.,Use the AWS Cloud Development Kit (AWS CDK) to build the application. Use the cdk bootstrap command to deploy the incremental changes.,"# Answer
- **Correct option:** A
- **Reason:** The AWS Serverless Application Model (AWS SAM) provides a streamlined way to build serverless applications. The `sam sync` command specifically allows developers to deploy only the changes made to their application without requiring a full deployment, which is ideal for an incremental development workflow. This feature significantly speeds up testing and development cycles by avoiding the overhead of re-deploying the entire application.

# Example / Analogy
- Think of a painter who doesn't need to repaint the entire canvas every time they want to change a single color. Instead, they only touch up sections that need work. Similarly, `sam sync` lets a developer modify and deploy just the portions of their application that have changed, saving time and resources.

# Common Mistakes / Traps
- A common mistake is confusing `sam init` with deploying changes. `sam init` is used to initialize a new serverless application, not for deploying changes. Also, it’s easy to think that all commands will always deploy the entire application, but specific commands like `sam sync` exist to facilitate incremental deployments.

# Memory Tip
- Remember: **""Sync to change, not full paint.""** Use `sam sync` for swift, incremental deployments!"
253,"A developer is building an application that will use an Amazon API Gateway API
with an AWS Lambda backend. The team that will develop the frontend requires
immediate access to the API endpoints to build the UI. To prepare the backend
application for integration, the developer needs to set up endpoints. The
endpoints need to return predefined HTTP status codes and JSON responses for the
frontend team. The developer creates a method for an API resource.

Which solution will meet these requirements?",Set the integration type to AWS_PROXY. Provision Lambda functions to return hardcoded JSON data.,Set the integration type to MOCK. Configure the method's integration request and integration response to associate a JSON responses with specific HTTP status codes.,Set the integration type to HTTP_PROXY. Configure API Gateway to pass all requests to an external placeholder API. which the team will build.,Set the integration type to MOCK. Use a method request to define HTTP status codes. Use an integration request to define JSON responses.,"# Answer
- **Correct option:** B

- **Reason:** Setting the integration type to MOCK allows the developer to simulate responses from the API Gateway without involving any backend resources such as Lambda functions. This setup enables the developer to define specific HTTP status codes and return JSON responses, which is exactly what the frontend team needs for their UI development. It’s a quick way to provide immediate access to endpoints without needing a functional backend yet.

# Example / Analogy
- Think of a mock API as a stage play where actors perform a script. The actors (API Gateway) present predefined lines (HTTP status codes and JSON responses) even if the actual backend (the real script) isn't ready. This lets the frontend team see and use the “performance” for their development while the backend is still being built.

# Common Mistakes / Traps
- A common mistake is choosing AWS_PROXY, thinking it's easier or quicker. However, AWS_PROXY sends requests to actual Lambda functions, which requires them to be coded and deployed, making it less suitable for quick mock responses required by the frontend team.

# Memory Tip
- Remember: **MOCK setups provide quick responses without a working backend.**"
254,"A company runs a serverless application on AWS. The application includes an AWS
Lambda function. The Lambda function processes data and stores the data in an
Amazon RDS for PostgreSQL database. A developer created a user credentials in
the database for the application.

The developer needs to use AWS Secrets Manager to manage the user credentials.
The password must to be rotated on a regular basis. The solution needs to ensure
that there is high availability and no downtime for the application during
secret rotation.

What should the developer do to meet these requirements?",Configure managed rotation with the single user rotation strategy.,Configure managed rotation with the alternating users rotation strategy.,Configure automatic rotation with the single user rotation strategy.,Configure automatic rotation with the alternating users rotation strategy.,"# Answer
- **Correct option:** B: Configure managed rotation with the alternating users rotation strategy.

- **Reason:** The alternating users rotation strategy allows for high availability by maintaining both the old and new credentials valid during the rotation process. It ensures that the application can seamlessly switch from the old password to the new one without any downtime, as both credentials are valid temporarily, reducing the risk of service disruption.

# Example / Analogy
- Imagine a scenario in a restaurant where a chef is accustomed to working with a specific set of tools (the old credentials). When a new set of tools (the new credentials) arrives, instead of disposing of the old tools immediately, the chef keeps both sets at hand. This way, the chef can gradually transition to the new tools while still being able to prepare dishes using the old ones if needed. This strategy ensures that dinner service continues uninterrupted!

# Common Mistakes / Traps
- A common mistake is assuming that all rotation strategies guarantee no downtime. Many may confuse the single user rotation strategy to be sufficient, but it can lead to downtime as the old user credential becomes invalid right after rotation. 

# Memory Tip
- **Think of “alternating” as “always ready” – two users meaning no downtime!**"
255,"A developer is building an application on AWS. The application includes an AWS
Lambda function that processes messages from an Amazon Simple Queue Service
(Amazon SQS) queue.

The Lambda function sometimes fails or times out. The developer needs to figure
out why the Lambda function fails to process some messages.

Which solution will meet these requirements with the LEAST operational overhead?",Increase the maximum timeout of the Lambda function to 15 minutes. Check the AWS CloudTrail event history for error details.,Increase the visibility timeout of the SQS queue. Check logs in Amazon CloudWatch Logs for error details.,Create a dead-letter queue. Configure the Lambda function to send the failed messages to the dead-letter queue.,Create an Amazon DynamoDB table. Update the Lambda function to send the failed messages to the DynamoDB table.,"# Answer
- **Correct option:** C

- **Reason:** Creating a dead-letter queue (DLQ) for the Lambda function allows it to handle failed messages without losing them. This means that when processing a message fails, the function automatically sends that message to the DLQ. With the DLQ, the developer can review all failed messages later in a separate queue, analyze the issues that caused failures, and take appropriate actions to debug the Lambda function's handling.

# Example / Analogy
- Think of a dead-letter queue like a lost and found box at a school. When a student can’t find their homework, instead of just getting rid of it, the teacher places it in the lost and found. Later, the student can check the box to retrieve their homework and find out what went wrong.

# Common Mistakes / Traps
- A common mistake is to assume that simply increasing timeouts or changing configurations of the SQS queue will resolve the core issue. These measures do not provide insights into the nature of the failures, whereas a DLQ gives direct visibility into what messages are failing.

# Memory Tip
- **Remember**: A dead-letter queue catches lost messages for review, helping you debug and fix failures efficiently."
256,"A developer needs to deploy an application in three AWS Regions by using AWS
CloudFormation. Each Region will use an AWS Elastic Beanstalk environment with
an Application Load Balancer (ALB). The developer wants to use AWS Certificate
Manager (ACM) to deploy SSL certificates to each ALB.

Which solution will meet these requirements?",Create a certificate in ACM in any one of the Regions. Import the certificate into the ALB that is in each Region.,Create a global certificate in ACM. Update the CloudFormation template to deploy the global certificate to each ALB.,Create a certificate in ACM in each Region. Import the certificate into the ALB for each Region.,Create a certificate in ACM in the us-east-1 Region. Update the CloudFormation template to deploy the certificate to each ALB.,"# Answer
- **Correct option:** C : Create a certificate in ACM in each Region. Import the certificate into the ALB for each Region.

- **Reason:** AWS Certificate Manager (ACM) manages certificates that are region-specific for AWS services. For an SSL certificate to be used with an ALB in a specific AWS Region, the certificate must be created in that same Region. Therefore, the correct approach is to create a certificate in ACM for each Region where the application will be deployed and then associate each certificate with the respective ALB.

# Example / Analogy
- Think of it like getting a driver’s license. You must apply for and obtain a driver’s license in the state where you reside. If you move to a different state, you can't use your old driver's license; you need to get a new one from the new state. Similarly, SSL certificates must be created in the specific region they will be used.

# Common Mistakes / Traps
- A common mistake is thinking that SSL certificates can be shared across regions or assuming a single certificate can apply to multiple regions. This misconception could lead to deployment issues.

# Memory Tip
- ""Certificates are regional: always create them where you deploy!"""
257,"A company built an online event platform. For each event, the company organizes
quizzes and generates leaderboards that are based on the quiz scores. The
company stores the leaderboard data in Amazon DynamoDB and retains the data for
30 days after an event is complete. The company then uses a scheduled job to
delete the old leaderboard data.

The DynamoDB table is configured with a fixed write capacity. During the months
when many events occur, the DynamoDB write API requests are throttled when the
scheduled delete job runs.

A developer must create a long-term solution that deletes the old leaderboard
data and optimizes write throughput.

Which solution meets these requirements?",Configure a TTL attribute for the leaderboard data.,Use DynamoDB Streams to schedule and delete the leaderboard data.,Use AWS Step Functions to schedule and delete the leaderboard data.,Set a higher write capacity when the scheduled delete job runs.,"# Answer
- **Correct option:** A : Configure a TTL attribute for the leaderboard data.
- **Reason:** Configuring a Time to Live (TTL) attribute for the leaderboard data allows DynamoDB to automatically delete items after a specified time period (30 days in this case). This solution optimizes write throughput because it eliminates the need for a scheduled job that can cause throttling during peak periods, as DynamoDB manages the deletion process in the background.

# Example / Analogy
- Real-life example or memorable comparison: Think of TTL as setting an expiration date on food. Just as you automatically throw away expired food to maintain space in your pantry without needing to plan a dedicated trash day, TTL automatically removes old data from DynamoDB without you needing to schedule and run a separate deletion job.

# Common Mistakes / Traps
- Common trap points, if any: A common mistake could be thinking that a scheduled delete job is the only solution. Candidates may overlook the efficiency and automation provided by TTL. Additionally, some may confuse TTL with DynamoDB Streams or AWS Step Functions, which are not built specifically for automatic data expiration.

# Memory Tip
- 1 quick tip: ""TTL is like a food expiration date—automatically clears out the old stuff!"""
258,"A company uses an AWS Lambda function that reads messages from an Amazon Simple
Queue Service (Amazon SQS) standard queue. The Lambda function makes an HTTP
call to a third-party API for each message. The company wants to ensure that the
Lambda function does not overwhelm the third-party API with more than two
concurrent requests.

Which solution will meet these requirements?",Configure a provisioned concurrency of two on the Lambda function.,Configure a batch size of two on the Amazon SQS event source mapping for the Lambda function.,Configure Lambda event filtering to process two messages from Amazon SQS at every invocations.,Configure a maximum concurrency of two on the Amazon SQS event source mapping for the Lambda function.,"# Answer
- **Correct option:** D : Configure a maximum concurrency of two on the Amazon SQS event source mapping for the Lambda function.
- **Reason:** Setting a maximum concurrency of two on the event source mapping directly limits the number of concurrent executions of the Lambda function that can be triggered by messages from the SQS queue. This ensures that only two requests at a time are sent to the third-party API, thus preventing overload.

# Example / Analogy
- Imagine a restaurant kitchen where chefs can only handle two orders at a time. If three or more orders come in, the restaurant won't accept more until one of the chefs has finished. This way, they ensure high quality and timely service, much like how the maximum concurrency setting in Lambda ensures the third-party API isn't overwhelmed with requests.

# Common Mistakes / Traps
- A common mistake is confusing ""provisioned concurrency"" with ""maximum concurrency."" Provisioned concurrency guarantees a certain number of instances running, while maximum concurrency limits how many can be running concurrently for a specific event source.

# Memory Tip
- **Limit requests like a busy kitchen: set a maximum to avoid overwhelming the chef!**"
259,"A company is using Amazon API Gateway to develop an API for its application on
AWS. A developer needs to test and generate API responses. Other teams are
required to test the API immediately.

What should the developer do to meet these requirements?",Set up a mock integration request in API Gateway. Configure the method's integration request and integration response to associate a response with a given status code.,Set up the request validators in the API's OpenAPI definition file. Import the OpenAPI definitions into API Gateway to test the API.,Set up a gateway response for the API in API Gateway. Configure response headers with hardcoded HTTP status codes and responses.,Set up a request parameter-based Lambda authorizer to control access to the API. Configure the Lambda function with the necessary mapping template.,"# Answer
- **Correct option:** A

- **Reason:** Setting up a mock integration request in API Gateway allows the developer to create simulated responses without needing to implement the backend logic immediately. This enables quick testing and immediate access for other teams to interact with the API, as they can see what responses to expect for given requests.

# Example / Analogy
- Imagine you're a chef preparing a new dish. Before the main ingredient (like the meat) arrives, you can still create a mock version of the dish with similar flavors using substitutes. This allows your team (the taste testers) to experience the dish and provide feedback before the final version is ready.

# Common Mistakes / Traps
- A common mistake may be to think that setting up a Lambda function for responses (Option D) might also suffice; however, it involves more complexity and is not necessary for immediate testing purposes. Also, some might confuse mock integration with more complex methods like API Gateway integration with AWS services, which aren't needed in this scenario.

# Memory Tip
- **Mock your API responses to speed up testing!**"
260,"A developer built an application that calls an external API to obtain data,
processes the data, and saves the result to Amazon S3. The developer built a
container image with all of the necessary dependencies to run the application as
a container.

The application runs locally and requires minimal CPU and RAM resources. The
developer has created an Amazon ECS cluster. The developer needs to run the
application hourly in Amazon Elastic Container Service (Amazon ECS).

Which solution will meet these requirements with the LEAST amount of
infrastructure management overhead?",Add a capacity provider to manage instances.,Add an Amazon EC2 instance that runs the application.,Define a task definition with an AWS Fargate launch type.,Create an Amazon ECS cluster and add the managed node groups feature to run the application.,"# Answer
- **Correct option:** C : Define a task definition with an AWS Fargate launch type.

- **Reason:** Using AWS Fargate for this application allows the developer to run the container without having to manage the underlying EC2 instances. Fargate automatically provisions the compute resources needed to run containers, which minimizes the operational overhead. Since the application requires minimal CPU and RAM, Fargate is a cost-effective and efficient solution for running it on a schedule, such as hourly.

# Example / Analogy
- Think of AWS Fargate like a ride-share service (like Uber or Lyft) for your containers. Instead of owning a vehicle (EC2 instances), you simply request a ride (compute resources) when you need it, and you don't worry about vehicle maintenance or fuel costs. This allows you to focus on your application rather than infrastructure.

# Common Mistakes / Traps
- A common mistake is overestimating the need for infrastructure management. Some may think they need to use EC2 instances (options B and D) without realizing that Fargate provides a simpler, scalable option with less hassle. Another trap is underestimating the flexibility and ease of Fargate for running containers on a schedule.

# Memory Tip
- Remember: Fargate = No servers to manage!"
261,"A company runs an application on AWS. The application stores data in an Amazon
DynamoDB table. Some queries are taking a long time to run. These slow queries
involve an attribute that is not the table's partition key or sort key.

The amount of data that the application stores in the DynamoDB table is expected
to increase significantly. A developer must increase the performance of the
queries.

Which solution will meet these requirements?",Increase the page size for each request by setting the Limit parameter to be higher than the default value. Configure the application to retry any request that exceeds the provisioned throughput.,Create a global secondary index (GSI). Set query attribute to be the partition key of the index.,"Perform a parallel scan operation by issuing individual scan requests. In the parameters, specify the segment for the scan requests and the total number of segments for the parallel scan.",Turn on read capacity auto scaling for the DynamoDB table. Increase the maximum read capacity units (RCUs).,"# Answer
- **Correct option:** B : Create a global secondary index (GSI). Set query attribute to be the partition key of the index.

- **Reason:** A global secondary index (GSI) allows you to query the data in a DynamoDB table using an alternative key. Since the slow queries involve an attribute that is not the table's primary key (the partition key or sort key), creating a GSI with the desired attribute as the partition key will significantly enhance the query performance. This is because the GSI grants you the ability to retrieve data based on that non-key attribute, facilitating faster lookups and reducing query times, especially as the amount of data grows.

# Example / Analogy
- Think of a library where books are organized only by author names (the primary key). If you want to find books by genre (a non-primary attribute), it would take a long time to search through all the shelves. However, if the library created a separate section where books are organized by genre (like a GSI), you could find them quickly without having to search through every shelf.

# Common Mistakes / Traps
- A common mistake is to assume that simply increasing the limits or read capacity will address the performance issue without restructuring how the data is accessed. Also, many might think that parallel scans will improve performance, but scans are generally less efficient than queries, especially on large datasets.

# Memory Tip
- **Remember:** Use a GSI for fast queries on non-key attributes!"
