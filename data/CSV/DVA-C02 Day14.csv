No,Content,A,B,C,D,Answered
262,"A company is using Amazon API Gateway to invoke a new AWS Lambda function. The
company has Lambda function versions in its PROD and DEV environments. In each
environment, there is a Lambda function alias pointing to the corresponding
Lambda function version. API Gateway has one stage that is configured to point
at the PROD alias.

The company wants to configure API Gateway to enable the PROD and DEV Lambda
function versions to be simultaneously and distinctly available.

Which solution will meet these requirements?",Enable a Lambda authorizer for the Lambda function alias in API Gateway. Republish PROD and create a new stage for DEV. Create API Gateway stage variables for the PROD and DEV stages. Point each stage variable to the PROD Lambda authorizer to the DEV Lambda authorizer.,Set up a gateway response in API Gateway for the Lambda function alias. Republish PROD and create a new stage for DEV. Create gateway responses in API Gateway for PROD and DEV Lambda aliases.,Use an environment variable for the Lambda function alias in API Gateway. Republish PROD and create a new stage for development. Create API gateway environment variables for PROD and DEV stages. Point each stage variable to the PROD Lambda function alias to the DEV Lambda function alias.,Use an API Gateway stage variable to configure the Lambda function alias. Republish PROD and create a new stage for development. Create API Gateway stage variables for PROD and DEV stages. Point each stage variable to the PROD Lambda function alias and to the DEV Lambda function alias.,"# Answer
- **Correct option:** D
- **Reason:** Option D correctly indicates the use of API Gateway stage variables to configure the Lambda function alias. By creating separate stages for PROD and DEV in API Gateway and using stage variables, the company can easily point to the respective Lambda function aliases for production and development environments simultaneously.

# Example / Analogy
- Imagine a restaurant that has two menus, one for regular customers (PROD) and another for special tasting events (DEV). By having two distinct menus (API Gateway stages) but using the same kitchen (Lambda function aliases), customers can choose what they want without issues, and the restaurant can manage culinary creations distinctly.

# Common Mistakes / Traps
- A common mistake is to confuse stage variables with environment variables or to think they work in the same way. Environment variables are typically used inside Lambda functions for configurations, while stage variables in API Gateway are not interchangeable with them.

# Memory Tip
- Remember: **“Stage variables in API Gateway = distinct paths for different environments!”**"
263,"A company runs a batch processing application by using AWS Lambda functions and
Amazon API Gateway APIs with deployment stages for development, user acceptance
testing, and production. A development team needs to configure the APIs in the
deployment stages to connect to third-party service endpoints.

Which solution will meet this requirement?",Store the third-party service endpoints in Lambda layers that correspond to the stage.,Store the third-party service endpoints in API Gateway stage variables that correspond to the stage.,Encode the third-party service endpoints as query parameters in the API Gateway request URL.,Store the third-party service endpoint for each environment in AWS AppConfig.,"# Answer
- **Correct option:** B : Store the third-party service endpoints in API Gateway stage variables that correspond to the stage.

- **Reason:** API Gateway stage variables are designed specifically for storing configuration values that can differ between deployment stages (like development, testing, and production). By using stage variables, you can easily switch between different third-party service endpoints based on the deployment stage without changing your API's code. This makes configuration management simpler and reduces the risk of errors.

# Example / Analogy
- Think of stage variables as different room setups for a conference. In one room (development), you might have a projector, while in another room (production), you might have a large screen. Each setup is tailored to its purpose, and you don't need to change the presentation itself; you simply choose a room that has the right setup.

# Common Mistakes / Traps
- A common mistake is to think that hardcoding service endpoints into the Lambda function or making them as query parameters is a good practice. This approach can lead to error-prone deployments since it requires code changes for environment transitions and can expose sensitive information in URLs.

# Memory Tip
- ""Use stage variables in API Gateway for clean environment configuration."""
264,"A developer is migrating an application to Amazon Elastic Kubernetes Service
(Amazon EKS). The developer migrates the application to Amazon Elastic Container
Registry (Amazon ECR) with an EKS cluster. As part of the application migration
to a new backend, the developer creates a new AWS account. The developer makes
configuration changes to the application to point the application to the new AWS
account and to use new backend resources. The developer successfully tests the
changes within the application by deploying the pipeline.

The Docker image build and the pipeline deployment are successful, but the
application is still connecting to the old backend. The developer finds that the
application's configuration is still referencing the original EKS cluster and
not referencing the new backend resources.

Which reason can explain why the application is not connecting to the new
resources?",The developer did not successfully create the new AWS account.,The developer added a new tag to the Docker image.,The developer did not update the Docker image tag to a new version.,The developer pushed the changes to a new Docker image tag.,"# Answer
- **Correct option:** C : The developer did not update the Docker image tag to a new version.

- **Reason:** The application is likely still using the previous version of the Docker image that references the old backend configuration. In Kubernetes, when you deploy a new version of a containerized application, you need to specify the new image tag that corresponds to the updated application code. If the tag remains unchanged, Kubernetes will continue to deploy the existing image, thus connecting to the old backend.

# Example / Analogy
- Imagine you are a chef in a restaurant, and you have two versions of a recipe: one for spaghetti with sauce A and one for spaghetti with sauce B. If you plan to serve the new spaghetti with sauce B but mistakenly use the old recipe meant for sauce A, the customers will receive the same old dish. Similarly, in this case, without updating the Docker image tag, the application continues to use the old ""recipe"" pointing to the previous backend.

# Common Mistakes / Traps
- A common misconception is assuming that changes in the application code or environment variables are sufficient to point to new resources. It's crucial to also ensure that the Docker image is updated with the correct tag that contains these changes.

# Memory Tip
- Always remember: **Update the Docker image tag to serve the latest recipe!**"
265,"A developer is creating an application that reads and writes to multiple Amazon
S3 buckets. The application will be deployed to an Amazon EC2 instance. The
developer wants to make secure API requests from the EC2 instances without the
need to manage the security credentials for the application. The developer needs
to apply the principle of least privilege.

Which solution will meet these requirements?",Create an IAM user. Create access keys and secret keys for the user. Associate the user with an IAM policy that allows s3:* permissions.,Associate the EC2 instance with an IAM role that has an IAM policy that allows s3:ListBucket and s3:*Object permissions for specific S3 buckets.,Associate the EC2 instance with an IAM role that has an AmazonS3FullAccess AWS managed policy.,Create a bucket policy on the S3 bucket that allows s3:ListBucket and s3:*Object permissions to the EC2 instance.,"# Answer
- **Correct option:** B
- **Reason:** Associating the EC2 instance with an IAM role allows the application to securely obtain temporary AWS credentials that grant the necessary permissions (like s3:ListBucket and s3:*Object) without hardcoding access keys or managing them. This approach adheres to the principle of least privilege by giving only the necessary permissions to the specific S3 buckets needed.

# Example / Analogy
- Imagine you are a guest at a hotel (the EC2 instance) that provides you with a temporary keycard (the IAM role) that only works for your room (the specific S3 buckets). You don’t have to worry about losing a permanent key or having to check-in and out all the time; your access is limited to what you need and is managed by the hotel.

# Common Mistakes / Traps
- A common mistake is to choose option A and create an IAM user with static access keys. This approach requires managing and rotating credentials, which is not secure and does not follow best practices. Option C grants excessive permissions using AmazonS3FullAccess, violating the least privilege principle. Option D incorrectly assumes bucket policies can replace IAM roles, which they cannot for instance-level permissions.

# Memory Tip
- Always use IAM roles for EC2 instances to avoid hardcoded credentials and ensure least privilege!"
266,"A developer is writing an application that will retrieve sensitive data from a
third-party system. The application will format the data into a PDF file. The
PDF file could be more than 1 MB. The application will encrypt the data to disk
by using AWS Key Management Service (AWS KMS). The application will decrypt the
file when a user requests to download it. The retrieval and formatting portions
of the application are complete.

The developer needs to use the GenerateDataKey API to encrypt the PDF file so
that the PDF file can be decrypted later. The developer needs to use an AWS KMS
symmetric customer managed key for encryption.

Which solutions will meet these requirements?",Write the encrypted key from the GenerateDataKey API to disk for later use. Use the plaintext key from the GenerateDataKey API and a symmetric encryption algorithm to encrypt the file.,Write the plain text key from the GenerateDataKey API to disk for later use. Use the encrypted key from the GenerateDataKey API and a symmetric encryption algorithm to encrypt the file.,Write the encrypted key from the GenerateDataKey API to disk for later use. Use the plaintext key from the GenerateDataKey API to encrypt the file by using the KMS Encrypt API.,Write the plain text key from the GenerateDataKey API to disk for later use. Use the encrypted key from the GenerateDataKey API to encrypt the file by using the KMS Encrypt API.,"# Answer
- **Correct option:** A
- **Reason:** The GenerateDataKey API call returns two keys: a plaintext data key and an encrypted copy of the data key. To securely encrypt the PDF file, you would use the plaintext key to perform the encryption directly. You then store the encrypted key on disk for later use to decrypt the data key when you need to access it. This ensures that the plaintext key, which is used to encrypt sensitive data, is never written to disk.

# Example / Analogy
- Imagine you have a very valuable jewelry box (the PDF file) that you want to lock. Instead of writing down the combination to the lock and leaving it next to the box (which would be like writing the plaintext key to disk), you write down the secret in a secure vault (storing the encrypted key). When you need to open the box, you can retrieve your combination from the vault (decrypt the key) and safely access your jewelry (the encrypted PDF).

# Common Mistakes / Traps
- A common mistake is to confuse which key to store and how to use them. Some may think that the plaintext key can be stored safely, but it should never be written to disk. Participants should focus on how the keys are intended to be used and secured.

# Memory Tip
- ""Encrypt with the key you hold, store the lock securely."""
267,"A company wants to test its web application more frequently. The company deploys
the application by using a separate AWS CloudFormation stack for each
environment. The company deploys the same CloudFormation template to each stack
as the application progresses through the development lifecycle.

A developer needs to build in notifications for the quality assurance (QA) team.
The developer wants the notifications to occur for new deployments in the final
preproduction environment.

Which solution will meet these requirements?",Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the QA team to the Amazon SNS topic. Update the CloudFormation stack options to point to the SNS topic in the pre-production environment.,Create an AWS Lambda function that notifies the QA team. Create an Amazon EventBridge rule to invoke the Lambda function on the default event bus. Filter the events on the CloudFormation service and on the CloudFormation stack Amazon Resource Name (ARN).,Create an Amazon CloudWatch alarm that monitors the metrics from CloudFormation. Filter the metrics on the stack name and the stack status. Configure the CloudWatch alarm to notify the QA team.,Create an AWS Lambda function that notifies the QA team. Configure the event source mapping to receive events from CloudFormation. Specify the filtering values to limit invocations to the desired CloudFormation stack.,"# Answer
- **Correct option:** A

- **Reason:**  
Option A is the most straightforward solution for sending notifications to the QA team upon deploying the CloudFormation stack in the pre-production environment. By creating an Amazon Simple Notification Service (Amazon SNS) topic and subscribing the QA team to it, the developer can configure the CloudFormation stack to publish notifications to that SNS topic whenever there is a new deployment. This keeps the process simple and directly tied to CloudFormation without additional complexity.

# Example / Analogy
- Real-life example or memorable comparison: Think of SNS as a public announcement system in a building. When a new office is opened (a new deployment), the building manager makes an announcement over the PA system that alerts everyone, including the QA team, about this new change. It’s an efficient and direct way to ensure everyone is informed.

# Common Mistakes / Traps
- Common trap points, if any: A common mistake would be to overcomplicate the solution by trying to use Lambda functions when simpler solutions like SNS notification can be directly integrated. Some may also overlook the need for clear communication to be established upon deployment, thinking more complex systems will suffice when a straightforward alert mechanism is available.

# Memory Tip
- 1 quick tip: ""Use SNS for simple notifications and keep it direct!"""
268,"A developer manages three AWS accounts. Each account contains an Amazon RDS DB
instance in a private subnet. The developer needs to define users in each
database in a consistent way. The developer must ensure that the same users are
created and updated later in all three accounts.

Which solution will meet these requirements with the MOST operational
efficiency?",Create an AWS CloudFormation template. Declare the users in the template. Attach the users to the database. Deploy the template in each account.,Create an AWS CloudFormation template that contains a custom resource to create the users in the database. Deploy the template in each account.,Write a script that creates the users. Deploy an Amazon EC2 instance in each account to run the script on the databases. Run the script in each account.,Implement an AWS Lambda function that creates the users in the database. Provide the function with the details of all three accounts.,"# Answer
- **Correct option:** B 
- **Reason:** Creating an AWS CloudFormation template that includes a custom resource to handle user creation in the database allows for consistent, repeatable deployment across different accounts. CloudFormation manages resources declaratively and can encapsulate the logic required to create and update users, ensuring that the same users are maintained properly in all instances.

# Example / Analogy
- Think of CloudFormation as a blueprint for building a house. If you want the same house in three different neighborhoods (accounts), you can simply share the same blueprint with a builder (CloudFormation). If you need to update something about the house (users), you only need to modify the blueprint, and the builders will know how to implement that change efficiently without needing to remember all the specific details of each house.

# Common Mistakes / Traps
- A common mistake would be to think that deploying scripts manually or through EC2 instances (options C and D) is sufficient. This approach introduces more operational overhead and inconsistency, as it relies on manual execution and could lead to errors or discrepancies across the accounts.

# Memory Tip
- Use CloudFormation for consistent resource management—it's your uniform blueprint for AWS resources!"
269,"A company is building a new application that runs on AWS and uses Amazon API
Gateway to expose APIs. Teams of developers are working on separate components
of the application in parallel. The company wants to publish an API without an
integrated backend so that teams that depend on the application backend can
continue the development work before the API backend development is complete.

Which solution will meet these requirements?",Create API Gateway resources and set the integration type value to MOCK. Configure the method integration request and integration response to associate a response with an HTTP status code. Create an API Gateway stage and deploy the API.,Create an AWS Lambda function that returns mocked responses and various HTTP status codes. Create API Gateway resources and set the integration type value to AWS_PROXY. Deploy the API.,Create an EC2 application that returns mocked HTTP responses. Create API Gateway resources and set the integration type value to AWS. Create an API Gateway stage and deploy the API.,Create API Gateway resources and set the integration type value set to HTTP_PROXY. Add mapping templates and deploy the API. Create an AWS Lambda layer that returns various HTTP status codes. Associate the Lambda layer with the API deployment.,"# Answer
- **Correct option:** A 
- **Reason:** Creating API Gateway resources with the integration type set to MOCK allows developers to simulate responses for their APIs without needing a functioning backend. This helps teams continue working and testing their components while the actual backend is still under development.

# Example / Analogy
- Think of it like building a movie set without the actors. You can create the scenery (API with mocked responses) to test how the camera angles (API calls) will work before the actors (backend services) arrive. This way, the film crew can work on lighting and scene setups while waiting for the performers.

# Common Mistakes / Traps
- A common mistake is to assume that we need to have a Lambda function or a specific backend setup to mock responses when API Gateway's built-in MOCK integration can efficiently fulfill the requirement without any additional setup.

# Memory Tip
- Remember: ""MOCK integration = build the set for the actors to come."""
270,"An application that runs on AWS receives messages from an Amazon Simple Queue
Service (Amazon SQS) queue and processes the messages in batches. The
application sends the data to another SQS queue to be consumed by another legacy
application. The legacy system can take up to 5 minutes to process some
transaction data.

A developer wants to ensure that there are no out-of-order updates in the legacy
system. The developer cannot alter the behavior of the legacy system.

Which solution will meet these requirements?",Use an SQS FIFO queue. Configure the visibility timeout value.,Use an SQS standard queue with a SendMessageBatchRequestEntry data type. Configure the DelaySeconds values.,Use an SQS standard queue with a SendMessageBatchRequestEntry data type. Configure the visibility timeout value.,Use an SQS FIFO queue. Configure the DelaySeconds value.,"# Answer
- **Correct option:** A: Use an SQS FIFO queue. Configure the visibility timeout value.

- **Reason:** A FIFO (First-In-First-Out) queue ensures that messages are processed in the exact order they are received. This is crucial for the legacy system that cannot process messages in parallel without risking out-of-order updates. The visibility timeout setting allows you to manage how long a message remains invisible to other consumers after being read, giving the legacy system ample time (up to 5 minutes) to process the message before it becomes visible again in the queue.

# Example / Analogy
- Imagine you’re at a ticket counter where people must be served in the order they arrived. If you take a number and wait, you are guaranteed that when your number is called, no one who arrived later will cut in line. Similarly, a FIFO queue processes messages in the exact order they are received, preventing confusion and ensuring that everyone (or in this case, every message) is handled properly without interruptions.

# Common Mistakes / Traps
- A common mistake is to assume that a standard SQS queue can handle ordered processing due to its ability to send messages in batches. Standard queues may lead to out-of-order delivery, which does not meet the requirements specified for the legacy application.

# Memory Tip
- Always remember: FIFO = First-In-First-Out, for ordered and reliable message processing!"
271,"A company is building a compute-intensive application that will run on a fleet
of Amazon EC2 instances. The application uses attached Amazon Elastic Block
Store (Amazon EBS) volumes for storing data. The Amazon EBS volumes will be
created at time of initial deployment. The application will process sensitive
information. All of the data must be encrypted. The solution should not impact
the application's performance.

Which solution will meet these requirements?",Configure the fleet of EC2 instances to use encrypted EBS volumes to store data.,Configure the application to write all data to an encrypted Amazon S3 bucket.,Configure a custom encryption algorithm for the application that will encrypt and decrypt all data.,Configure an Amazon Machine Image (AMI) that has an encrypted root volume and store the data to ephemeral disks.,"# Answer
- **Correct option:** A : Configure the fleet of EC2 instances to use encrypted EBS volumes to store data.
- **Reason:** Encrypted EBS volumes provide a way to secure data at rest while maintaining high performance. AWS manages encryption transparently without significantly affecting I/O performance, making it ideal for compute-intensive applications that process sensitive information.

# Example / Analogy
- Think of encrypted EBS volumes like locking your files in a safe while still being able to access them quickly. Just as you can open the safe and retrieve your documents without delay, EBS encryption allows your application to read and write data securely without introducing significant overhead.

# Common Mistakes / Traps
- A common mistake could be assuming that writing data to S3 (Option B) would be the best answer. While S3 does provide encryption, it introduces latency and would not be suitable for compute-intensive applications that require fast access to data stored directly on EBS volumes.

# Memory Tip
- Remember: ""Encrypted EBS is like a safe with quick access - secure and fast!"""
272,"A developer is updating the production version of an AWS Lambda function to fix
a defect. The developer has tested the updated code in a test environment. The
developer wants to slowly roll out the updates to a small subset of production
users before rolling out the changes to all users. Only 10% of the users should
be initially exposed to the new code in production.

Which solution will meet these requirements?","Update the Lambda code and create a new version of the Lambda function. Create a Lambda function trigger. Configure the traffic weights in the trigger between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version.","Create a new Lambda function that uses the updated code. Create a Lambda alias for the production Lambda function. Configure the Lambda alias to send 90% of the traffic to the production Lambda function, and send 10% of the traffic to the test Lambda function.","Update the Lambda code and create a new version of the Lambda function. Create a Lambda proxy integration. Configure the Lambda proxy to split traffic between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version.","Update the Lambda code and create a new version of the Lambda function. Create a Lambda function alias. Configure the traffic weights in the Lambda alias between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version.","# Answer
- **Correct option:** D
- **Reason:** Using a Lambda alias, you can manage different versions of a Lambda function and control traffic between them. This allows you to send 90% of the traffic to the stable production version and 10% to the new version. This method ensures that you can monitor the new code's behavior in the live environment while keeping most users on the older, more stable version.

# Example / Analogy
- Think of a restaurant introducing a new dish. Instead of serving it to everyone, they offer it to just 10% of customers initially to see how it performs. If it’s a hit, they gradually roll it out to more customers. Similarly, using Lambda aliases allows you to test new code with a small portion of users before full deployment.

# Common Mistakes / Traps
- A common mistake is choosing options that do not use the alias feature, such as A or B. These options create new Lambda functions or use triggers improperly, which can complicate the rollout process and reduce clarity and control over traffic management.

# Memory Tip
- **Remember:** Use Lambda aliases for safe traffic shifting between versions in production!"
273,"A developer is creating an AWS Lambda function that consumes messages from an
Amazon Simple Queue Service (Amazon SQS) standard queue. The developer notices
that the Lambda function processes some messages multiple times.

How should developer resolve this issue MOST cost-effectively?",Change the Amazon SQS standard queue to an Amazon SQS FIFO queue by using the Amazon SQS message deduplication ID.,Set up a dead-letter queue.,Set the maximum concurrency limit of the AWS Lambda function to 1.,Change the message processing to use Amazon Kinesis Data Streams instead of Amazon SQS.,"# Answer
- **Correct option:** A : Change the Amazon SQS standard queue to an Amazon SQS FIFO queue by using the Amazon SQS message deduplication ID. 
- **Reason:** Standard queues in Amazon SQS do not guarantee the order of messages and can deliver messages multiple times, especially in high-throughput scenarios. By switching to a FIFO (First-In-First-Out) queue, the developer can ensure that messages are processed in the exact order they arrive and utilize deduplication IDs to prevent duplicate processing of the same message.

# Example / Analogy
- Think of a FIFO queue like a line at a coffee shop; customers are served in the order they arrive. If you served customers out of order or allowed some to be served twice, it would create confusion. A FIFO queue ensures that each customer (message) is served exactly once and in the right order.

# Common Mistakes / Traps
- A common mistake is to think that using a dead-letter queue will solve the issue of duplicate message processing. While it helps with handling failures, it does not prevent duplicate delivery from happening in the first place.

# Memory Tip
- **""FIFO queues prevent the chaos of double servings!""**"
274,"A developer is optimizing an AWS Lambda function and wants to test the changes
in production on a small percentage of all traffic. The Lambda function serves
requests to a RE ST API in Amazon API Gateway. The developer needs to deploy
their changes and perform a test in production without changing the API Gateway
URL.

Which solution will meet these requirements?","Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. On the production API Gateway stage, define a canary release and set the percentage of traffic to direct to the canary release. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Publish the API to the canary stage.",Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Deploy a new API Gateway stage.,"Define an alias on the $LATEST version of the Lambda function. Update the API Gateway endpoint to reference the new Lambda function alias. Upload and publish the optimized Lambda function code. On the production API Gateway stage, define a canary release and set the percentage of traffic to direct to the canary release. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Publish to the canary stage.",Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Deploy the API to the production API Gateway stage.,"# Answer
- **Correct option:** C

- **Reason:** Option C outlines the correct steps to implement a canary release while maintaining traffic control without changing the API Gateway URL. It defines an alias for the Lambda function to point to the optimized version, which can then use the canary release feature of API Gateway. By using an alias, you can direct a specific percentage of traffic to the newly optimized function while keeping the old version intact for the remaining requests.

# Example / Analogy
- Think of this like trying a new flavor of ice cream in a shop. Instead of replacing the entire batch, the shop scoops out a small amount of the new flavor and allows customers to have a taste while still offering their popular flavors. This way, if the new flavor isn't well-received, they can easily revert back to just serving the old favorites.

# Common Mistakes / Traps
- A common mistake is to overlook the use of aliases and versions in Lambda functions. Many assume that updating to the latest version is enough, but this will redirect all traffic indiscriminately, which is not what the developer wants in a canary release. 

# Memory Tip
- ""Use an alias for gradual changes — it’s your safety net in production!"""
275,"A company notices that credentials that the company uses to connect to an
external software as a service (SaaS) vendor are stored in a configuration file
as plaintext.

The developer needs to secure the API credentials and enforce automatic
credentials rotation on a quarterly basis.

Which solution will meet these requirements MOST securely?",Use AWS Key Management Service (AWS KMS) to encrypt the configuration file. Decrypt the configuration file when users make API calls to the SaaS vendor. Enable rotation.,Retrieve temporary credentials from AWS Security Token Service (AWS STS) every 15 minutes. Use the temporary credentials when users make API calls to the SaaS vendor.,Store the credentials in AWS Secrets Manager and enable rotation. Configure the API to have Secrets Manager access.,Store the credentials in AWS Systems Manager Parameter Store and enable rotation. Retrieve the credentials when users make API calls to the SaaS vendor.,"# Answer
- **Correct option:** C
- **Reason:** Storing the API credentials in AWS Secrets Manager allows for secure storage, management, and automatic rotation of secrets. Secrets Manager is specifically designed for handling credentials and has built-in support for automatic rotation, ensuring that credentials are kept up-to-date without manual intervention.

# Example / Analogy
- Imagine you have a mailbox that you can open safely only when needed, and every month a key is automatically changed, ensuring that only authorized personnel can access it. AWS Secrets Manager works similarly; it securely manages your API keys and changes them automatically, so you don't have to worry about them being old or insecure.

# Common Mistakes / Traps
- A common mistake is to think that encrypting a plaintext file using AWS KMS will provide adequate security. While encryption is good, managing and rotating credentials through Secrets Manager is much more effective and secure in the context of secret management.

# Memory Tip
- Remember: **Secrets Manager = Secure storage + Automatic rotation = Peace of mind!**"
276,"A company has an application that is hosted on Amazon EC2 instances. The
application stores objects in an Amazon S3 bucket and allows users to download
objects from the S3 bucket. A developer turns on S3 Block Public Access for the
S3 bucket. After this change, users report errors when they attempt to download
objects. The developer needs to implement a solution so that only users who are
signed in to the application can access objects in the S3 bucket.

Which combination of steps will meet these requirements in the MOST secure way?
(Choose two.)",Create an EC2 instance profile and role with an appropriate policy. Associate the role with the EC2 instances.,Create an IAM user with an appropriate policy. Store the access key ID and secret access key on the EC2 instances.,Modify the application to use the S3 GeneratePresignedUrl API call.,Modify the application to use the S3 GetObject API call and to return the object handle to the user.,"# Answer
- **Correct options:** A, C 

- **Reason:** 
   - **Option A:** Creating an EC2 instance profile and role allows the EC2 instances to securely access AWS resources, such as S3, without needing to store access keys on the instance itself. This ensures that only authorized actions (like accessing S3 objects) can be performed by the application running on the EC2 instance.
   - **Option C:** Modifying the application to use the S3 GeneratePresignedUrl API call allows the application to generate temporary, time-limited URLs that users can use to download the objects. This approach ensures that only authenticated users (those signed into the application) can get the URL and access the S3 objects securely.

# Example / Analogy
Imagine a VIP concert where only ticket holders can enter. Instead of giving out permanent passes (like IAM users with access keys), the venue gives out temporary wristbands (presigned URLs) that allow access for just a few hours during the show (the expiration time). To get a wristband, attendees must prove they’re on the guest list (authenticating with the application).

# Common Mistakes / Traps
- A common mistake is choosing option B, which suggests using an IAM user's static credentials, compromising security by exposing access keys on the EC2 instance. This goes against best security practices.

# Memory Tip
- Remember: ""Use roles for EC2, presigned URLs for users."""
277,"An Amazon Simple Queue Service (Amazon SQS) queue serves as an event source for
an AWS Lambda function. In the SQS queue, each item corresponds to a video file
that the Lambda function must convert to a smaller resolution. The Lambda
function is timing out on longer video files, but the Lambda function's timeout
is already configured to its maximum value.

What should a developer do to avoid the timeouts without additional code
changes?",Increase the memory configuration of the Lambda function.,Increase the visibility timeout on the SQS queue.,Increase the instance size of the host that runs the Lambda function.,Use multi-threading for the conversion.,"# Answer
- **Correct option:** A: Increase the memory configuration of the Lambda function.

- **Reason:** Increasing the memory allocation for an AWS Lambda function not only provides more memory but also increases the CPU power available for that function. More memory and CPU can lead to faster processing times, which can help the Lambda function finish executing within its timeout limit even with longer video files. Since the function is already at its maximum timeout, optimizing resource allocation is the only possible solution.

# Example / Analogy
- Think of a Lambda function like a baker who can only bake a certain number of cakes at a time. If the baker is overwhelmed with large orders (longer video files), increasing the size of the oven (memory allocation) allows the baker to bake more cakes faster. This way, the baker completes all orders without exceeding their work time limit.

# Common Mistakes / Traps
- A common mistake is to overlook that simply increasing the timeout value will not solve the underlying performance issue if the function is already at its max timeout. Many might consider adjusting the SQS visibility timeout, which does not influence the Lambda execution time.

# Memory Tip
- Remember: ""More memory, more speed!"""
278,"A company is building an application on AWS. The application's backend includes
an Amazon API Gateway REST API. The company's frontend application developers
cannot continue work until the backend API is ready for integration. The company
needs a solution that will allow the frontend application developers to continue
their work.

Which solution will meet these requirements in the MOST operationally efficient
way?",Configure mock integrations for API Gateway API methods.,Integrate a Lambda function with API Gateway and return a mocked response.,Add new API endpoints to the API Gateway stage and returns a mocked response.,Configure a proxy resource for API Gateway API methods.,"# Answer
- **Correct option:** A : Configure mock integrations for API Gateway API methods.

- **Reason:** Mock integrations in API Gateway allow developers to create API endpoints that simulate responses from backend services without needing those services to be implemented. This enables frontend developers to continue their work asynchronously while the backend is being developed.

# Example / Analogy
- Think of a movie script where the actors need to rehearse their lines before the filming starts. By using a ""mock"" script that outlines potential dialogue (even if the actual script isn't ready), the actors can practice and prepare. In this context, a mock integration lets frontend developers ""practice"" making API calls to a simulated response, enabling them to proceed with development.

# Common Mistakes / Traps
- One common trap is choosing options that involve actual backend processes, such as integrating Lambda functions or adding endpoints that expect a real response. These options could create additional dependencies and are more complex than necessary when the goal is to simply allow frontend development to continue.

# Memory Tip
- To remember mock integrations: **""Mock first, build later!""**"
279,"A company is preparing to migrate an application to the company's first AWS
environment. Before this migration, a developer is creating a proof-of-concept
application to validate a model for building and deploying container-based
applications on AWS.

Which combination of steps should the developer take to deploy the containerized
proof-of-concept application with the LEAST operational effort? (Choose two.)",Package the application into a .zip file by using a command line tool. Upload the package to Amazon S3.,Package the application into a container image by using the Docker CLI. Upload the image to Amazon Elastic Container Registry (Amazon ECR).,Deploy the application to an Amazon EC2 instance by using AWS CodeDeploy.,Deploy the application to Amazon Elastic Kubernetes Service (Amazon EKS) on AWS Fargate.,"# Answer
- **Correct options:** B, D  
- **Reason:** 
  - Option B is correct because packaging the application into a container image using Docker allows the application to be deployed consistently across different environments. Uploading the image to Amazon ECR facilitates easy access for deployment.
  - Option D is correct because deploying the application to Amazon EKS on AWS Fargate means that the developer can run containers without needing to manage the underlying EC2 instances, reducing the operational effort significantly. Fargate handles the orchestration and scaling of the container, allowing the developer to focus on the application itself.

# Example / Analogy
- Imagine preparing a meal (the application) and using a meal kit service. Option B is like choosing pre-packaged ingredients (container image) that are sent to you (ECR), and option D is like having a chef (Fargate) who cooks the meal for you without you needing to worry about the kitchen tools or assistant cooks (EC2 instances). This way, you enjoy the meal with minimal effort in preparation and cleanup.

# Common Mistakes / Traps
- A common mistake is to think that deploying directly to EC2 or using CodeDeploy (options C) is the best approach. However, these methods typically involve more operational overhead as they require manual management of infrastructure. This question specifically looks for the **least operational effort**, making B and D the better choices.

# Memory Tip
- **Think containers over VMs for simplicity: Pack it (image) and serve it (Fargate)!**"
280,"A developer supports an application that accesses data in an Amazon DynamoDB
table. One of the item attributes is expirationDate in the timestamp format. The
application uses this attribute to find items, archive them, and remove them
from the table based on the timestamp value.

The application will be decommissioned soon, and the developer must find another
way to implement this functionality. The developer needs a solution that will
require the least amount of code to write.

Which solution will meet these requirements?",Enable TTL on the expirationDate attribute in the table. Create a DynamoDB stream. Create an AWS Lambda function to process the deleted items. Create a DynamoDB trigger for the Lambda function.,Create two AWS Lambda functions: one to delete the items and one to process the items. Create a DynamoDB stream. Use the DeleteItem API operation to delete the items based on the expirationDate attribute. Use the GetRecords API operation to get the items from the DynamoDB stream and process them.,Create two AWS Lambda functions: one to delete the items and one to process the items. Create an Amazon EventBridge scheduled rule to invoke the Lambda functions. Use the DeleteItem API operation to delete the items based on the expirationDate attribute. Use the GetRecords API operation to get the items from the DynamoDB table and process them.,Enable TTL on the expirationDate attribute in the table. Specify an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as the target to delete the items. Create an AWS Lambda function to process the items.,"# Answer
- **Correct option:** A

- **Reason:** Enabling TTL (Time to Live) on the `expirationDate` attribute allows DynamoDB to automatically delete items from the table when the timestamp value has passed, thus requiring the least amount of code to implement. Once TTL is enabled, DynamoDB handles the deletion without requiring any additional Lambda functions or custom code for periodic cleanup. The DynamoDB stream can capture any changes (deletions in this case) made to the items, which can then be processed if necessary using a Lambda function.

# Example / Analogy
- Think of TTL like food expiration dates on products in your pantry. Just as you might choose to throw out expired items automatically, DynamoDB automatically removes items when they ""expire"" based on the `expirationDate`. This means less effort is required to keep your pantry (or database) clean and organized!

# Common Mistakes / Traps
- A common trap is thinking you need to manage item deletions manually with Lambda functions or other complex solutions. People might overlook the built-in simplicity of TTL which can handle automatic deletions without extra code.

# Memory Tip
- Remember: **TTL automates expiration like food labels, saving time on manual cleanup!**"
281,"A developer needs to implement a custom machine learning (ML) library in an
application. The size of the library is 15 GB. The size of the library is
increasing. The application uses AWS Lambda functions. All the Lambda functions
must have access to the library.

Which solution will meet these requirements?",Save the library in Lambda layers. Attach the layers to all Lambda functions.,Save the library in Amazon S3. Download the library from Amazon S3 inside the Lambda function.,Save the library as a Lambda container image. Redeploy the Lambda functions with the new image.,Save the library in an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all the Lambda functions.,"# Answer
- **Correct option:** D : Save the library in an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all the Lambda functions.

- **Reason:** AWS Lambda has a size limit for deployment packages and Lambda layers, which is currently 10 MB for compressed files and 250 MB for uncompressed files. Given that the custom ML library is 15 GB, it exceeds these limits if directly saved as a layer or deployment package. Using Amazon EFS allows you to store large files, and it can be mounted directly to Lambda functions, providing them with access to the library without hitting the size constraints.

# Example / Analogy
- Think of Amazon EFS as a shared drive in a workplace. Just like multiple employees can access large files from a shared drive without needing to duplicate them on their individual computers (which may have limited storage), Lambda functions can access a large ML library stored on EFS without the storage limitations of their individual deployment packages.

# Common Mistakes / Traps
- A common mistake is to assume that Lambda layers can handle large libraries. Many forget that there are size limits on layers. Additionally, the option to use S3 may seem appealing for larger files, but it requires downloading the library at runtime, which may introduce latency and complexity.

# Memory Tip
- ""For large libraries, think EFS for shared access, not layers or packages!"""
