No,Content,A,B,C,D,Answered
102,"[""An e-commerce web application that shares session state on-premises is being\nmigrated to AWS. The application must be fault tolerant, natively highly\nscalable, and any service interruption should not affect the user experience.\n\nWhat is the best option to store the session state?""]",Store the session state in Amazon ElastiCache.,Store the session state in Amazon CloudFront.,Store the session state in Amazon S3.,Enable session stickiness using elastic load balancers.,"# Answer
- **Correct option:** A: Store the session state in Amazon ElastiCache.
- **Reason:** Amazon ElastiCache is designed for in-memory data storage and can provide fast access to session data, which is crucial for maintaining a smooth user experience. It supports both Redis and Memcached, enabling fault tolerance and high scalability. This approach ensures that even if one instance goes down, another can pick up the session state without any service interruption, which aligns with the application's requirements.

# Example / Analogy
- Imagine a bus station where multiple buses (application servers) need access to the same schedule (session state). If the schedule is kept in a central location (like ElastiCache), any bus can access or update it quickly, keeping all buses synchronized and running on time, even if one runs late or has to be replaced.

# Common Mistakes / Traps
- A common trap is to confuse session stickiness (option D) with session state storage. While stickiness helps route a user to the same server, it doesn't store state information. Additionally, options B (CloudFront) and C (S3) are not suitable for session state storage as they are more suited for caching and file storage respectively.

# Memory Tip
- Think of ElastiCache as the ""fast memory"" for your app, keeping your session data quick to access and fail-proof."
103,"[""A developer is leveraging a Border Gateway Protocol (BGP)-based AWS VPN\nconnection to connect from on-premises to Amazon EC2 instances in the\ndeveloper's account. The developer is able to access an EC2 instance in subnet\nA, but is unable to access an EC2 instance in subnet B in the same VPC.\n\nWhich logs can the developer use to verify whether the traffic is reaching\nsubnet B?""]",VPN logs,BGP logs,VPC Flow Logs,AWS CloudTrail logs,"# Answer
- **Correct option:** C : VPC Flow Logs
- **Reason:** VPC Flow Logs allow the developer to capture information about the IP traffic going to and from the network interfaces in the VPC. By analyzing these logs, the developer can determine if traffic is reaching subnet B from the on-premises network, helping diagnose any potential issues with connectivity.

# Example / Analogy
- Think of VPC Flow Logs like a traffic camera at a busy intersection. Just as the camera records all the cars passing through, VPC Flow Logs track all the data packets moving in and out of your VPC subnets, helping you understand where traffic is flowing and where blockages may occur.

# Common Mistakes / Traps
- A common mistake is to assume that VPN logs or BGP logs would directly indicate the status of traffic reaching specific subnets, but they primarily focus on the status and health of the VPN connection itself rather than on the traffic flow within the VPC.

# Memory Tip
- **Remember:** VPC Flow Logs = Traffic Reports for your VPC zones."
104,"[""A developer has created an AWS Lambda function to provide notification through\nAmazon Simple Notification Service (Amazon SNS) whenever a file is uploaded to\nAmazon S3 that is larger than 50 MB. The developer has deployed and tested the\nLambda function by using the CLI. However, when the event notification is added\nto the S3 bucket and a 3,000 MB file is uploaded, the Lambda function does not\nlaunch.\n\nWhich of the following is a possible reason for the Lambda function's inability\nto launch?""]","The S3 event notification does not activate for files that are larger than 1,000 MB.",The resource-based policy for the Lambda function does not have the required permissions to be invoked by Amazon S3.,Lambda functions cannot be invoked directly from an S3 event.,The S3 bucket needs to be made public.,"# Answer
- **Correct option:** B
- **Reason:** The resource-based policy for the Lambda function does not have the required permissions to be invoked by Amazon S3. When setting up event notifications in S3, the Lambda function must include permissions to allow S3 to invoke it, which is managed through a resource-based policy.

# Example / Analogy
- Think of it like having a special door (the Lambda function) that only opens if you have the right key (permissions). If the door doesn’t recognize the key presented by S3 to open it, the door stays shut. In this case, if the Lambda function's policy isn't set up to allow S3 to invoke it, even if the event occurs (the file upload), the door won't open.

# Common Mistakes / Traps
- A common mistake is to assume that simply uploading a file will automatically trigger the Lambda, without considering permissions. Developers might overlook the need to explicitly allow S3 to invoke the Lambda function.

# Memory Tip
- Always check function permissions when integrating AWS services—it's like making sure your keys fit the locks!"
105,"[""A company moved some of its secure files to a private Amazon S3 bucket that has\nno public access. The company wants to develop a serverless application that\ngives its employees the ability to log in and securely share the files with\nother users.\n\nWhich AWS feature should the company use to share and access the files securely?""]",Amazon Cognito user pool,S3 presigned URLs,S3 bucket policy,Amazon Cognito identity pool,"# Answer
- **Correct option:** B: S3 presigned URLs
- **Reason:** S3 presigned URLs allow users to generate temporary URLs that grant time-limited access to private S3 objects. This feature is ideal for securely sharing files stored in a private S3 bucket, as it does not require changing the bucket's permissions or making it public.

# Example / Analogy
- Think of S3 presigned URLs like giving a friend a temporary key to your house. You can specify how long they can use that key before it stops working, allowing them to access a specific room (the file) without granting them permanent access to your entire home (the bucket).

# Common Mistakes / Traps
- A common mistake might be choosing options like Amazon Cognito user pool or identity pool, which are more focused on user authentication and identity management rather than specifically sharing secure files. Remember that user management and file access are different tasks.

# Memory Tip
- Remember: ""Presigned URLs = Temporary keys for secure file access!"""
106,"[""A company needs to develop a proof of concept for a web service application. The\napplication will show the weather forecast for one of the company's office\nlocations. The application will provide a REST endpoint that clients can call.\nWhere possible, the application should use caching features provided by AWS to\nlimit the number of requests to the backend service. The application backend\nwill receive a small amount of traffic only during testing.\n\nWhich approach should the developer take to provide the REST endpoint MOST\ncost-effectively?""]",Create a container image. Deploy the container image by using Amazon Elastic Kubernetes Service (Amazon EKS). Expose the functionality by using Amazon API Gateway.,Create an AWS Lambda function by using the AWS Serverless Application Model (AWS SAM). Expose the Lambda functionality by using Amazon API Gateway.,Create a container image. Deploy the container image by using Amazon Elastic Container Service (Amazon ECS). Expose the functionality by using Amazon API Gateway.,Create a microservices application. Deploy the application to AWS Elastic Beanstalk. Expose the AWS Lambda functionality by using an Application Load Balancer.,"# Answer
- **Correct option:** B
- **Reason:** Creating an AWS Lambda function with the AWS Serverless Application Model (AWS SAM) and exposing it via Amazon API Gateway is the most cost-effective approach for a low-traffic proof of concept. It leverages serverless architecture, which means you only pay for actual usage without provisioning dedicated servers. Lambda automatically scales to handle incoming requests while caching capabilities in API Gateway can further reduce backend requests efficiently.

# Example / Analogy
- Think of AWS Lambda like a food truck that only serves meals when customers request them. When the truck is closed, you don't incur any costs, and when it's open, you only pay for the meals made. API Gateway is like the ordering system that handles customer requests and manages the flow to the truck efficiently, ensuring minimal wait times and optimal service.

# Common Mistakes / Traps
- Many might assume that deploying containers is a better choice for a proof of concept due to familiarity or perceived performance benefits. However, deploying containers incurs higher operational overhead and costs in this context, especially for small, intermittent traffic, making serverless options more appealing.

# Memory Tip
- Remember: ""For low traffic and cost savings, go serverless with Lambda!"""
107,"[""A company has an AWS Lambda function that processes incoming requests from an\nAmazon API Gateway API. The API calls the Lambda function by using a Lambda\nalias. A developer updated the Lambda function code to handle more details\nrelated to the incoming requests. The developer wants to deploy the new Lambda\nfunction for more testing by other developers with no impact to customers that\nuse the API.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Create a new version of the Lambda function. Create a new stage on API Gateway with integration to the new Lambda version. Use the new API Gateway stage to test the Lambda function.,Update the existing Lambda alias used by API Gateway to a weighted alias. Add the new Lambda version as an additional Lambda function with a weight of 10%. Use the existing API Gateway stage for testing.,"Create a new version of the Lambda function. Create and deploy a second Lambda function to filter incoming requests from API Gateway. If the filtering Lambda function detects a test request, the filtering Lambda function will invoke the new Lambda version of the code. For other requests, the filtering Lambda function will invoke the old Lambda version. Update the API Gateway API to use the filtering Lambda function.",Create a new version of the Lambda function. Create a new API Gateway API for testing purposes. Update the integration of the new API with the new Lambda version. Use the new API for testing.,"# Answer
- **Correct option:** A

- **Reason:** Option A entails creating a new version of the Lambda function and deploying it through a new stage in API Gateway. This approach allows for isolated testing of the new function without affecting the existing production setup since the existing API Gateway stage remains unchanged. It provides a clear segregation between production and testing environments with minimal operational overhead. The use of stages in API Gateway makes it straightforward to manage different versions of the API, which is essential for ensuring no impact on current customers.

# Example / Analogy
- Imagine a restaurant that wants to test a new dish but doesn't want to affect customers who are enjoying the existing menu. Instead of modifying the current menu, the restaurant creates a special ""test menu"" that only selected customers can order from. This way, the restaurant can gather feedback without disrupting the regular dining experience.

# Common Mistakes / Traps
- A common mistake is to think that updating the existing Lambda alias or using a weighted alias (as in option B) is the best strategy. While these methods can create a transition between versions, they might inadvertently let some customers access the new Lambda code prematurely, which is not desirable in a testing scenario.

# Memory Tip
- Remember, isolating environments for testing ensures a stable production experience—like having a test menu for new dishes!"
108,"[""A developer has written an application that runs on Amazon EC2 instances. The\ndeveloper is adding functionality for the application to write objects to an\nAmazon S3 bucket.\n\nWhich policy must the developer modify to allow the instances to write these\nobjects?""]",The IAM policy that is attached to the EC2 instance profile role,The session policy that is applied to the EC2 instance role session,The AWS Key Management Service (AWS KMS) key policy that is attached to the EC2 instance profile role,The Amazon VPC endpoint policy,"# Answer
- **Correct option:** A: The IAM policy that is attached to the EC2 instance profile role
- **Reason:** In AWS, when an EC2 instance needs to perform actions like writing to an S3 bucket, it relies on an IAM role that is associated with the instance. This role must have the appropriate permissions. The IAM policy attached to the EC2 instance profile role is what defines the permissions for actions taken by the instance, such as writing objects to S3.

# Example / Analogy
- Consider the EC2 instance as a person who needs permission to enter a building (S3 bucket) and leave items (write objects). The IAM policy is like an ID badge that grants access to certain areas and tasks. If the ID badge does not allow entry or defines what tasks can be performed inside, the person (EC2 instance) will be restricted from completing the desired activities (writing to S3).

# Common Mistakes / Traps
- A common mistake is to assume that policies related to KMS key management, VPC endpoints, or session benefits apply to granting permissions to S3 resources, when in fact they manage different aspects of the AWS environment and do not directly impact permissions for S3 object operations.

# Memory Tip
- **Remember:** ""IAM Role = Access Badge; Ensure it grants the right permissions!"""
109,"[""A company uses AWS Lambda functions and an Amazon S3 trigger to process images\ninto an S3 bucket. A development team set up multiple environments in a single\nAWS account.\n\nAfter a recent production deployment, the development team observed that the\ndevelopment S3 buckets invoked the production environment Lambda functions.\nThese invocations caused unwanted execution of development S3 files by using\nproduction Lambda functions. The development team must prevent these\ninvocations. The team must follow security best practices.\n\nWhich solution will meet these requirements?""]",Update the Lambda execution role for the production Lambda function to add a policy that allows the execution role to read from only the production environment S3 bucket.,Move the development and production environments into separate AWS accounts. Add a resource policy to each Lambda function to allow only S3 buckets that are within the same account to invoke the function.,Add a resource policy to the production Lambda function to allow only the production environment S3 bucket to invoke the function.,Move the development and production environments into separate AWS accounts. Update the Lambda execution role for each function to add a policy that allows the execution role to read from the S3 bucket that is within the same account.,"# Answer
- **Correct option:** C
- **Reason:** Adding a resource policy to the production Lambda function limits its invocation to only the production environment S3 bucket. This effectively prevents development S3 buckets from triggering production Lambda functions, maintaining a clear separation between environments and adhering to security best practices.

# Example / Analogy
- Think of a restaurant with separate dining areas for different groups (like families and business meetings). If you want to ensure that families don’t accidentally enter the business area, you can install a door that only allows certain groups to enter specific spaces. Similarly, a resource policy on the Lambda function acts as that restrictive door, ensuring that only the right S3 bucket can invoke the associated Lambda function.

# Common Mistakes / Traps
- A common mistake is to think that changing the Lambda execution role alone (as in option A) will solve the invocation issue. In reality, the trigger permission settings (resource policies) on the Lambda function itself are what control which S3 buckets can invoke it.

# Memory Tip
- ""Use resource policies to lock down Lambda invocations to the correct S3 buckets."""
110,"[""A developer wants to use AWS Elastic Beanstalk to test a new version of an\napplication in a test environment.\n\nWhich deployment method offers the FASTEST deployment?""]",Immutable,Rolling,Rolling with additional batch,All at once,"# Answer
- **Correct option:** D : All at once
- **Reason:** The ""All at once"" deployment method in AWS Elastic Beanstalk updates all instances of the application at the same time, providing the fastest deployment option. While this method can lead to downtime, it is the quickest way to roll out a new version when speed is the top priority.

# Example / Analogy
- Think of it like flipping a switch to turn off all the lights in a room at once versus turning them off one by one. If your goal is to darken the room as quickly as possible, flipping the switch gives immediate results.

# Common Mistakes / Traps
- A common mistake is to confuse ""All at once"" with methods that provide gradual updates, such as ""Rolling"" or ""Immutable,"" which are safer but not as fast. Remember that, while faster, ""All at once"" can also increase the risk of making the application unavailable.

# Memory Tip
- For the fastest deployment, remember: ""All at once goes fastest!"""
111,"[""A developer is creating an AWS Lambda function in VPC mode. An Amazon S3 event\nwill invoke the Lambda function when an object is uploaded into an S3 bucket.\nThe Lambda function will process the object and produce some analytic results\nthat will be recorded into a file. Each processed object will also generate a\nlog entry that will be recorded into a file.\n\nOther Lambda functions, AWS services, and on-premises resources must have access\nto the result files and log file. Each log entry must also be appended to the\nsame shared log file. The developer needs a solution that can share files and\nappend results into an existing file.\n\nWhich solution should the developer use to meet these requirements?""]",Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in Lambda. Store the result files and log file in the mount point. Append the log entries to the log file.,"Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach enabled volume. Attach the EBS volume to all Lambda functions. Update the Lambda function code to download the log file, append the log entries, and upload the modified log file to Amazon EBS.",Create a reference to the /tmp local directory. Store the result files and log file by using the directory reference. Append the log entry to the log file.,Create a reference to the /opt storage directory. Store the result files and log file by using the directory reference. Append the log entry to the log file.,"# Answer
- **Correct option:** A
- **Reason:** Creating an Amazon Elastic File System (EFS) file system allows multiple AWS Lambda functions, as well as other AWS services and on-premises resources, to easily share files. EFS is a fully-managed NFS (Network File System) that can be mounted in AWS Lambda. This enables the Lambda function to store result files and log files in EFS, and append new log entries to the existing log file. The ability to share the same file system across different resources makes it ideal for this use case.

# Example / Analogy
- Think of Amazon EFS like a shared drive in a corporate office where multiple team members (Lambda functions, services, etc.) can access and edit documents (result and log files) concurrently, adding notes and updates as needed. Just as everyone can see and modify the same file in a shared drive, various AWS resources can seamlessly read from and write to EFS.

# Common Mistakes / Traps
- A common mistake is to choose Amazon EBS, thinking it can be attached to multiple Lambda functions. However, EBS volumes cannot be shared across multiple instances simultaneously unless they use a specific arrangement like Multi-Attach with additional limitations, making it unsuitable for this scenario. Also, referencing local directories like `/tmp` only provides temporary storage (limited to 512 MB) per invocation, and nothing persists beyond the execution lifetime.

# Memory Tip
- **EFS = Easy File Sharing**: Remember that Amazon EFS is designed for shared storage across multiple resources."
112,"[""A company is using an Amazon API Gateway REST API endpoint as a webhook to\npublish events from an on-premises source control management (SCM) system to\nAmazon EventBridge. The company has configured an EventBridge rule to listen for\nthe events and to control application deployment in a central AWS account. The\ncompany needs to receive the same events across multiple receiver AWS accounts.\n\nHow can a developer meet these requirements without changing the configuration\nof the SCM system?""]",Deploy the API Gateway REST API to all the required AWS accounts. Use the same custom domain name for all the gateway endpoints so that a single SCM webhook can be used for all events from all accounts.,Deploy the API Gateway REST API to all the receiver AWS accounts. Create as many SCM webhooks as the number of AWS accounts.,Grant permission to the central AWS account for EventBridge to access the receiver AWS accounts. Add an EventBridge event bus on the receiver AWS accounts as the targets to the existing EventBridge rule.,Convert the API Gateway type from REST API to HTTP API.,"# Answer
- **Correct option:** C
- **Reason:** Granting permission to the central AWS account for EventBridge to access the receiver AWS accounts allows you to route the events from the central EventBridge rule directly to the event buses in the receiver accounts. This method enables all designated accounts to receive the same events without needing to change the existing SCM webhook configuration or requiring separate webhooks for each AWS account.

# Example / Analogy
- Think of it like a central post office (the central AWS account) that can send packages (events) to various homes (receiver AWS accounts) under a shared agreement. The post office has permission to deliver to these homes, so you only need to send one package instead of multiple copies to each house.

# Common Mistakes / Traps
- A common mistake is to consider options that suggest deploying multiple versions of resources (like deploying distinct API Gateway instances or creating multiple webhooks), which would complicate the architecture unnecessarily and increase maintenance overhead.

# Memory Tip
- ""One central post office delivers to many homes."""
113,"[""A developer is working on a web application that uses Amazon DynamoDB as its\ndata store. The application has two DynamoDB tables: one table that is named\nartists and one table that is named songs. The artists table has artistName as\nthe partition key. The songs table has songName as the partition key and\nartistName as the sort key.\n\nThe table usage patterns include the retrieval of multiple songs and artists in\na single database operation from the webpage. The developer needs a way to\nretrieve this information with minimal network traffic and optimal application\nperformance.\n\nWhich solution will meet these requirements?""]",Perform a BatchGetltem operation that returns items from the two tables. Use the list of songName/artistName keys for the songs table and the list of artistName key for the artists table.,Create a local secondary index (LSI) on the songs table that uses artistName as the partition key. Perform a query operation for each artistName on the songs table that filters by the list of songName. Perform a query operation for each artistName on the artists table.,Perform a BatchGetitem operation on the songs table that uses the songName/artistName keys. Perform a BatchGetltem operation on the artists table that uses artistName as the key.,Perform a Scan operation on each table that filters by the list of songName/artistName for the songs table and the list of artistName in the artists table.,"# Answer
- **Correct option:** A
- **Reason:** A BatchGetItem operation allows you to retrieve multiple items from different tables in a single request, reducing network traffic and improving performance by minimizing the number of round trips to the database. You can specify the keys for both tables, thus efficiently obtaining the required information.

# Example / Analogy
- Imagine you are ordering groceries online. Instead of making separate trips to each aisle in a store (which is akin to performing multiple queries or scans), you provide a list of items you need at once (like the BatchGetItem operation) and the store gathers everything for you in one go. This saves time and effort, just like reducing network calls in a web application.

# Common Mistakes / Traps
- A common trap is choosing options that suggest performing separate queries or scans on each table. While these methods will eventually yield results, they are less efficient compared to BatchGetItem, especially in applications that require optimal performance and minimal latency.

# Memory Tip
- **BatchGetItem: Get many, fetch once!**"
114,"[""A company is developing an ecommerce application that uses Amazon API Gateway\nAPIs. The application uses AWS Lambda as a backend. The company needs to test\nthe code in a dedicated, monitored test environment before the company releases\nthe code to the production environment.\n\nWhich solution will meet these requirements?""]",Use a single stage in API Gateway. Create a Lambda function for each environment. Configure API clients to send a query parameter that indicates the environment and the specific Lambda function.,Use multiple stages in API Gateway. Create a single Lambda function for all environments. Add different code blocks for different environments in the Lambda function based on Lambda environment variables.,Use multiple stages in API Gateway. Create a Lambda function for each environment. Configure API Gateway stage variables to route traffic to the Lambda function in different environments.,Use a single stage in API Gateway. Configure API clients to send a query parameter that indicates the environment. Add different code blocks for different environments in the Lambda function to match the value of the query parameter.,"# Answer
- **Correct option:** C
- **Reason:** This option uses multiple stages in API Gateway to differentiate between environments. By creating a separate Lambda function for each environment and deploying them using stage variables in API Gateway, you ensure that each environment can be monitored and tested independently. This method also aligns with AWS best practices, allowing for separate configurations while minimizing the risk of accidentally mixing production and test environments.

# Example / Analogy
- Think of stages in API Gateway like different lanes in a racing track. Each lane can have its own car (Lambda function) that competes (runs) independently based on the rules (stage variables). Just like each car can be tuned specifically for its track, each environment can be configured without affecting the others.

# Common Mistakes / Traps
- A common mistake is assuming that a single Lambda function can efficiently handle different environments by just using query parameters or environment variables. This approach can lead to complex and error-prone code, making it harder to manage deploys and tests effectively.

# Memory Tip
- ""Separate lanes avoid collisions"": Use different stages and functions for clear separation of environments."
115,"[""A developer is creating an AWS Lambda function. The Lambda function will consume\nmessages from an Amazon Simple Queue Service (Amazon SQS) queue. The developer\nwants to integrate unit testing as part of the function's continuous integration\nand continuous delivery (CI/CD) process.\n\nHow can the developer unit test the function?""]",Create an AWS CloudFormation template that creates an SQS queue and deploys the Lambda function. Create a stack from the template during the CI/CD process. Invoke the deployed function. Verify the output.,Create an SQS event for tests. Use a test that consumes messages from the SQS queue during the function's Cl/CD process.,Create an SQS queue for tests. Use this SQS queue in the application's unit test. Run the unit tests during the CI/CD process.,Use the aws lambda invoke command with a test event during the CIICD process.,"# Answer
- **Correct option:** D
- **Reason:** Using the `aws lambda invoke` command with a test event allows the developer to run unit tests on the Lambda function directly in the CI/CD process without needing to set up external resources like an SQS queue. This approach facilitates focused testing of the function's logic using predefined test events that simulate various scenarios.

# Example / Analogy
- Think of it like using a simulation tool to test a car's performance instead of actually driving it on the road. The test events simulate real inputs, enabling you to validate the car's features (the Lambda function’s logic) in a safe and controlled environment.

# Common Mistakes / Traps
- A common mistake is to assume that integration with external services like SQS is necessary for unit testing. While integration tests may require the SQS queue, unit testing can and should focus on validating the function's logic in isolation.

# Memory Tip
- ""Test your Lambda functions in isolation using the `aws lambda invoke` command."""
116,"[""A company is developing a serverless multi-tier application on AWS. The company\nwill build the serverless logic tier by using Amazon API Gateway and AWS Lambda.\nWhile the company builds the logic tier, a developer who works on the frontend\nof the application must develop integration tests. The tests must cover both\npositive and negative scenarios, depending on success and error HTTP status\ncodes.\n\nWhich solution will meet these requirements with the LEAST effort?""]","Set up a mock integration for API methods in API Gateway. In the integration request from Method Execution, add simple logic to return either a success or error based on HTTP status code. In the integration response, add messages that correspond to the HTTP status codes.","Create two mock integration resources for API methods in API Gateway. In the integration request, return a success HTTP status code for one resource and an error HTTP status code for the other resource. In the integration response, add messages that correspond to the HTTP status codes.","Create Lambda functions to perform tests. Add simple logic to return either success or error, based on the HTTP status codes. Build an API Gateway Lambda integration. Select appropriate Lambda functions that correspond to the HTTP status codes.",Create a Lambda function to perform tests. Add simple logic to return either success or error-based HTTP status codes. Create a mock integration in API Gateway. Select the Lambda function that corresponds to the HTTP status codes.,"# Answer
- **Correct option:** A  
- **Reason:** Option A allows for the simplest and quickest setup by using a mock integration within API Gateway to simulate both success and error HTTP status responses. It provides the developer with greater flexibility and control over the response details without needing to deploy additional resources or implement Lambda functions, making it the least effort solution.

# Example / Analogy
- Think of a restaurant menu that displays various food items along with their prices. Before actually cooking the meals, the restaurant can simulate the experience by providing sample descriptions and prices without having to serve real food. Similarly, using API Gateway mock integrations allows you to simulate responses without needing a fully deployed backend.

# Common Mistakes / Traps
- A common mistake could be overcomplicating the setup by choosing options that involve creating multiple resources (like in option B) or using Lambda functions (options C and D) when a simpler mock can suffice. Developers may think they need a full backend logic for testing when these mock scenarios are adequate.

# Memory Tip
- ""Mock it to test it: Keep it simple with mock integrations in API Gateway."""
117,"[""A developer is setting up a deployment pipeline. The pipeline includes an AWS\nCodeBuild build stage that requires access to a database to run integration\ntests. The developer is using a buildspec.yml file to configure the database\nconnection. Company policy requires automatic rotation of all database\ncredentials.\n\nWhich solution will handle the database credentials MOST securely?""]",Retrieve the credentials from variables that are hardcoded in the buildspec.yml file. Configure an AWS Lambda function to rotate the credentials.,Retrieve the credentials from an environment variable that is linked to a SecureString parameter in AWS Systems Manager Parameter Store. Configure Parameter Store for automatic rotation.,Retrieve the credentials from an environment variable that is linked to an AWS Secrets Manager secret. Configure Secrets Manager for automatic rotation.,Retrieve the credentials from an environment variable that contains the connection string in plaintext. Configure an Amazon EventBridge event to rotate the credentials.,"# Answer
- **Correct option:** C
- **Reason:** AWS Secrets Manager is designed specifically for managing sensitive information like database credentials. It allows for automatic rotation of secrets, which aligns with the company's policy on credential management. Using a secure variable that links to a Secrets Manager secret ensures the credentials are handled securely.

# Example / Analogy
- Think of AWS Secrets Manager as a safe where you store your valuable items (credentials). You have a key (secure variable) that lets you access the safe without exposing its contents. Just like a locksmith who can automatically change the locks on a safe at regular intervals, Secrets Manager can automatically rotate your credentials, ensuring that your valuables are always protected.

# Common Mistakes / Traps
- Mistake: Choosing options that involve hardcoding or using plaintext credentials, which are not secure. Many might think that using environment variables without proper encryption is sufficient, but this poses a security risk.

# Memory Tip
- Remember: ""Secrets are best kept in Secrets Manager!"""
118,"[""A company created four AWS Lambda functions that connect to a relational\ndatabase server that runs on an Amazon RDS instance. A security team requires\nthe company to automatically change the database password every 30 days.\n\nWhich solution will meet these requirements MOST securely?""]",Store the database credentials in the environment variables of the Lambda function. Deploy the Lambda function with the new credentials every 30 days.,Store the database credentials in AWS Secrets Manager. Configure a 30-day rotation schedule for the credentials.,Store the database credentials in AWS Systems Manager Parameter Store secure strings. Configure a 30-day schedule for the secure strings.,Store the database credentials in an Amazon S3 bucket that uses server-side encryption with customer-provided encryption keys (SSE-C). Configure a 30-day key rotation schedule for the customer key.,"# Answer
- **Correct option:** B
- **Reason:** Storing the database credentials in AWS Secrets Manager allows for automated credential rotation, which meets the requirement to change the password every 30 days securely. Secrets Manager handles all aspects of credential management, including encryption, versioning, and automatic rotation without requiring redeployment of the Lambda functions.

# Example / Analogy
- Think of AWS Secrets Manager like a personal safe that automatically changes the combination every month. You trust it to keep your cash (credentials) secure, and you don’t have to remember the new combination every time. Instead, you just turn the dial and access your cash without worrying about theft or needing to change things manually.

# Common Mistakes / Traps
- A common mistake might be to choose option A, which suggests storing credentials in environment variables, thinking it's convenient. However, this approach lacks automated rotation and requires redeployment, which adds risk and complexity. Option C, while valid, doesn't have the same level of built-in rotation and management features as Secrets Manager. Option D is also insecure and impractical since S3 is not designed to manage sensitive credentials effectively.

# Memory Tip
- Remember: **AWS Secrets Manager is your go-to for secure, automated credential management.**"
119,"[""A company is developing a serverless application that consists of various AWS\nLambda functions behind Amazon API Gateway APIs. A developer needs to automate\nthe deployment of Lambda function code. The developer will deploy updated Lambda\nfunctions with AWS CodeDeploy. The deployment must minimize the exposure of\npotential errors to end users. When the application is in production, the\napplication cannot experience downtime outside the specified maintenance window.\n\nWhich deployment configuration will meet these requirements with the LEAST\ndeployment time?""]",Use the AWS CodeDeploy in-place deployment configuration for the Lambda functions. Shift all traffic immediately after deployment.,Use the AWS CodeDeploy linear deployment configuration to shift 10% of the traffic every minute.,Use the AWS CodeDeploy all-at-once deployment configuration to shift all traffic to the updated versions immediately.,Use the AWS CodeDeploy predefined canary deployment configuration to shift 10% of the traffic immediately and shift the remaining traffic after 5 minutes.,"# Answer
- **Correct option:** D : Use the AWS CodeDeploy predefined canary deployment configuration to shift 10% of the traffic immediately and shift the remaining traffic after 5 minutes.

- **Reason:** This deployment configuration allows for an initial 10% of the traffic to be routed to the new version of the Lambda function, enabling the team to monitor the performance and detect any potential issues with the updated version. If any problems arise, they can be addressed without affecting the majority of users. After a brief 5-minute waiting period, if everything looks good, the remaining traffic can be shifted to the new version. This minimizes user exposure to potential errors and avoids downtime, aligning with the requirements stated in the question.

# Example / Analogy
- Think of it like a restaurant trying a new dish. Instead of serving the new dish to all customers at once, they first offer it to a small group (10% of the traffic). If that group enjoys the dish without complaints, the restaurant can confidently serve it to everyone else after checking for any issues over a short period of time. This way, they ensure customer satisfaction while minimizing risk.

# Common Mistakes / Traps
- A common trap is choosing the ""all-at-once"" configuration (Option C) because it allows for the fastest deployment. However, it carries a significantly higher risk of exposing all users to potential problems if the new version has issues. Similarly, the in-place deployment (Option A) also poses risks as it shifts all traffic immediately, which does not align with the requirement to minimize user impact during production.

# Memory Tip
- **Canary deployment means testing with 10% first** to reduce risks while ensuring a smooth roll-out."
120,"[""A developer needs to store configuration variables for an application. The\ndeveloper needs to set an expiration date and time for the configuration. The\ndeveloper wants to receive notifications before the configuration expires.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Create a standard parameter in AWS Systems Manager Parameter Store. Set Expiration and ExpirationNotification policy types.,Create a standard parameter in AWS Systems Manager Parameter Store. Create an AWS Lambda function to expire the configuration and to send Amazon Simple Notification Service (Amazon SNS) notifications.,Create an advanced parameter in AWS Systems Manager Parameter Store. Set Expiration and ExpirationNotification policy types.,Create an advanced parameter in AWS Systems Manager Parameter Store. Create an Amazon EC2 instance with a cron job to expire the configuration and to send notifications.,"# Answer
- **Correct option:** C : Create an advanced parameter in AWS Systems Manager Parameter Store. Set Expiration and ExpirationNotification policy types.

- **Reason:** The advanced parameters in AWS Systems Manager Parameter Store provide built-in support for setting expiration dates and sending notifications before they expire without requiring additional resources or custom code. This solution minimizes operational overhead by utilizing the features available directly within the service.

# Example / Analogy
- Think of it like a time-sensitive coupon that has an automatic reminder feature. Just as the coupon provider informs you before the expiration date approaches, the advanced parameter can automatically notify you, ensuring you don't miss important updates in your application configuration.

# Common Mistakes / Traps
- A common mistake is to underestimate the capabilities of advanced parameters and assume that additional manual setups (like Lambda or EC2 instances) are necessary for expiration and notification, leading to more complex solutions than needed for the task.

# Memory Tip
- Remember: “Advanced equals automated!”"
121,"[""A developer creates an AWS Lambda function that retrieves and groups data from\nseveral public API endpoints. The Lambda function has been updated and\nconfigured to connect to the private subnet of a VPC. An internet gateway is\nattached to the VPC. The VPC uses the default network ACL and security group\nconfigurations.\n\nThe developer finds that the Lambda function can no longer access the public\nAPI. The developer has ensured that the public API is accessible, but the Lambda\nfunction cannot connect to the API\n\nHow should the developer fix the connection issue?""]",Ensure that the network ACL allows outbound traffic to the public internet.,Ensure that the security group allows outbound traffic to the public internet.,Ensure that outbound traffic from the private subnet is routed to a public NAT gateway.,Ensure that outbound traffic from the private subnet is routed to a new internet gateway.,"# Answer
- **Correct option:** C
- **Reason:** A Lambda function in a private subnet cannot directly access the internet. To connect to external public APIs, it needs to route outbound traffic through a NAT Gateway. A NAT Gateway provides the necessary network address translation, allowing instances in a private subnet to initiate outbound traffic to the internet while still preventing unsolicited inbound traffic.

# Example / Analogy
- Think of the private subnet as a hotel (private space) where guests (the Lambda function) can make phone calls (send requests) to the outside world (public APIs). However, the hotel has no direct phone line to the outside. Instead, there is a receptionist (NAT Gateway) who can place calls on behalf of guests, allowing them to communicate without directly being exposed to the outside.

# Common Mistakes / Traps
- A common mistake is to assume that having an internet gateway is sufficient for outbound access from a private subnet. An execution in a VPC will not have internet access unless routed through a NAT Gateway. Security groups and NACLs need to be configured appropriately, but they don’t replace the need for a NAT setup.

# Memory Tip
- ""Private subnets need a NAT to talk to the outside (APIs)!"""
