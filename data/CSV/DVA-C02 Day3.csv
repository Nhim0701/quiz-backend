No,Content,A,B,C,D,Answered
42,"[""An Amazon Kinesis Data Firehose delivery stream is receiving customer data that\ncontains personally identifiable information. A developer needs to remove\npattern-based customer identifiers from the data and store the modified data in\nan Amazon S3 bucket.\nWhat should the developer do to meet these requirements?""]",Implement Kinesis Data Firehose data transformation as an AWS Lambda function. Configure the function to remove the customer identifiers. Set an Amazon S3 bucket as the destination of the delivery stream.,Launch an Amazon EC2 instance. Set the EC2 instance as the destination of the delivery stream. Run an application on the EC2 instance to remove the customer identifiers. Store the transformed data in an Amazon S3 bucket.,Create an Amazon OpenSearch Service instance. Set the OpenSearch Service instance as the destination of the delivery stream. Use search and replace to remove the customer identifiers. Export the data to an Amazon S3 bucket.,"Create an AWS Step Functions workflow to remove the customer identifiers. As the last step in the workflow, store the transformed data in an Amazon S3 bucket. Set the workflow as the destination of the delivery stream.","# Answer
- **Correct option:** A 
- **Reason:** Implementing Kinesis Data Firehose data transformation as an AWS Lambda function allows for the on-the-fly modification of incoming data streams. The Lambda function can be configured specifically to remove customer identifiers directly before the data is sent to Amazon S3, thus effectively meeting the requirement to remove personally identifiable information.

# Example / Analogy
- Think of Kinesis Data Firehose like a conveyor belt carrying boxes of goods (the data). In this scenario, the Lambda function acts like a quality control station along the conveyor belt that inspects each box and removes any sensitive items before they reach the storage area (Amazon S3). This ensures only safe, modified boxes are stored.

# Common Mistakes / Traps
- A common pitfall is choosing solutions that involve additional, unnecessary services (like EC2 or Step Functions) that complicate the architecture. The goal is to efficiently handle stream data processing without introducing more components than needed.

# Memory Tip
- **Lambda is your data filter: remove sensitive info on-the-go!**"
43,"[""An application uses Lambda functions to extract metadata from files uploaded to\nan S3 bucket; the metadata is stored in Amazon DynamoDB. The application starts\nbehaving unexpectedly, and the developer wants to examine the logs of the Lambda\nfunction code for errors.\nBased on this system configuration, where would the developer find the logs?""]",Amazon S3,AWS CloudTrail,Amazon CloudWatch,Amazon DynamoDB,"# Answer
- **Correct option:** C : Amazon CloudWatch
- **Reason:** AWS Lambda automatically integrates with Amazon CloudWatch to log output and error messages generated by your Lambda functions. When a Lambda function fails or logs a statement, this information is sent to CloudWatch Logs, where developers can access it for debugging and monitoring.

# Example / Analogy
- Consider CloudWatch as a security camera for your Lambda function—it records everything that happens during execution (like logs) so you can review and diagnose issues later.

# Common Mistakes / Traps
- A common mistake is thinking that logs are stored in S3 or that CloudTrail (which records API activity) would provide the specific logs of the Lambda function’s execution. CloudTrail captures management events but not the runtime execution logs of Lambda.

# Memory Tip
- Think ""CloudWatch for checking what went wrong in my Lambda."""
44,"[""A company is using an AWS Lambda function to process records from an Amazon\nKinesis data stream. The company recently observed slow processing of the\nrecords. A developer notices that the iterator age metric for the function is\nincreasing and that the Lambda run duration is constantly above normal.\nWhich actions should the developer take to increase the processing speed?\n(Choose two.)""]",Increase the number of shards of the Kinesis data stream.,Decrease the timeout of the Lambda function.,Increase the memory that is allocated to the Lambda function.,Decrease the number of shards of the Kinesis data stream.,"# Answer
- **Correct options:** A, C  

- **Reason:**  
  Option A (Increase the number of shards of the Kinesis data stream) effectively allows for more parallel processing of records, enabling multiple Lambda instances to consume from the stream at the same time. The iterator age metric indicates how old the records in the stream are compared to the current processing point; if it's increasing, it suggests that Lambda is not keeping up with the incoming data. By increasing the number of shards, you can distribute the load more efficiently.  
  Option C (Increase the memory that is allocated to the Lambda function) not only increases the available memory but can also lead to a higher CPU allocation, which can reduce processing time for each function invocation. When you boost memory for Lambda, it can handle more complex processing or run faster, reducing the overall duration of function executions.

# Example / Analogy
- Think of a highway with multiple lanes (shards) where cars (data records) travel. If the highway has too few lanes, traffic slows down, leading to cars piling up (increased iterator age). Adding more lanes (shards) allows more cars to travel at once, improving overall traffic speed. Similarly, if one car (Lambda function) isn't powerful enough (limited memory), it can't accelerate effectively; giving it a more powerful engine (more memory) helps it go faster.

# Common Mistakes / Traps
- A common mistake is to assume that decreasing the timeout or the number of shards will help. Decreasing the timeout (Option B) only means the Lambda will stop processing sooner, which does not resolve the underlying problem of processing speed. Additionally, decreasing the number of shards (Option D) contradicts the goal of improving throughput.

# Memory Tip
- **More shards = more lanes; more memory = a more powerful engine.**"
45,"[""A developer is incorporating AWS X-Ray into an application that handles personal\nidentifiable information (PII). The application is hosted on Amazon EC2\ninstances. The application trace messages include encrypted PII and go to Amazon\nCloudWatch. The developer needs to ensure that no PII goes outside of the EC2\ninstances.\nWhich solution will meet these requirements?""]",Manually instrument the X-Ray SDK in the application code.,Use the X-Ray auto-instrumentation agent.,Use Amazon Macie to detect and hide PII. Call the X-Ray API from AWS Lambda.,Use AWS Distro for Open Telemetry.,"# Answer
- **Correct option:** A: Manually instrument the X-Ray SDK in the application code.

- **Reason:** Manually instrumenting the X-Ray SDK allows the developer to have complete control over what data is sent to AWS X-Ray, ensuring that encrypted PII can be excluded from trace data. By selectively including trace messages that do not contain any PII, the developer can ensure compliance with privacy requirements and prevent sensitive information from going outside the EC2 instances.

# Example / Analogy
- Think of it like manually deciding what to share on social media versus automatically sharing everything from your phone. If you manually select which photos to post (like manually instrumenting the X-Ray SDK), you ensure that no personal or sensitive images (PII) are shared publicly. Automatic sharing might include everything, potentially compromising your privacy.

# Common Mistakes / Traps
- A common trap in this type of question is assuming that auto-instrumentation (Option B) is safer. While it is quicker and easier to set up, it may capture all data, including sensitive PII, unless carefully configured. Similarly, using services like AWS Lambda to process data (Option C) does not inherently prevent PII from being sent outside EC2 instances, as the data received may still include sensitive information.

# Memory Tip
- Manually control your data flow to keep your PII personal and protected!"
46,"[""A company needs to harden its container images before the images are in a\nrunning state. The company's application uses Amazon Elastic Container Registry\n(Amazon ECR) as an image registry. Amazon Elastic Kubernetes Service (Amazon\nEKS) for compute, and an AWS CodePipeline pipeline that orchestrates a\ncontinuous integration and continuous delivery (CI/CD) workflow.\nDynamic application security testing occurs in the final stage of the pipeline\nafter a new image is deployed to a development namespace in the EKS cluster. A\ndeveloper needs to place an analysis stage before this deployment to analyze the\ncontainer image earlier in the CI/CD pipeline.\nWhich solution will meet these requirements with the MOST operational\nefficiency?""]",Build the container image and run the docker scan command locally. Mitigate any findings before pushing changes to the source code repository. Write a pre-commit hook that enforces the use of this workflow before commit.,Create a new CodePipeline stage that occurs after the container image is built. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.,Create a new CodePipeline stage that occurs after source code has been retrieved from its repository. Run a security scanner on the latest revision of the source code. Fail the pipeline if there are findings.,Add an action to the deployment stage of the pipeline so that the action occurs before the deployment to the EKS cluster. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.,"# Answer
- **Correct option:** B
- **Reason:** This option creates a dedicated stage in the CodePipeline that scans the container image immediately after it is built and before it is deployed. By utilizing ECR's built-in image scanning capability, the solution ensures that container vulnerabilities are identified early. The Lambda function can automate the process of checking the scan results and stopping the pipeline if any issues are detected, providing a robust security check without manual intervention.

# Example / Analogy
- Think of this as a safety inspection for a vehicle before it hits the road. Just like a thorough inspection can identify issues with the brakes or engines before the car is used, the image scanning step identifies security vulnerabilities before the container is deployed. This proactive approach prevents problematic containers from running in a production environment.

# Common Mistakes / Traps
- A common mistake could be thinking that scanning after the deployment is sufficient, or assuming local scanning is enough for a CI/CD pipeline. Both of these overlook the efficiencies and automation provided by integrating the scan before deployment in the pipeline.

# Memory Tip
- Always scan your images *before* deployment for early detection of vulnerabilities—secure from the start!"
47,"[""A developer is testing a new file storage application that uses an Amazon\nCloudFront distribution to serve content from an Amazon S3 bucket. The\ndistribution accesses the S3 bucket by using an origin access identity (OAI).\nThe S3 bucket's permissions explicitly deny access to all other users.\nThe application prompts users to authenticate on a login page and then uses\nsigned cookies to allow users to access their personal storage directories. The\ndeveloper has configured the distribution to use its default cache behavior with\nrestricted viewer access and has set the origin to point to the S3 bucket.\nHowever, when the developer tries to navigate to the login page, the developer\nreceives a 403 Forbidden error.\nThe developer needs to implement a solution to allow unauthenticated access to\nthe login page. The solution also must keep all private content secure.\nWhich solution will meet these requirements?""]","Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to the path of the login page, and make viewer access unrestricted. Keep the default cache behavior's settings unchanged.","Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to *, and make viewer access restricted. Change the default cache behavior's path pattern to the path of the login page, and make viewer access unrestricted.","Add a second origin as a failover origin to the default cache behavior. Point the failover origin to the S3 bucket. Set the path pattern for the primary origin to *, and make viewer access restricted. Set the path pattern for the failover origin to the path of the login page, and make viewer access unrestricted.",Add a bucket policy to the S3 bucket to allow read access. Set the resource on the policy to the Amazon Resource Name (ARN) of the login page object in the S3 bucket. Add a CloudFront function to the default cache behavior to redirect unauthorized requests to the login page's S3 URL.,"# Answer
- **Correct option:** A
- **Reason:** Option A suggests adding a second cache behavior to the CloudFront distribution that points specifically to the login page with unrestricted viewer access. This allows anyone to access the login page while keeping the default cache behavior (which is restricted) for all other content, ensuring that private content remains secure.

# Example / Analogy
- Imagine you run a secure building with a main entrance that requires keycard access (like the restricted access for private content). However, you also have a public lobby area (the login page) where anyone can enter without restriction. By setting up a separate entrance just for the lobby (the second cache behavior), you allow unauthenticated users to access that area while keeping the main building secure.

# Common Mistakes / Traps
- A common trap is to think that simply adjusting the default cache behavior will solve the problem. However, that would not grant access to the login page for unauthenticated users, leading to a 403 error. It's essential to identify that a targeted solution (like a separate cache behavior) is needed for public access.

# Memory Tip
- **One behavior for access, another for security!**"
48,"[""A developer is using AWS Amplify Hosting to build and deploy an application. The\ndeveloper is receiving an increased number of bug reports from users. The\ndeveloper wants to add end-to-end testing to the application to eliminate as\nmany bugs as possible before the bugs reach production.\nWhich solution should the developer implement to meet these requirements?""]",Run the amplify add test command in the Amplify CLI.,Create unit tests in the application. Deploy the unit tests by using the amplify push command in the Amplify CLI.,Add a test phase to the amplify.yml build settings for the application.,Add a test phase to the aws-exports.js file for the application.,"# Answer
- **Correct option:** C
- **Reason:** Adding a test phase to the `amplify.yml` build settings allows you to automate end-to-end testing during the build process on AWS Amplify Hosting. This ensures that tests are run each time you deploy the application, helping to catch bugs before the application reaches production.

# Example / Analogy
- Think of `amplify.yml` build settings like a chef's recipe that includes a baking step (test phase) at the end. If the chef doesn't taste the dish (run tests) before serving it, they might deliver a flawed dish (application) to the customers (users). Having a test phase ensures that the dish is perfect before it gets to the table.

# Common Mistakes / Traps
- A common mistake is to think that adding unit tests (Choice B) is sufficient on its own. While unit tests are important, end-to-end tests cover broader functionality and integration, ensuring the entire application works well together.

# Memory Tip
- ""Test early, test often"" to remember that integrating testing into build settings is essential for quality."
49,"[""A company has deployed an application on AWS Elastic Beanstalk. The company has\nconfigured the Auto Scaling group that is associated with the Elastic Beanstalk\nenvironment to have five Amazon EC2 instances. If the capacity is fewer than\nfour EC2 instances during the deployment, application performance degrades. The\ncompany is using the all-at-once deployment policy.\nWhat is the MOST cost-effective way to solve the deployment issue?""]",Change the Auto Scaling group to six desired instances.,Change the deployment policy to traffic splitting. Specify an evaluation time of 1 hour.,Change the deployment policy to rolling with additional batch. Specify a batch size of 1.,Change the deployment policy to rolling. Specify a batch size of 2.,"# Answer
- **Correct option:** C : Change the deployment policy to rolling with additional batch. Specify a batch size of 1.

- **Reason:** Changing the deployment policy to ""rolling with additional batch"" and setting the batch size to 1 allows one instance to be updated at a time while maintaining a minimum of four instances available. This ensures that the application has enough capacity to handle traffic without degrading performance during deployments, thus addressing the issue of application performance degradation.

# Example / Analogy
- Imagine a bakery that gets busy every day. If the bakery decides to renovate all its counters at once (like the all-at-once deployment), it might not have enough space to serve customers, resulting in long lines and unhappy customers. Instead, if the bakery updates one counter at a time (similar to rolling deployment with batch size of 1), it can continue serving customers and won't cause a bottleneck, ensuring happy customers and continued sales.

# Common Mistakes / Traps
- A common mistake is choosing the ""all-at-once"" deployment because it sounds quicker but leads to the performance issue. Additionally, some may mistakenly think that increasing the number of instances is the only solution without realizing that the deployment strategy is also critical for maintaining performance.

# Memory Tip
- Aim to ""roll"" one at a time for smoother transitions during deployments."
50,"[""A company is building a serverless application that uses AWS Lambda functions.\nThe company needs to create a set of test events to test Lambda functions in a\ndevelopment environment. The test events will be created once and then will be\nused by all the developers in an IAM developer group. The test events must be\neditable by any of the IAM users in the IAM developer group.\n\nWhich solution will meet these requirements?""]",Create and store the test events in Amazon S3 as JSON objects. Allow S3 bucket access to all IAM users.,Create the test events. Configure the event sharing settings to make the test events shareable.,Create and store the test events in Amazon DynamoDB. Allow access to DynamoDB by using IAM roles.,Create the test events. Configure the event sharing settings to make the test events private.,"# Answer
- **Correct option:** A

- **Reason:** Storing the test events in an Amazon S3 bucket as JSON objects allows all IAM users in the developer group to access and edit the test events as needed. S3 is designed for easy sharing and storage of files and provides fine-grained access control through IAM policies. Once the events are stored, any developer in the appropriate group can read, modify, and reuse these events without conflicts.

# Example / Analogy
- Think of S3 as a shared drive where all developers store and collaborate on project documents. By placing your test events there (much like filing docs in a shared folder), everyone can retrieve, edit, and share their updates seamlessly, ensuring everyone is working with the same information.

# Common Mistakes / Traps
- A common mistake might be thinking that event sharing settings (as mentioned in options B and D) can facilitate direct collaboration like S3 does. However, Lambda test events do not have built-in sharing features like S3. Additionally, confusing DynamoDB (option C) as an appropriate place for test event storage may lead to unnecessary complexity and cost since S3 is simpler and more suited for this use case.

# Memory Tip
- ""For collaborative storage and easy access, think S3 – the shared drive of AWS."""
51,"[""An engineer created an A/B test of a new feature on an Amazon CloudWatch\nEvidently project. The engineer configured two variations of the feature\n(Variation A and Variation B) for the test. The engineer wants to work\nexclusively with Variation A. The engineer needs to make updates so that\nVariation A is the only variation that appears when the engineer hits the\napplication's endpoint.\n\nWhich solution will meet this requirement?""]",Add an override to the feature. Set the identifier of the override to the engineer's user ID. Set the variation to Variation A.,Add an override to the feature. Set the identifier of the override to Variation A. Set the variation to 100%.,Add an experiment to the project. Set the identifier of the experiment to Variation B. Set the variation to 0%.,Add an experiment to the project. Set the identifier of the experiment to the AWS account's account ISet the variation to Variation A.,"# Answer
- **Correct option:** A
- **Reason:** Adding an override with the engineer's user ID ensures that the application will always serve Variation A to that specific user, effectively bypassing the A/B test for them. This approach allows testing variations without impacting other users by forcing the desired variation for a designated user.

# Example / Analogy
- Think of it like a restaurant where you’re conducting a taste test for two new dishes (Variation A and Variation B). If you want to ensure one specific customer always receives their favorite dish (Variation A), you could create a note in the kitchen that says, ""Always serve dish A to this customer."" This way, they get what they prefer while you continue to experiment with others.

# Common Mistakes / Traps
- A common mistake is to confuse the variations with user identifiers. Option B suggests setting the variation to 100%, which might seem like a good solution, but it does not target specific users and might confuse the overall testing framework, reducing the integrity of the A/B test.

# Memory Tip
- **Override with user ID** to give specific users their desired variation without disrupting the A/B test for others."
52,"[""A developer is working on an existing application that uses Amazon DynamoDB as\nits data store. The DynamoDB table has the following attributes: partNumber\n(partition key), vendor (sort key), description, productFamily, and productType.\nWhen the developer analyzes the usage patterns, the developer notices that there\nare application modules that frequently look for a list of products based on the\nproductFamily and productType attributes.\n\nThe developer wants to make changes to the application to improve performance of\nthe query operations.\n\nWhich solution will meet these requirements?""]",Create a global secondary index (GSI) with productFamily as the partition key and productType as the sort key.,Create a local secondary index (LSI) with productFamily as the partition key and productType as the sort key.,"Recreate the table. Add partNumber as the partition key and vendor as the sort key. During table creation, add a local secondary index (LSI) with productFamily as the partition key and productType as the sort key.",Update the queries to use Scan operations with productFamily as the partition key and productType as the sort key.,"# Answer
- **Correct option:** A: Create a global secondary index (GSI) with productFamily as the partition key and productType as the sort key.
  
- **Reason:** A Global Secondary Index (GSI) allows you to query the DynamoDB table using different key attributes. By creating a GSI with `productFamily` as the partition key and `productType` as the sort key, the application will be able to efficiently retrieve items based on both of these attributes. This improves performance compared to scanning the full table.

# Example / Analogy
- Imagine you have a library organized solely by the author's name (like a DynamoDB table with only one index). If you wanted to find all books in a specific genre, you would need to search through every book, which is very time-consuming (akin to a Scan operation). However, if the library had a separate catalog (GSI) organized by genre and then sub-organized by title, you could quickly find them, saving time (just like the GSI allows for efficient querying based on `productFamily` and `productType`).

# Common Mistakes / Traps
- A common mistake is choosing a Local Secondary Index (LSI) when you actually need an index that allows queries based on different partition keys across the entire dataset. An LSI requires the same partition key as the base table, making it unsuitable in this scenario.

# Memory Tip
- **Tip:** Use GSIs for alternative querying patterns to efficiently retrieve data beyond primary keys."
53,"[""A developer creates a VPC named VPC-A that has public and private subnets. The\ndeveloper also creates an Amazon RDS database inside the private subnet of\nVPC-A. To perform some queries, the developer creates an AWS Lambda function in\nthe default VPC. The Lambda function has code to access the RDS database. When\nthe Lambda function runs, an error message indicates that the function cannot\nconnect to the RDS database.\n\nHow can the developer solve this problem?""]",Modify the RDS security group. Add a rule to allow traffic from all the ports from the VPC CIDR block.,Redeploy the Lambda function in the same subnet as the RDS instance. Ensure that the RDS security group allows traffic from the Lambda function.,Create a security group for the Lambda function. Add a new rule in the RDS security group to allow traffic from the new Lambda security group.,Create an IAM role. Attach a policy that allows access to the RDS database. Attach the role to the Lambda function.,"# Answer
- **Correct option:** B
- **Reason:** The Lambda function is created in the default VPC, which has no direct connection to the private subnet of VPC-A where the RDS database resides. By redeploying the Lambda function within the same VPC (specifically the same subnet), it will have network access to the RDS database. Additionally, ensuring that the RDS's security group allows traffic from the Lambda function's IP or security group is necessary for a successful connection.

# Example / Analogy
- Imagine trying to make a phone call to someone who is out of your service area; no matter how good your phone is (the Lambda function), you won't be able to connect (access the RDS) unless you are both in the same service area (VPC and subnet). By moving your phone to a location with service (redeploying Lambda), you can successfully connect with the person on the other end (RDS database).

# Common Mistakes / Traps
- A common mistake is focusing only on security group modifications (as suggested in options A and C) without considering that a network path must exist. Simply modifying security groups does not allow the Lambda function in a different VPC to communicate with the RDS database.

# Memory Tip
- **Always deploy Lambda functions in the same VPC as the resources they need to access.**"
54,"[""A company runs an application on AWS. The company deployed the application on\nAmazon EC2 instances. The application stores data on Amazon Aurora.\n\nThe application recently logged multiple application-specific custom\nDECRYP_ERROR errors to Amazon CloudWatch logs. The company did not detect the\nissue until the automated tests that run every 30 minutes failed. A developer\nmust implement a solution that will monitor for the custom errors and alert a\ndevelopment team in real time when these errors occur in the production\nenvironment.\n\nWhich solution will meet these requirements with the LEAST operational overhead?""]",Configure the application to create a custom metric and to push the metric to CloudWatch. Create an AWS CloudTrail alarm. Configure the CloudTrail alarm to use an Amazon Simple Notification Service (Amazon SNS) topic to send notifications.,Create an AWS Lambda function to run every 5 minutes to scan the CloudWatch logs for the keyword DECRYP_ERROR. Configure the Lambda function to use Amazon Simple Notification Service (Amazon SNS) to send a notification.,Use Amazon CloudWatch Logs to create a metric filter that has a filter pattern for DECRYP_ERROR. Create a CloudWatch alarm on this metric for a threshold >=1. Configure the alarm to send Amazon Simple Notification Service (Amazon SNS) notifications.,Install the CloudWatch unified agent on the EC2 instance. Configure the application to generate a metric for the keyword DECRYP_ERROR errors. Configure the agent to send Amazon Simple Notification Service (Amazon SNS) notifications.,"# Answer
- **Correct option:** C  
- **Reason:** This option utilizes Amazon CloudWatch Logs to create a metric filter specifically for the DECRYP_ERROR logs. By creating a metric filter, the errors can be monitored in real time, and a CloudWatch alarm can be set up to trigger notifications through Amazon Simple Notification Service (SNS) when the error count meets or exceeds a threshold of 1. This solution is efficient as it directly leverages existing tools and services without requiring additional compute resources or complex configurations.

# Example / Analogy
- Imagine you are in a large office building with multiple floors. Instead of assigning someone to walk around and check if any lights are malfunctioning every few minutes (like the Lambda solution), you install automatic sensors (metric filters) that will alert the maintenance team (SNS) the instant a light goes out (DECRYP_ERROR occurs). This way, you get immediate feedback with minimal effort.

# Common Mistakes / Traps
- One common mistake is overcomplicating the solution when simpler native AWS functionalities can suffice. For example, options that require creating custom metrics or using AWS Lambda solutions might seem attractive but introduce unnecessary complexity and overhead compared to CloudWatch Logs capabilities.

# Memory Tip
- For real-time monitoring of logs, always think **""filter and alert""** with CloudWatch Logs."
55,"[""A developer created an AWS Lambda function that accesses resources in a VPC. The\nLambda function polls an Amazon Simple Queue Service (Amazon SQS) queue for new\nmessages through a VPC endpoint. Then the function calculates a rolling average\nof the numeric values that are contained in the messages. After initial tests of\nthe Lambda function, the developer found that the value of the rolling average\nthat the function returned was not accurate.\n\nHow can the developer ensure that the function calculates an accurate rolling\naverage?""]",Set the function's reserved concurrency to 1. Calculate the rolling average in the function. Store the calculated rolling average in Amazon ElastiCache.,"Modify the function to store the values in Amazon ElastiCache. When the function initializes, use the previous values from the cache to calculate the rolling average.",Set the function's provisioned concurrency to 1. Calculate the rolling average in the function. Store the calculated rolling average in Amazon ElastiCache.,"Modify the function to store the values in the function's layers. When the function initializes, use the previously stored values to calculate the rolling average.","# Answer
- **Correct option:** B
- **Reason:** By modifying the function to store the values in Amazon ElastiCache, the developer can ensure that the function has access to the historical data needed for calculating the rolling average. This allows the Lambda function to retrieve previously stored values whenever it initializes, thus ensuring that the rolling average calculation incorporates all relevant data, leading to an accurate output.

# Example / Analogy
- Think of it like a chef who wants to make a consistent soup every day. Instead of starting from scratch every time, the chef keeps a notebook (Amazon ElastiCache) with the recipes and adjustments from the previous days. This helps the chef create a consistent flavor by using past modifications instead of relying solely on memory.

# Common Mistakes / Traps
- A common mistake is to think that setting concurrency limits (as in options A and C) directly impacts the accuracy of calculations. While controlling concurrency is important for resource management, it does not address the fundamental issue of historical value access needed for accurate calculations.

# Memory Tip
- Use ElastiCache to store past values for consistent rolling calculations!"
56,"[""A developer is writing unit tests for a new application that will be deployed on\nAWS. The developer wants to validate all pull requests with unit tests and merge\nthe code with the main branch only when all tests pass.\n\nThe developer stores the code in AWS CodeCommit and sets up AWS CodeBuild to run\nthe unit tests. The developer creates an AWS Lambda function to start the\nCodeBuild task. The developer needs to identify the CodeCommit events in an\nAmazon EventBridge event that can invoke the Lambda function when a pull request\nis created or updated.\n\nWhich CodeCommit event will meet these requirements?""]",[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image4.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image5.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image6.png],[https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image7.png],"# Answer
- **Correct option:** C

- **Reason:** The CodeCommit event that triggers on pull request creation or update is specifically designed to notify changes related to pull requests. This event would capture scenarios where a new pull request is created, or an existing one is updated, thus allowing the developer's Lambda function to invoke CodeBuild to run unit tests.

# Example / Analogy
- Think of this scenario like a gatekeeper at a concert. The gatekeeper only allows entry (the merge of the code) when a ticket is shown (all tests pass) for each new arrival (pull request). If someone tries to change their ticket (update the pull request), the gatekeeper checks it again before granting access.

# Common Mistakes / Traps
- A common mistake is to confuse pull request events with commits. Pull request actions are different from standard commit actions, and only specific events related to pull requests should be considered for triggering the Lambda function in this scenario.

# Memory Tip
- **Remember “PR Events for Tests”**: Pull Request events in CodeCommit are your key to validating code before it merges."
57,"[""A company hosts a batch processing application on AWS Elastic Beanstalk with\ninstances that run the most recent version of Amazon Linux. The application\nsorts and processes large datasets.\n\nIn recent weeks, the application's performance has decreased significantly\nduring a peak period for traffic. A developer suspects that the application\nissues are related to the memory usage. The developer checks the Elastic\nBeanstalk console and notices that memory usage is not being tracked.\n\nHow should the developer gather more information about the application\nperformance issues?""]",Configure the Amazon CloudWatch agent to push logs to Amazon CloudWatch Logs by using port 443.,Configure the Elastic Beanstalk .ebextensions directory to track the memory usage of the instances.,Configure the Amazon CloudWatch agent to track the memory usage of the instances.,Configure an Amazon CloudWatch dashboard to track the memory usage of the instances.,"# Answer
- **Correct option:** C : Configure the Amazon CloudWatch agent to track the memory usage of the instances.  
- **Reason:** Configuring the Amazon CloudWatch agent allows you to collect detailed metrics, including memory usage, from the EC2 instances running within the Elastic Beanstalk environment. This is essential for diagnosing performance issues related to memory.

# Example / Analogy
- Think of the Amazon CloudWatch agent like a health monitor for your application—just as a doctor would need detailed information about your vital signs (blood pressure, heart rate) to diagnose potential health issues, your application needs memory metrics to identify performance bottlenecks.

# Common Mistakes / Traps
- A common mistake is to think that just configuring CloudWatch alarms or logs (like options A and D) will give insight into performance metrics. However, these will not track actual memory usage unless the agent is configured specifically for that purpose, as mentioned in option C.

# Memory Tip
- Remember: **CloudWatch agent = detailed metrics, including memory!**"
58,"[""A developer is implementing an AWS Cloud Development Kit (AWS CDK) serverless\napplication. The developer will provision several AWS Lambda functions and\nAmazon API Gateway APIs during AWS CloudFormation stack creation. The\ndeveloper's workstation has the AWS Serverless Application Model (AWS SAM) and\nthe AWS CDK installed locally.\n\nHow can the developer test a specific Lambda function locally?""]",Run the sam package and sam deploy commands. Create a Lambda test event from the AWS Management Console. Test the Lambda function.,Run the cdk synth and cdk deploy commands. Create a Lambda test event from the AWS Management Console. Test the Lambda function.,Run the cdk synth and sam local invoke commands with the function construct identifier and the path to the synthesized CloudFormation template.,Run the cdk synth and sam local start-lambda commands with the function construct identifier and the path to the synthesized CloudFormation template.,"# Answer
- **Correct option:** C 
- **Reason:** This option correctly involves using the `cdk synth` command to synthesize the CloudFormation template generated by AWS CDK, followed by `sam local invoke` which allows testing of a specific Lambda function locally using the generated template context. This approach enables the developer to run the function with their local development environment set up properly.

# Example / Analogy
- Think of it like baking a cake from a recipe: you first need to write down the recipe (synthesizing the CloudFormation template with `cdk synth`), and then you can cook a slice of that cake (testing the Lambda function with `sam local invoke`).

# Common Mistakes / Traps
- A common mistake would be to assume that `sam local start-lambda` is required for testing a single function. In fact, `sam local invoke` is more appropriate for executing and testing individual Lambda functions while also allowing event simulation.

# Memory Tip
- **Remember:** Synthesize first, then invoke—CDK creates, SAM executes!"
59,"[""A developer is building an application that uses Amazon DynamoDB. The developer\nwants to retrieve multiple specific items from the database with a single API\ncall.\n\nWhich DynamoDB API call will meet these requirements with the MINIMUM impact on\nthe database?""]",BatchGetItem,GetItem,Scan,Query,"# Answer
- **Correct option:** A : BatchGetItem
- **Reason:** The `BatchGetItem` API call is specifically designed to retrieve multiple items from one or more tables in a single request. This method minimizes the number of requests sent to the database, thus reducing the overall impact on read throughput and improving efficiency.

# Example / Analogy
- Imagine you are at a grocery store and you need to buy ingredients for a recipe. Instead of going to different aisles and checking out separately, you take a shopping list with all the items you need and go through the store once, picking everything up and checking out in a single transaction. Similarly, `BatchGetItem` allows you to pull multiple items from DynamoDB in one API call rather than making separate requests for each item.

# Common Mistakes / Traps
- A common mistake is to confuse `BatchGetItem` with `GetItem`, which retrieves a single item per call. Additionally, some might consider using `Scan` or `Query`, but those methods do not specifically target multiple individual items efficiently and can lead to higher read costs and performance issues.

# Memory Tip
- Remember: ""Batch for multiple, Single for one – save time with BatchGetItem."""
60,"[""A developer is creating a service that uses an Amazon S3 bucket for image\nuploads. The service will use an AWS Lambda function to create a thumbnail of\neach image. Each time an image is uploaded, the service needs to send an email\nnotification and create the thumbnail. The developer needs to configure the\nimage processing and email notifications setup.\n\nWhich solution will meet these requirements?""]",Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure S3 event notifications with a destination of the SNS topic. Subscribe the Lambda function to the SNS topic. Create an email notification subscription to the SNS topic.,Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure S3 event notifications with a destination of the SNS topic. Subscribe the Lambda function to the SNS topic. Create an Amazon Simple Queue Service (Amazon SQS) queue. Subscribe the SQS queue to the SNS topic. Create an email notification subscription to the SQS queue.,Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure S3 event notifications with a destination of the SQS queue. Subscribe the Lambda function to the SQS queue. Create an email notification subscription to the SQS queue.,Create an Amazon Simple Queue Service (Amazon SQS) queue. Send S3 event notifications to Amazon EventBridge. Create an EventBridge rule that runs the Lambda function when images are uploaded to the S3 bucket. Create an EventBridge rule that sends notifications to the SQS queue. Create an email notification subscription to the SQS queue.,"# Answer
- **Correct option:** A

- **Reason:** Option A is the most straightforward and efficient solution to meet the requirements of processing images and sending email notifications. By creating an Amazon SNS topic and configuring S3 event notifications to trigger when an object is uploaded to the S3 bucket, you can easily notify multiple subscribers. The Lambda function can then be subscribed to the SNS topic to process the image and create thumbnails, while an email notification subscription will ensure that notifications are sent whenever an image is uploaded.

# Example / Analogy
- Think of it like a notification system in a workplace. When a new project file (image) is uploaded to the shared drive (S3 bucket), everyone is alerted (SNS topic) about it, while a specific team (Lambda function) immediately starts working on it (creating a thumbnail). At the same time, a receptionist (email subscription) informs everyone that a new file is available.

# Common Mistakes / Traps
- A common mistake is to overlook the direct relationship between S3 and SNS notifications. Some might assume they need to add unnecessary layers (like SQS) when the simpler SNS solution is available. Additionally, people may confuse duties between SNS and SQS, not realizing that SNS can directly handle the email notifications.

# Memory Tip
- **Remember:** SNS = Simple Notifications for Subscriptions; it's the go-to for alerts and awareness of what's happening with your data."
61,"[""A developer has designed an application to store incoming data as JSON files in\nAmazon S3 objects. Custom business logic in an AWS Lambda function then\ntransforms the objects, and the Lambda function loads the data into an Amazon\nDynamoDB table. Recently, the workload has experienced sudden and significant\nchanges in traffic. The flow of data to the DynamoDB table is becoming\nthrottled.\n\nThe developer needs to implement a solution to eliminate the throttling and load\nthe data into the DynamoDB table more consistently.\n\nWhich solution will meet these requirements?""]",Refactor the Lambda function into two functions. Configure one function to transform the data and one function to load the data into the DynamoDB table. Create an Amazon Simple Queue Service (Amazon SQS) queue in between the functions to hold the items as messages and to invoke the second function.,Turn on auto scaling for the DynamoDB table. Use Amazon CloudWatch to monitor the table's read and write capacity metrics and to track consumed capacity.,Create an alias for the Lambda function. Configure provisioned concurrency for the application to use.,Refactor the Lambda function into two functions. Configure one function to store the data in the DynamoDB table. Configure the second function to process the data and update the items after the data is stored in DynamoDB. Create a DynamoDB stream to invoke the second function after the data is stored.,"# Answer
- **Correct option:** A

- **Reason:** Refactoring the Lambda function into two separate functions allows for better management of workloads and reduces throttling issues. By introducing an Amazon SQS queue, the first Lambda function can handle data transformation and place the messages in SQS, while the second function can consume messages from the queue and handle the loading into DynamoDB at its own pace. This decouples the processing and loading functions, thereby smoothing out the peaks in traffic and preventing DynamoDB from becoming overwhelmed.

# Example / Analogy
- Think of a restaurant where the kitchen (Lambda function) receives orders (data) to prepare meals (transform data) and serve them to the customers (DynamoDB). If too many customers arrive at once, the kitchen could get backed up (DynamoDB throttling). By adding a conveyor belt (SQS), the kitchen can prepare meals at its own pace and place them on the belt, while another staff member takes orders from the belt to serve to the customers. This way, the kitchen can focus on preparation without getting overwhelmed.

# Common Mistakes / Traps
- A common trap is to assume that simply increasing capacity (like auto-scaling for DynamoDB) will solve all performance issues. However, if the Lambda function is directly invoking DynamoDB at a high frequency, it could still lead to throttling, so breaking the workload into manageable parts through SQS is essential.

# Memory Tip
- ""Decouple processes with queues to manage peaks and avoid throttling."""
